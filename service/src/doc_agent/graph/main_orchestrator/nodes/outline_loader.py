"""
å¤§çº²åŠ è½½å™¨èŠ‚ç‚¹æ¨¡å—

è´Ÿè´£è¯»å–ç”¨æˆ·ä¸Šä¼ çš„å¤§çº²æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨å¤§æ¨¡å‹å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼çš„å¤§çº²
"""

import json
import os
import re
import tempfile

from doc_agent.core.logger import logger
from doc_agent.graph.callbacks import publish_event
from doc_agent.graph.state import ResearchState
from doc_agent.llm_clients.base import LLMClient
from doc_agent.tools.es_search import ESSearchTool
from doc_agent.tools.file_module import FileProcessor


async def outline_loader_node(state: ResearchState, llm_client: LLMClient,
                              es_search_tool: ESSearchTool) -> dict:
    """
    å¤§çº²åŠ è½½å™¨èŠ‚ç‚¹
    è¯»å–ç”¨æˆ·ä¸Šä¼ çš„å¤§çº²æ–‡ä»¶ï¼Œä½¿ç”¨å¤§æ¨¡å‹å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼çš„å¤§çº²
    
    Args:
        state: ç ”ç©¶çŠ¶æ€
        llm_client: LLMå®¢æˆ·ç«¯
        es_search_tool: ESæœç´¢å·¥å…·
        prompt_selector: æç¤ºè¯é€‰æ‹©å™¨
        genre: æ–‡æ¡£ç±»å‹
        
    Returns:
        dict: åŒ…å« document_outline çš„å­—å…¸
    """
    user_outline_file = state.get("user_outline_file", "")
    job_id = state.get("job_id", "")
    task_prompt = state.get("task_prompt", "")

    if not user_outline_file:
        raise ValueError("ç”¨æˆ·å¤§çº²æ–‡ä»¶tokenä¸èƒ½ä¸ºç©º")

    # å¦‚æœæ²¡æœ‰topicï¼Œå°è¯•ä»task_promptä¸­æå–
    if task_prompt:
        logger.info("ğŸ” ä»task_promptä¸­æå–topic...")
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡åˆ†æå¼•æ“ã€‚ä½ çš„å”¯ä¸€ç›®æ ‡æ˜¯è§£æç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œå¹¶ä»ä¸­æå–å…³é”®çš„ä»»åŠ¡è¦æ±‚ã€‚

ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æŒ‡ä»¤ï¼š
1.  åˆ†ææ–‡æœ¬ï¼Œè¯†åˆ«å‡ºä»»åŠ¡çš„ã€ä¸»é¢˜ã€‘ã€ã€å­—æ•°è¦æ±‚ã€‘å’Œã€å…¶ä»–è¦æ±‚ã€‘ã€‚
2.  ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€ã€æœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼Œä¸èƒ½åŒ…å«ä»»ä½• JSON æ ¼å¼ä¹‹å¤–çš„é¢å¤–æ–‡æœ¬ã€è§£é‡Šæˆ–æ³¨é‡Šã€‚
3.  ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„ schema å’Œå­—æ®µè§„åˆ™ç”Ÿæˆ JSONï¼š

```json
{{
    "topic": "ä»»åŠ¡çš„æ ¸å¿ƒä¸»é¢˜æˆ–æ ‡é¢˜ã€‚",
    "word_count": "ä»æ–‡æœ¬ä¸­æå–æ˜ç¡®çš„å­—æ•°è¦æ±‚ï¼Œåªä¿ç•™æ•°å­—éƒ¨åˆ†ä½œä¸ºå­—ç¬¦ä¸²ã€‚å¦‚æœæ²¡æœ‰æåˆ°ä»»ä½•å­—æ•°è¦æ±‚ï¼Œåˆ™è¯¥å­—æ®µçš„å€¼å¿…é¡»æ˜¯ '-1'ã€‚",
    "other_requirements": "é™¤äº†ä¸»é¢˜å’Œå­—æ•°å¤–çš„æ‰€æœ‰å…¶ä»–å…·ä½“è¦æ±‚ï¼Œä¾‹å¦‚æ ¼å¼ã€é£æ ¼ã€éœ€è¦åŒ…å«çš„è¦ç‚¹ã€å—ä¼—ç­‰ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™è¯¥å­—æ®µä¸ºç©ºå­—ç¬¦ä¸² ''ã€‚"
}}
```

ç”¨æˆ·è¾“å…¥: {task_prompt}

å¼€å§‹è¾“å‡º jsonï¼š

"""
        try:
            response = llm_client.invoke(prompt)
            logger.info(f"ğŸ” ä»»åŠ¡åˆ†æå“åº”: {response}")

            # æå– ```json ``` å†…çš„ json éƒ¨åˆ†
            json_pattern = r'```json\s*(.*?)\s*```'
            json_match = re.search(json_pattern, response, re.DOTALL)
            if json_match:
                response = json_match.group(1)
            response_data = json.loads(response)
            topic = response_data.get("topic", "")
            extracted_word_count = response_data.get("word_count", "-1")
            other_requirements = response_data.get("other_requirements", "")

            if not topic:
                logger.warning("ä»task_promptä¸­æå–çš„ä¸»é¢˜ä¸ºç©º")

            try:
                word_count = int(extracted_word_count)
            except ValueError:
                word_count = 5000  # é»˜è®¤å€¼
            if word_count < 0:
                word_count = 5000  # é»˜è®¤å€¼

            logger.info(f"âœ… æˆåŠŸæå–topic: {topic}")
            logger.info(f"âœ… å­—æ•°è¦æ±‚: {word_count}")
            logger.info(f"âœ… å…¶ä»–è¦æ±‚: {other_requirements}")

        except Exception as e:
            logger.warning(f"âš ï¸ ä»task_promptæå–topicå¤±è´¥: {str(e)}")
            topic = prompt
            word_count = 5000

    # å¦‚æœä»ç„¶æ²¡æœ‰topicï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not topic:
        topic = "æ–‡æ¡£å¤§çº²ç”Ÿæˆ"
        word_count = 5000

    logger.info(f"ğŸ“‹ å¼€å§‹åŠ è½½ç”¨æˆ·å¤§çº²æ–‡ä»¶: {user_outline_file}")
    logger.info(f"ä¸»é¢˜: {topic}")

    try:
        # 1. ä»ESä¸­è·å–å¤§çº²æ–‡ä»¶å†…å®¹
        logger.info("ğŸ” ä»ESä¸­æŸ¥è¯¢å¤§çº²æ–‡ä»¶å†…å®¹...")
        es_results = await es_search_tool.search_by_file_token(
            file_token=user_outline_file,
            top_k=1000  # è·å–è¶³å¤Ÿçš„å†…å®¹
        )

        if not es_results:
            logger.error(f"âŒ æœªæ‰¾åˆ°file_tokenä¸º {user_outline_file} çš„æ–‡æ¡£")
            raise ValueError(f"æœªæ‰¾åˆ°å¤§çº²æ–‡ä»¶: {user_outline_file}")

        # 2. ç›´æ¥ä½¿ç”¨ESè¿”å›çš„å†…å®¹
        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(es_results)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

        if not es_results:
            logger.error("âŒ æœªæ‰¾åˆ°å¤§çº²æ–‡ä»¶å†…å®¹")
            raise ValueError("æœªæ‰¾åˆ°å¤§çº²æ–‡ä»¶å†…å®¹")

        # æ‹¼æ¥æ‰€æœ‰ç»“æœ
        logger.info(f"ğŸ“ å¤§çº²æ–‡ä»¶å†…å®¹: {es_results} ")
        # ç”¨ metadata.slice_id ä¸ºé¡ºåºæ’åºåæ‹¼æ¥
        es_results.sort(key=lambda x: x.metadata.get("slice_id", 0))
        outline_content = "\n".join(
            [result.original_content for result in es_results])
        logger.info(f"ğŸ“ å¤§çº²æ–‡ä»¶å†…å®¹é•¿åº¦: {len(outline_content)} å­—ç¬¦")
        logger.info(f"ğŸ“ å¤§çº²æ–‡ä»¶å†…å®¹é¢„è§ˆ: {outline_content[:200]}...")

        # 4. ä½¿ç”¨hardcodeçš„æç¤ºè¯æ¨¡æ¿
        prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å¤§çº²åˆ†æä¸“å®¶ã€‚ç”¨æˆ·æä¾›äº†ä¸€ä¸ªå¤§çº²æ–‡ä»¶ï¼Œä½ éœ€è¦å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†çš„ç»“æ„åŒ–å¤§çº²æ ¼å¼ã€‚

è¯·åˆ†æç”¨æˆ·æä¾›çš„å¤§çº²æ–‡ä»¶å†…å®¹ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªç¬¦åˆä»¥ä¸‹JSONæ ¼å¼çš„æ ‡å‡†å¤§çº²ï¼š

```json
{{
    "title": "æ–‡æ¡£æ ‡é¢˜",
    "summary": "æ–‡æ¡£çš„ç®€çŸ­æ‘˜è¦ï¼ˆ50-100å­—ï¼‰",
    "chapters": [
        {{
            "number": 1,
            "title": "ç¬¬ä¸€ç« æ ‡é¢˜",
            "description": "æœ¬ç« çš„ç®€è¦æè¿°",
            "sections": [
                {{
                    "number": 1.1,
                    "title": "ç¬¬ä¸€èŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }},
                {{
                    "number": 1.2,
                    "title": "ç¬¬äºŒèŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }},
                {{
                    "number": 1.3,
                    "title": "ç¬¬ä¸‰èŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }}
            ]
        }},
        {{
            "number": 2,
            "title": "ç¬¬äºŒç« æ ‡é¢˜",
            "description": "æœ¬ç« çš„ç®€è¦æè¿°",
            "sections": [
                {{
                    "number": 2.1,
                    "title": "ç¬¬ä¸€èŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }},
                {{
                    "number": 2.2,
                    "title": "ç¬¬äºŒèŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }},
                {{
                    "number": 2.3,
                    "title": "ç¬¬ä¸‰èŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }}
            ]
        }},
        {{
            "number": 3,
            "title": "ç¬¬ä¸‰ç« æ ‡é¢˜",
            "description": "æœ¬ç« çš„ç®€è¦æè¿°",
            "sections": [
                {{
                    "number": 3.1,
                    "title": "ç¬¬ä¸€èŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }},
                {{
                    "number": 3.2,
                    "title": "ç¬¬äºŒèŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }},
                {{
                    "number": 3.3,
                    "title": "ç¬¬ä¸‰èŠ‚æ ‡é¢˜",
                    "description": "æœ¬èŠ‚çš„ç®€è¦æè¿°",
                    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
                }}
            ]
        }}
    ],
    "total_chapters": 3,
    "estimated_total_words": 5000
}}
```

è¦æ±‚ï¼š
1. ä¸¥æ ¼ä¿æŒç”¨æˆ·åŸå§‹å¤§çº²çš„ç»“æ„å’Œé€»è¾‘
2. ç¡®ä¿ç« èŠ‚å’Œå°èŠ‚çš„å±‚æ¬¡å…³ç³»æ¸…æ™°
3. ä¸ºæ¯ä¸ªç« èŠ‚å’Œå°èŠ‚æ·»åŠ åˆé€‚çš„æè¿°
4. ç« èŠ‚ç¼–å·ä»1å¼€å§‹ï¼Œå­èŠ‚ç¼–å·ä½¿ç”¨å°æ•°ç‚¹æ ¼å¼ï¼ˆå¦‚1.1, 1.2, 1.3ï¼‰
5. ç›®æ ‡æ€»å­—æ•°ä¸º{word_count}å­—å·¦å³
6. å°½é‡ä½¿ç”¨ç”¨æˆ·æä¾›ä¸»é¢˜ï¼Œè‹¥ç”¨æˆ·ä¸»é¢˜ä¸ºç©ºæˆ–æœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨å¤§çº²æ–‡ä»¶å†…å®¹ä¸­çš„ä¸»é¢˜
7. å¦‚æœç”¨æˆ·æä¾›äº†å…¶ä»–è¦æ±‚ï¼Œåˆ™å°½é‡æ»¡è¶³ç”¨æˆ·è¦æ±‚ï¼Œå¹¶æ”¾å…¥å¤§çº²çš„descriptionä¸­

ç”¨æˆ·ä¸»é¢˜ï¼š{topic}

ç”¨æˆ·å¤§çº²æ–‡ä»¶å†…å®¹ï¼š
{outline_content}

ç”¨æˆ·çš„å…¶ä»–è¦æ±‚ï¼š
{other_requirements}

è¯·ç”Ÿæˆæ ‡å‡†æ ¼å¼çš„å¤§çº²JSONï¼š"""

        # 5. æ„å»ºå®Œæ•´çš„æç¤ºè¯
        prompt = prompt_template.format(topic=topic,
                                        outline_content=outline_content,
                                        word_count=word_count,
                                        other_requirements=other_requirements)

        # 6. è°ƒç”¨LLMç”Ÿæˆå¤§çº²
        logger.info("ğŸ¤– è°ƒç”¨LLMåˆ†æå¤§çº²æ–‡ä»¶...")
        logger.info(f"ğŸ“ å‘é€ç»™LLMçš„prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        logger.info(f"ğŸ“ å‘é€ç»™LLMçš„promptå‰200å­—ç¬¦: {prompt[:200]}...")

        try:
            # ä½¿ç”¨invokeæ–¹æ³•è°ƒç”¨LLM
            logger.info("ğŸ”„ å¼€å§‹è°ƒç”¨LLM...")

            response = llm_client.invoke(prompt)
            logger.info("âœ… LLMè°ƒç”¨å®Œæˆ")

            if not response or not response.strip():
                logger.error("âŒ LLMè¿”å›ç©ºå“åº”")
                raise ValueError("LLMè¿”å›ç©ºå“åº”")

            logger.info(f"ğŸ“ LLMåŸå§‹å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            logger.info(f"ğŸ“ LLMåŸå§‹å“åº”å‰500å­—ç¬¦: {repr(response[:500])}")
            logger.info(f"ğŸ“ LLMåŸå§‹å“åº”å®Œæ•´å†…å®¹: {repr(response)}")

        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            raise ValueError(f"LLMè°ƒç”¨å¤±è´¥: {e}") from e

        # 7. è§£æJSONå“åº”
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            outline_data = json.loads(response)
        except json.JSONDecodeError:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            logger.warning("ç›´æ¥JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†")

            # å°è¯•å¤šç§JSONæå–æ¨¡å¼
            json_patterns = [
                r'```json\s*(.*?)\s*```',  # ```json ... ```
                r'```\s*(.*?)\s*```',  # ``` ... ```
                r'\{.*\}',  # ä»»ä½•JSONå¯¹è±¡
            ]

            for pattern in json_patterns:
                json_match = re.search(pattern, response, re.DOTALL)
                if json_match:
                    try:
                        json_str = json_match.group(
                            1) if pattern != r'\{.*\}' else json_match.group(0)
                        outline_data = json.loads(json_str)
                        logger.info(f"âœ… ä½¿ç”¨æ¨¡å¼ {pattern} æˆåŠŸè§£æJSON")
                        break
                    except json.JSONDecodeError:
                        continue
            else:
                logger.error(f"âŒ æ— æ³•è§£æLLMå“åº”ä¸ºJSON: {response}")
                raise ValueError("æ— æ³•è§£æLLMå“åº”ä¸ºJSON")

        # 8. éªŒè¯å¤§çº²æ ¼å¼
        if not isinstance(outline_data, dict):
            raise ValueError("å¤§çº²æ•°æ®æ ¼å¼é”™è¯¯")
        if word_count > 0:
            outline_data["estimated_total_words"] = word_count

        if "title" not in outline_data or "chapters" not in outline_data:
            raise ValueError("å¤§çº²ç¼ºå°‘å¿…è¦å­—æ®µ")

        # 9. å°†å¤§çº²ä¿å­˜ä¸ºæ–‡ä»¶å¹¶ä¸Šä¼ åˆ°å­˜å‚¨æœåŠ¡
        file_token = None
        try:
            # åˆå§‹åŒ–æ–‡ä»¶å¤„ç†å™¨
            file_processor = FileProcessor()

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w',
                                             suffix='.json',
                                             delete=False,
                                             encoding='utf-8') as temp_file:
                json.dump(outline_data,
                          temp_file,
                          ensure_ascii=False,
                          indent=2)
                temp_file_path = temp_file.name

            # ä¸Šä¼ æ–‡ä»¶
            file_token = file_processor.upload_file(temp_file_path)
            logger.info(f"ğŸ“ å¤§çº²æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ŒToken: {file_token}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file_path)

        except Exception as e:
            logger.error(f"å¤§çº²æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
            file_token = None

        # 10. å‘å¸ƒæˆåŠŸäº‹ä»¶
        publish_event(
            job_id,
            "å¤§çº²ç”Ÿæˆ",
            "outline_loader",
            "SUCCESS", {
                "outline": outline_data,
                "file_token": file_token,
                "description":
                f"æˆåŠŸåŠ è½½å¹¶è§£æç”¨æˆ·å¤§çº²æ–‡ä»¶ï¼ŒåŒ…å« {len(outline_data.get('chapters', []))} ä¸ªç« èŠ‚",
                "chapters_count": len(outline_data.get("chapters", [])),
                "title": outline_data.get("title", "")
            },
            task_finished=True)

        logger.success(
            f"âœ… å¤§çº²åŠ è½½å®Œæˆï¼Œç”Ÿæˆ {len(outline_data.get('chapters', []))} ä¸ªç« èŠ‚")

        return {
            "document_outline": outline_data,
            "initial_sources": [],  # ç”¨æˆ·ä¸Šä¼ çš„å¤§çº²æ–‡ä»¶ï¼Œä¸éœ€è¦é¢å¤–çš„æº
            "outline_source": "user_upload"  # æ ‡è®°å¤§çº²æ¥æº
        }

    except Exception as e:
        logger.error(f"âŒ å¤§çº²åŠ è½½å¤±è´¥: {e}", exc_info=True)
        publish_event(job_id,
                      "å¤§çº²åŠ è½½",
                      "outline_loader",
                      "ERROR", {"description": f"å¤§çº²åŠ è½½å¤±è´¥: {str(e)}"},
                      task_finished=True)
        raise