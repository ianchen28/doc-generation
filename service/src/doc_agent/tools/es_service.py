"""
Elasticsearch åº•å±‚æœåŠ¡æ¨¡å—
æä¾›åŸºç¡€çš„ESè¿æ¥å’Œæœç´¢åŠŸèƒ½ï¼Œæ”¯æŒKNNå‘é‡æœç´¢
"""

import asyncio
import time
from dataclasses import dataclass
# from tkinter import W
from typing import Any, Optional

from elasticsearch import AsyncElasticsearch

from doc_agent.core.logger import logger
from doc_agent.utils.timing import CodeTimer


@dataclass
class ESSearchResult:
    """ESæœç´¢ç»“æœ"""
    id: str
    doc_id: str  # file_token
    index: str  # ç´¢å¼•
    domain_id: str  # index æ˜ å°„ä¹‹åçš„ domain_id
    doc_from: str  # æ¥æº self/data_platform ï¼ˆç”¨æˆ·ä¸Šä¼ ä¸º selfï¼Œ å…¶ä»–ä¸º data_platformï¼‰
    file_token: str  # file_token (å¤§æ¦‚ç‡æ²¡æœ‰å€¼)
    original_content: str  # åŸå§‹å†…å®¹
    div_content: str = ""  # åˆ‡åˆ†åçš„å†…å®¹
    source: str = ""
    score: float = 0.0
    metadata: dict[str, Any] = None
    alias_name: str = ""  # æ¥æºç´¢å¼•åˆ«å

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class ESService:
    """ESåº•å±‚æœåŠ¡ç±»"""

    def __init__(self,
                 hosts: list[str],
                 username: str = "",
                 password: str = "",
                 timeout: int = 30,
                 connections_per_node: int = 25):
        """
        åˆå§‹åŒ–ESæœåŠ¡

        Args:
            hosts: ESæœåŠ¡å™¨åœ°å€åˆ—è¡¨
            username: ç”¨æˆ·å
            password: å¯†ç 
            timeout: è¶…æ—¶æ—¶é—´
            connections_per_node: æ¯ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ•°
        """
        self.hosts = hosts
        self.username = username
        self.password = password
        self.timeout = timeout
        self.connections_per_node = connections_per_node
        self._client: Optional[AsyncElasticsearch] = None
        self._initialized = False
        logger.info("åˆå§‹åŒ–ESæœåŠ¡")
        self.domain_index_map = {
            "documentUploadAnswer": "personal_knowledge_base",
            "standard": "standard_index_prod",
            "thesis": "thesis_index_prod",
            "book": "book_index_prod",
            "other": "other_index_prod",
            "internal": "internal_index_prod_v2",
            "policy": "hdy_knowledge_prod_v2",
            "executivevoice": "hdy_knowledge_prod_v2",
            "corporatenews": "hdy_knowledge_prod_v2",
            "announcement": "hdy_knowledge_prod_v2",
            "ai_demo": "ai_demo"
        }
        self.index_aliases = {}
        self.augmented_index_domain_map = {}
        self.valid_indeces = []
        self._es_request_semaphore = asyncio.Semaphore(connections_per_node)

    async def connect(self) -> bool:
        """è¿æ¥ESæœåŠ¡"""
        logger.info("å¼€å§‹è¿æ¥ESæœåŠ¡")
        try:
            es_kwargs = {
                "hosts": self.hosts,
                "verify_certs": False,
                "ssl_show_warn": False,
                "request_timeout": self.timeout,
                "max_retries": 3,
                "retry_on_timeout": True,
                # è¿æ¥æ± é…ç½®ï¼ˆå‚è€ƒAsyncElasticsearchæºç ï¼‰
                "connections_per_node": self.connections_per_node,  # æ¯ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ•°
                # HTTPè¿æ¥å™¨é…ç½®
                "http_compress": True,  # å¯ç”¨HTTPå‹ç¼©
                # é‡è¯•é…ç½®ï¼ˆæ ¹æ®æºç æ”¯æŒçš„å‚æ•°ï¼‰
                "retry_on_status": [502, 503, 504],  # æŒ‡å®šé‡è¯•çš„HTTPçŠ¶æ€ç 
                # è¿æ¥ç®¡ç†é…ç½®
                "dead_node_backoff_factor": 2.0,  # æ­»èŠ‚ç‚¹é€€é¿å› å­
                "max_dead_node_backoff": 300.0,  # æœ€å¤§æ­»èŠ‚ç‚¹é€€é¿æ—¶é—´ï¼ˆç§’ï¼‰
                # èŠ‚ç‚¹å—…æ¢é…ç½®ï¼ˆåœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹å»ºè®®å…³é—­ï¼‰
                "sniff_on_start": False,  # å¯åŠ¨æ—¶ä¸å—…æ¢èŠ‚ç‚¹
                "sniff_before_requests": False,  # è¯·æ±‚å‰ä¸å—…æ¢èŠ‚ç‚¹
                "sniff_on_node_failure": False,  # èŠ‚ç‚¹å¤±è´¥æ—¶ä¸å—…æ¢
            }

            if self.username and self.password:
                es_kwargs["basic_auth"] = (self.username, self.password)
                logger.debug("ä½¿ç”¨åŸºæœ¬è®¤è¯è¿æ¥ES")

            self._client = AsyncElasticsearch(**es_kwargs)

            # æµ‹è¯•è¿æ¥
            await self._client.ping()
            logger.info("ESè¿æ¥æˆåŠŸ")

            # è·å–ç´¢å¼•åˆ«å
            aliases_info = await self._client.indices.get_alias(index="*")
            for index_name, info in aliases_info.items():
                if 'aliases' in info:
                    self.index_aliases[index_name] = list(
                        info['aliases'].keys())
                else:
                    self.index_aliases[index_name] = []

            logger.info(f"æˆåŠŸè·å–ç´¢å¼•åˆ«åæ˜ å°„ï¼Œå…± {len(self.index_aliases)} ä¸ªç´¢å¼•")

            # æ„å»ºç´¢å¼•åˆ°åŸŸåçš„æ˜ å°„
            for idx, alias_list in self.index_aliases.items():
                print(f"{idx}: {alias_list}")

                # æŸ¥æ‰¾åŒ¹é…çš„åŸŸå
                matched_domain_id = None
                for domain_id, domain_idx in self.domain_index_map.items():
                    if (domain_idx == idx or domain_idx in alias_list):
                        matched_domain_id = domain_id
                        break

                # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„åŸŸåï¼Œæ·»åŠ åˆ°æ˜ å°„è¡¨
                if matched_domain_id:
                    self.augmented_index_domain_map[idx] = matched_domain_id
                    for alias_idx in alias_list:
                        self.augmented_index_domain_map[
                            alias_idx] = matched_domain_id

                # æ·»åŠ æ‰€æœ‰åŒ¹é…domain_index_mapçš„ç´¢å¼•åˆ°æœ‰æ•ˆç´¢å¼•åˆ—è¡¨
                if matched_domain_id:
                    for alias_idx in alias_list:
                        if (alias_idx not in self.valid_indeces
                                and alias_idx != "personal_knowledge_base"):
                            self.valid_indeces.append(alias_idx)

            logger.info(f"ğŸ” ç´¢å¼•åˆ«å: {self.index_aliases}")
            logger.info(f"æ‰©å±•æ˜ å°„è¡¨: {self.augmented_index_domain_map}")
            logger.info(f"æœ‰æ•ˆç´¢å¼•: {self.valid_indeces}")

            self._initialized = True
            return True

        except Exception as e:
            logger.error(f"ESè¿æ¥å¤±è´¥: {str(e)}")
            self._initialized = False
            return False

    async def _ensure_connected(self):
        """ç¡®ä¿å·²è¿æ¥"""
        if not self._initialized or not self._client:
            logger.debug("ESå®¢æˆ·ç«¯æœªè¿æ¥ï¼Œå°è¯•è¿æ¥")
            await self.connect()

    async def search(
            self,
            index: str,
            query: str,
            top_k: int = 10,
            query_vector: Optional[list[float]] = None,
            filters: Optional[dict[str, Any]] = None) -> list[ESSearchResult]:
        """
        æ‰§è¡ŒESæœç´¢

        Args:
            index: ç´¢å¼•åç§°
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            query_vector: æŸ¥è¯¢å‘é‡ï¼ˆå¯é€‰ï¼‰
            filters: è¿‡æ»¤æ¡ä»¶ï¼ˆå¯é€‰ï¼‰

        Returns:
            List[ESSearchResult]: æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹ESæœç´¢ï¼Œç´¢å¼•: {index}, æŸ¥è¯¢: {query[:50]}...")
        logger.debug(f"æœç´¢å‚æ•° - top_k: {top_k}")
        if query_vector:
            logger.debug(f"æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vector)}")
        if filters:
            logger.debug(f"è¿‡æ»¤æ¡ä»¶: {filters}")

        # éªŒè¯ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼ˆå…è®¸é€šé…ç¬¦ç´¢å¼•ç”¨äºæ–‡æ¡£èŒƒå›´æœç´¢ï¼‰
        if index != "*" and index not in self.valid_indeces:
            logger.warning(f"ç´¢å¼• {index} ä¸åœ¨æœ‰æ•ˆç´¢å¼•èŒƒå›´å†…: {self.valid_indeces}")
            return []
        if index == "*":
            index = self.valid_indeces

        await self._ensure_connected()

        if not self._client:
            logger.error("ESå®¢æˆ·ç«¯æœªè¿æ¥")
            return []

        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_body = self._build_search_body(query, query_vector, filters,
                                                  top_k)
            # logger.debug(f"æœç´¢æŸ¥è¯¢ä½“: {search_body}")

            logger.info(f"æœç´¢query <es_search>: {query}")
            start_time = time.time()
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            with CodeTimer("es_search <timer>"):
                async with self._es_request_semaphore:
                    # æ‰§è¡Œæœç´¢
                    response = await self._client.search(index=index,
                                                         body=search_body)
            end_time = time.time()
            logger.info(
                f"ESæœç´¢è€—æ—¶<es_search>: {query} {end_time - start_time:.4f} ç§’")

            # è§£æç»“æœ
            results = []
            for hit in response['hits']['hits']:
                doc_data = hit['_source']

                # è·å–åŸå§‹å†…å®¹å’Œåˆ‡åˆ†åçš„å†…å®¹
                original_content = (doc_data.get('content_view')
                                    or doc_data.get('content')
                                    or doc_data.get('text')
                                    or doc_data.get('title') or '')

                div_content = (doc_data.get('content') or doc_data.get('text')
                               or doc_data.get('title') or '')

                # çµæ´»è·å–æ¥æºå­—æ®µ
                source = (doc_data.get('meta_data', {}).get('file_name')
                          or doc_data.get('file_name') or doc_data.get('name')
                          or '')

                # å®‰å…¨è·å– doc_idï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ _id
                doc_id = doc_data.get('doc_id', "")
                index = hit["_index"]
                domain_id = self.augmented_index_domain_map.get(index, "")

                # å¦‚æœæ‰¾ä¸åˆ°domain_idï¼Œå°è¯•ä»ç´¢å¼•åç§°æ¨æ–­
                if not domain_id:
                    # å°è¯•ä»ç´¢å¼•åç§°æ¨æ–­åŸŸå
                    for known_domain, known_index in self.domain_index_map.items(
                    ):
                        if index == known_index or index in self.index_aliases.get(
                                known_index, []):
                            domain_id = known_domain
                            break

                    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç´¢å¼•åç§°ä½œä¸ºdomain_id
                    if not domain_id:
                        domain_id = index
                        logger.debug(f"æœªæ‰¾åˆ°ç´¢å¼• {index} çš„åŸŸåæ˜ å°„ï¼Œä½¿ç”¨ç´¢å¼•åç§°ä½œä¸ºdomain_id")

                doc_from = "self" if domain_id == "documentUploadAnswer" else "data_platform"

                logger.debug(
                    f"æœç´¢ç»“æœ - ç´¢å¼•: {index}, domain_id: {domain_id}, doc_from: {doc_from}"
                )

                result = ESSearchResult(id=hit['_id'],
                                        doc_id=doc_id,
                                        index=index,
                                        domain_id=domain_id,
                                        doc_from=doc_from,
                                        file_token=doc_data.get(
                                            'file_token', ""),
                                        original_content=original_content,
                                        div_content=div_content,
                                        source=source,
                                        score=hit['_score'],
                                        metadata=doc_data.get('meta_data', {}),
                                        alias_name=index)
                # ä¿®æ”¹ metadata.source = doc_from
                result.metadata["source"] = doc_from
                results.append(result)

            logger.info(f"ESæœç´¢æˆåŠŸï¼Œè¿”å› {len(results)} ä¸ªæ–‡æ¡£")
            return results

        except Exception as e:
            logger.error(f"ESæœç´¢å¤±è´¥: {str(e)}")
            return []

    def _build_search_body(self,
                           query: str,
                           query_vector: Optional[list[float]] = None,
                           filters: Optional[dict[str, Any]] = None,
                           top_k: int = 10) -> dict[str, Any]:
        """
        æ„å»ºæœç´¢æŸ¥è¯¢ä½“

        Args:
            query: æœç´¢æŸ¥è¯¢
            query_vector: æŸ¥è¯¢å‘é‡
            filters: è¿‡æ»¤æ¡ä»¶
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            Dict[str, Any]: æœç´¢æŸ¥è¯¢ä½“
        """
        # å¦‚æœæœ‰å‘é‡æŸ¥è¯¢ï¼Œä¼˜å…ˆä½¿ç”¨KNNæœç´¢
        if query_vector:
            logger.debug("ä½¿ç”¨KNNå‘é‡æœç´¢")
            return self._build_knn_search_body(query_vector, query, filters,
                                               top_k)
        else:
            logger.debug("ä½¿ç”¨æ–‡æœ¬æœç´¢")
            return self._build_text_search_body(query, filters, top_k)

    def _build_knn_search_body(self,
                               query_vector: list[float],
                               query: str = "",
                               filters: Optional[dict[str, Any]] = None,
                               top_k: int = 10) -> dict[str, Any]:
        """
        æ„å»ºKNNå‘é‡æœç´¢æŸ¥è¯¢ä½“

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            query: æ–‡æœ¬æŸ¥è¯¢ï¼ˆå¯é€‰ï¼Œç”¨äºæ··åˆæœç´¢ï¼‰
            filters: è¿‡æ»¤æ¡ä»¶
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            Dict[str, Any]: KNNæœç´¢æŸ¥è¯¢ä½“
        """
        # ç¡®ä¿å‘é‡ç»´åº¦æ­£ç¡®
        if len(query_vector) != 1536:
            if len(query_vector) > 1536:
                query_vector = query_vector[:1536]
                logger.debug("æˆªæ–­å‘é‡ç»´åº¦åˆ° 1536")
            else:
                query_vector.extend([0.0] * (1536 - len(query_vector)))
                logger.debug("æ‰©å±•å‘é‡ç»´åº¦åˆ° 1536")

        # æ„å»ºåŸºç¡€KNNæŸ¥è¯¢
        search_body = {
            "size": top_k,
            "_source": {
                "excludes": ["content"]  # æ’é™¤å¤§å­—æ®µä»¥æé«˜æ€§èƒ½
            },
            "knn": {
                "field": "context_vector",
                "query_vector": query_vector,
                "k": top_k,
                "num_candidates": top_k * 2
            }
        }

        # å¦‚æœæœ‰æ–‡æœ¬æŸ¥è¯¢ï¼Œä½¿ç”¨æ··åˆæœç´¢
        if query:
            logger.debug("æ„å»ºæ··åˆæœç´¢æŸ¥è¯¢")
            search_body = {
                "size": top_k,
                "knn": {
                    "field": "context_vector",
                    "query_vector": query_vector,
                    "k": top_k,  # æ‰©å¤§å€™é€‰æ± 
                    "num_candidates": top_k * 2  # å¢åŠ å€™é€‰æ•°é‡
                },
                "query": {
                    "bool": {
                        "filter": [{
                            "multi_match": {
                                "query": query,
                                "fields": ["content", "title", "text"],
                                "type": "best_fields"
                            }
                        }]
                    }
                }
            }

        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filters:
            logger.debug(f"æ·»åŠ è¿‡æ»¤æ¡ä»¶: {filters}")
            filter_conditions = self._build_filter_conditions(filters)
            if "knn" in search_body:
                search_body["knn"]["filter"] = filter_conditions
            elif "query" in search_body:
                # å¯¹äºæ··åˆæœç´¢ï¼Œå°†è¿‡æ»¤æ¡ä»¶æ·»åŠ åˆ°boolæŸ¥è¯¢ä¸­
                if "bool" in search_body["query"]["script_score"]["query"]:
                    search_body["query"]["script_score"]["query"]["bool"][
                        "filter"] = filter_conditions["bool"]["must"]

        return search_body

    def _build_text_search_body(self,
                                query: str,
                                filters: Optional[dict[str, Any]] = None,
                                top_k: int = 10) -> dict[str, Any]:
        """
        æ„å»ºæ–‡æœ¬æœç´¢æŸ¥è¯¢ä½“

        Args:
            query: æœç´¢æŸ¥è¯¢
            filters: è¿‡æ»¤æ¡ä»¶
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            Dict[str, Any]: æ–‡æœ¬æœç´¢æŸ¥è¯¢ä½“
        """
        # åŸºç¡€æ–‡æœ¬æŸ¥è¯¢
        search_body = {
            "size": top_k,
            "query": {
                "bool": {
                    "should": [{
                        "multi_match": {
                            "query":
                            query,
                            "fields": [
                                "content^2", "file_name", "title^2", "text^2",
                                "name"
                            ],
                            "type":
                            "best_fields"
                        }
                    }],
                    "minimum_should_match":
                    1
                }
            }
        }

        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filters:
            logger.debug(f"æ·»åŠ è¿‡æ»¤æ¡ä»¶: {filters}")
            filter_conditions = self._build_filter_conditions(filters)
            search_body["query"]["bool"]["filter"] = filter_conditions["bool"][
                "must"]

        return search_body

    def _build_filter_conditions(self, filters: dict[str,
                                                     Any]) -> dict[str, Any]:
        """
        æ„å»ºè¿‡æ»¤æ¡ä»¶

        Args:
            filters: è¿‡æ»¤æ¡ä»¶å­—å…¸

        Returns:
            Dict[str, Any]: è¿‡æ»¤æ¡ä»¶æŸ¥è¯¢ä½“
        """
        filter_conditions = {"bool": {"must": [], "must_not": []}}

        for key, value in filters.items():
            if isinstance(value, list):
                if value:  # éç©ºåˆ—è¡¨
                    filter_conditions["bool"]["must"].append(
                        {"terms": {
                            key: value
                        }})
            elif value is not None:
                filter_conditions["bool"]["must"].append(
                    {"term": {
                        key: value
                    }})

        logger.debug(f"æ„å»ºçš„è¿‡æ»¤æ¡ä»¶: {filter_conditions}")
        return filter_conditions

    async def search_multiple_indices(
            self,
            indices: list[str],
            query: str,
            top_k: int = 10,
            query_vector: Optional[list[float]] = None,
            filters: Optional[dict[str, Any]] = None) -> list[ESSearchResult]:
        """
        åœ¨å¤šä¸ªç´¢å¼•ä¸­æœç´¢

        Args:
            indices: ç´¢å¼•åˆ—è¡¨
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            query_vector: æŸ¥è¯¢å‘é‡
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            List[ESSearchResult]: æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å¤šç´¢å¼•æœç´¢ï¼Œç´¢å¼•æ•°é‡: {len(indices)}")
        logger.debug(f"ç´¢å¼•åˆ—è¡¨: {indices}")
        logger.debug(f"æœç´¢å‚æ•° - top_k: {top_k}")
        if query_vector:
            logger.debug(f"æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vector)}")
        if filters:
            logger.debug(f"è¿‡æ»¤æ¡ä»¶: {filters}")

        if not indices:
            logger.warning("ç´¢å¼•åˆ—è¡¨ä¸ºç©º")
            return []

        # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„ç´¢å¼•
        valid_indices = [idx for idx in indices if idx in self.valid_indeces]
        if not valid_indices:
            logger.warning(f"æ‰€æœ‰ç´¢å¼•éƒ½ä¸åœ¨æœ‰æ•ˆèŒƒå›´å†…: {indices}")
            return []

        if len(valid_indices) != len(indices):
            invalid_indices = [
                idx for idx in indices if idx not in self.valid_indeces
            ]
            logger.warning(f"è¿‡æ»¤æ‰æ— æ•ˆç´¢å¼•: {invalid_indices}")

        await self._ensure_connected()

        if not self._client:
            logger.error("ESå®¢æˆ·ç«¯æœªè¿æ¥")
            return []

        try:
            # æ„å»ºmsearchè¯·æ±‚ä½“
            msearch_body = []
            search_body = self._build_search_body(query, query_vector, filters,
                                                  top_k)

            for index in valid_indices:
                msearch_body.append({"index": index})
                msearch_body.append(search_body)

            logger.debug(f"æ„å»ºmsearchè¯·æ±‚ä½“ï¼ŒåŒ…å« {len(indices)} ä¸ªç´¢å¼•")

            # æ‰§è¡Œmsearch
            response = await self._client.msearch(body=msearch_body)

            # å¤„ç†ç»“æœ
            all_results = []
            for i, search_response in enumerate(response["responses"]):
                if "hits" in search_response and "hits" in search_response[
                        "hits"]:
                    for hit in search_response["hits"]["hits"]:
                        doc_data = hit["_source"]

                        # è·å–å†…å®¹
                        original_content = (doc_data.get('content_view')
                                            or doc_data.get('content')
                                            or doc_data.get('text')
                                            or doc_data.get('title') or '')

                        div_content = (doc_data.get('content')
                                       or doc_data.get('text')
                                       or doc_data.get('title') or '')

                        # è·å–æ¥æº
                        source = (doc_data.get('meta_data',
                                               {}).get('file_name')
                                  or doc_data.get('file_name')
                                  or doc_data.get('name') or '')

                        # å®‰å…¨è·å– doc_idï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ _id
                        doc_id = doc_data.get('doc_id', "")
                        index = hit["_index"]
                        domain_id = self.augmented_index_domain_map.get(
                            index, "")

                        # å¦‚æœæ‰¾ä¸åˆ°domain_idï¼Œå°è¯•ä»ç´¢å¼•åç§°æ¨æ–­
                        if not domain_id:
                            # å°è¯•ä»ç´¢å¼•åç§°æ¨æ–­åŸŸå
                            for known_domain, known_index in self.domain_index_map.items(
                            ):
                                if index == known_index or index in self.index_aliases.get(
                                        known_index, []):
                                    domain_id = known_domain
                                    break

                            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç´¢å¼•åç§°ä½œä¸ºdomain_id
                            if not domain_id:
                                domain_id = index
                                logger.debug(
                                    f"æœªæ‰¾åˆ°ç´¢å¼• {index} çš„åŸŸåæ˜ å°„ï¼Œä½¿ç”¨ç´¢å¼•åç§°ä½œä¸ºdomain_id")

                        doc_from = "self" if domain_id == "documentUploadAnswer" else "data_platform"

                        logger.debug(
                            f"å¤šç´¢å¼•æœç´¢ç»“æœ - ç´¢å¼•: {index}, domain_id: {domain_id}, doc_from: {doc_from}"
                        )

                        result = ESSearchResult(
                            id=hit["_id"],
                            doc_id=doc_id,
                            index=index,
                            domain_id=domain_id,
                            doc_from=doc_from,
                            file_token=doc_data.get('file_token', ""),
                            original_content=original_content,
                            div_content=div_content,
                            source=source,
                            score=hit["_score"],
                            metadata=doc_data.get('meta_data', {}),
                            alias_name=valid_indices[i]
                            if i < len(valid_indices) else "")
                        # ä¿®æ”¹ metadata.source = doc_from
                        result.metadata["source"] = doc_from
                        all_results.append(result)

            logger.info(f"å¤šç´¢å¼•æœç´¢æˆåŠŸï¼Œè¿”å› {len(all_results)} ä¸ªæ–‡æ¡£")
            return all_results

        except Exception as e:
            logger.error(f"å¤šç´¢å¼•æœç´¢å¤±è´¥: {str(e)}")
            return []

    async def search_by_file_token(self,
                                   index: str,
                                   file_token: str,
                                   top_k: int = 100) -> list[ESSearchResult]:
        """
        æ ¹æ®file_tokenæŸ¥è¯¢æ–‡æ¡£å†…å®¹
        
        Args:
            index: ç´¢å¼•åç§°
            file_token: æ–‡ä»¶token
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[ESSearchResult]: æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æŒ‰file_tokenæŸ¥è¯¢ï¼Œç´¢å¼•: {index}, file_token: {file_token}")

        # éªŒè¯ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼ˆå…è®¸é€šé…ç¬¦ç´¢å¼•ç”¨äºæ–‡æ¡£èŒƒå›´æœç´¢ï¼‰
        if index != "*" and index not in self.valid_indeces:
            logger.warning(f"ç´¢å¼• {index} ä¸åœ¨æœ‰æ•ˆç´¢å¼•èŒƒå›´å†…: {self.valid_indeces}")
            return []

        await self._ensure_connected()

        if not self._client:
            logger.error("ESå®¢æˆ·ç«¯æœªè¿æ¥")
            return []

        try:
            search_body = {
                "size": top_k * 2,  # è®¾ç½®æ›´å¤§çš„size
                "query": {
                    "term": {
                        "doc_id": file_token
                    }
                }
                # ç§»é™¤æ’åºï¼Œé¿å…å­—æ®µä¸å­˜åœ¨çš„é—®é¢˜
            }

            logger.debug(f"file_tokenæŸ¥è¯¢ä½“: {search_body}")

            # æ‰§è¡Œæœç´¢
            response = await self._client.search(index=index, body=search_body)

            # è§£æç»“æœ
            results = []
            for hit in response['hits']['hits']:
                doc_data = hit['_source']

                # è·å–åŸå§‹å†…å®¹å’Œåˆ‡åˆ†åçš„å†…å®¹
                original_content = (doc_data.get('content_view')
                                    or doc_data.get('content')
                                    or doc_data.get('text')
                                    or doc_data.get('title') or '')

                div_content = (doc_data.get('content') or doc_data.get('text')
                               or doc_data.get('title') or '')

                # çµæ´»è·å–æ¥æºå­—æ®µ
                source = (doc_data.get('meta_data', {}).get('file_name')
                          or doc_data.get('file_name') or doc_data.get('name')
                          or '')

                # å®‰å…¨è·å– doc_id
                doc_id = doc_data.get('doc_id', "")
                index = hit["_index"]
                domain_id = self.augmented_index_domain_map.get(index, "")

                # å¦‚æœæ‰¾ä¸åˆ°domain_idï¼Œå°è¯•ä»ç´¢å¼•åç§°æ¨æ–­
                if not domain_id:
                    # å°è¯•ä»ç´¢å¼•åç§°æ¨æ–­åŸŸå
                    for known_domain, known_index in self.domain_index_map.items(
                    ):
                        if index == known_index or index in self.index_aliases.get(
                                known_index, []):
                            domain_id = known_domain
                            break

                    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç´¢å¼•åç§°ä½œä¸ºdomain_id
                    if not domain_id:
                        domain_id = index
                        logger.debug(f"æœªæ‰¾åˆ°ç´¢å¼• {index} çš„åŸŸåæ˜ å°„ï¼Œä½¿ç”¨ç´¢å¼•åç§°ä½œä¸ºdomain_id")

                doc_from = "self" if domain_id == "documentUploadAnswer" else "data_platform"

                logger.debug(
                    f"file_tokenæŸ¥è¯¢ç»“æœ - ç´¢å¼•: {index}, domain_id: {domain_id}, doc_from: {doc_from}"
                )

                result = ESSearchResult(id=hit['_id'],
                                        doc_id=doc_id,
                                        index=index,
                                        domain_id=domain_id,
                                        doc_from=doc_from,
                                        file_token=doc_data.get(
                                            'file_token', ""),
                                        original_content=original_content,
                                        div_content=div_content,
                                        source=source,
                                        score=hit['_score'],
                                        metadata=doc_data.get('meta_data', {}),
                                        alias_name=index)
                # ä¿®æ”¹ metadata.source = doc_from
                result.metadata["source"] = doc_from
                results.append(result)

            logger.info(f"file_tokenæŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(results)} ä¸ªæ–‡æ¡£")
            return results

        except Exception as e:
            logger.error(f"file_tokenæŸ¥è¯¢å¤±è´¥: {str(e)}")
            return []

    async def close(self):
        """å…³é—­è¿æ¥"""
        logger.info("å¼€å§‹å…³é—­ESè¿æ¥")
        if self._client:
            try:
                await self._client.close()
                self._client = None
                self._initialized = False
                logger.info("ESè¿æ¥å·²å…³é—­")

                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿è¿æ¥å®Œå…¨å…³é—­
                await asyncio.sleep(0.05)

            except Exception as e:
                logger.error(f"å…³é—­ESè¿æ¥å¤±è´¥: {str(e)}")
                self._client = None
                self._initialized = False

    async def get_indices(self) -> list[dict[str, Any]]:
        """è·å–æ‰€æœ‰ç´¢å¼•ä¿¡æ¯"""
        logger.debug("è·å–æ‰€æœ‰ç´¢å¼•ä¿¡æ¯")
        await self._ensure_connected()

        if not self._client:
            logger.error("ESå®¢æˆ·ç«¯æœªè¿æ¥")
            return []

        try:
            indices = [
                'personal_knowledge_base', '.infini_alert-message',
                '.infini_visualization', 'hdy_knowledge_base_v3_0422',
                '.infini_rbac-user', '.infini_alert-rule', '.infini_node',
                '.infini_layout',
                '.internal.alerts-observability.uptime.alerts-default-000001',
                'thesis_index_base', '.apm-source-map',
                '.infini_async_bulk_results-00001',
                '.slo-observability.summary-v3.temp',
                '.internal.alerts-transform.health.alerts-default-000001',
                'thesis_index_base_v2', 'thesis_index_base_v3',
                'text2sql_table', 'ai_demo',
                '.internal.alerts-observability.apm.alerts-default-000001',
                '.infini_dashboard', '.infini_requests_logging-00001',
                'internal_index_base_v2', '.infini_widget',
                'standard_index_base_v2', 'internal_index_base_v3',
                '.infini_channel',
                '.internal.alerts-security.alerts-default-000001',
                'other_index_base', '.infini_view',
                '.internal.alerts-observability.logs.alerts-default-000001',
                '.infini_configs', 'standard_index_base',
                '.infini_alert-history-000003', '.infini_alert-history-000002',
                '.infini_index', '.infini_instance',
                '.slo-observability.summary-v3', 'dev_user_knowledge_base_v1',
                '.kibana-observability-ai-assistant-kb-000001',
                '.infini_credential', '.infini_host', '.infini_task',
                'defection_index_base', '.slo-observability.sli-v3',
                'hdy_leader_index', '.infini_commands',
                '.internal.alerts-ml.anomaly-detection.alerts-default-000001',
                '.internal.alerts-observability.slo.alerts-default-000001',
                '.infini_activities-000002',
                '.internal.alerts-observability.metrics.alerts-default-000001',
                '.infini_activities-000003', 'book_index_base_v2',
                'hdy_knowledge_summary_index', 'thesis_summary_index_base',
                'dev_user_knowledge_base', '.monitoring-es-7-2025.08.13',
                '.monitoring-es-7-2025.08.14', '.monitoring-es-7-2025.08.15',
                '.monitoring-es-7-2025.08.16', '.monitoring-es-7-2025.08.17',
                '.internal.alerts-stack.alerts-default-000001',
                '.monitoring-es-7-2025.08.18', '.monitoring-es-7-2025.08.19',
                '.monitoring-kibana-7-2025.08.13', 'extract_image_index_v4',
                'book_index_base', '.infini_metrics-000005',
                '.monitoring-kibana-7-2025.08.19', '.infini_metrics-000004',
                '.monitoring-kibana-7-2025.08.18', '.infini_metrics-000007',
                '.infini_metrics-000006', '.monitoring-kibana-7-2025.08.15',
                '.monitoring-kibana-7-2025.08.14', 'book_index_base_v2_1',
                '.infini_metrics-000003', '.monitoring-kibana-7-2025.08.17',
                '.infini_logs-00001', '.monitoring-kibana-7-2025.08.16',
                '.kibana-observability-ai-assistant-conversations-000001',
                '.internal.alerts-observability.threshold.alerts-default-000001',
                'text2sql_enum_prod', '.infini_cluster', 'text2sql_enum',
                'hdy_knowledge_base_v5', 'hdy_knowledge_base_v3',
                'hdy_knowledge_base_v7', 'hdy_knowledge_base_v8',
                '.infini_rbac-role',
                '.internal.alerts-default.alerts-default-000001',
                'hdy_knowledge_base',
                '.internal.alerts-ml.anomaly-detection-health.alerts-default-000001',
                '.infini_notification', '.infini_email-server',
                '.infini_audit-logs-000002', '.infini_audit-logs-000003',
                'standard_summary_index_base_v6',
                'standard_summary_index_base_v4',
                'standard_summary_index_base_v5', 'book_index_base_v3_2',
                'standard_summary_index_base_v2', 'formula_index_base',
                'standard_summary_index_base_v3', 'internal_index_base',
                'standard_index_v1_1', 'standard_index_v1_2',
                'text2sql_table_prod', 'standard_summary_index_base',
                'hdy_leader_index_v4', 'hdy_leader_index_v2'
            ]
            logger.debug(f"è·å–åˆ° {len(indices)} ä¸ªç´¢å¼•")
            return indices
        except Exception as e:
            logger.error(f"è·å–ç´¢å¼•å¤±è´¥: {str(e)}")
            return []

    async def get_index_mapping(self, index: str) -> Optional[dict[str, Any]]:
        """è·å–ç´¢å¼•æ˜ å°„"""
        logger.debug(f"è·å–ç´¢å¼• {index} çš„æ˜ å°„ä¿¡æ¯")

        # éªŒè¯ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if index not in self.valid_indeces:
            logger.warning(f"ç´¢å¼• {index} ä¸åœ¨æœ‰æ•ˆç´¢å¼•èŒƒå›´å†…: {self.valid_indeces}")
            return None

        await self._ensure_connected()

        if not self._client:
            logger.error("ESå®¢æˆ·ç«¯æœªè¿æ¥")
            return None

        try:
            mapping = await self._client.indices.get_mapping(index=index)
            logger.debug(f"æˆåŠŸè·å–ç´¢å¼• {index} çš„æ˜ å°„")
            return mapping[index]['mappings']
        except Exception as e:
            logger.error(f"è·å–ç´¢å¼•æ˜ å°„å¤±è´¥: {str(e)}")
            return None

    def get_valid_indices(self) -> list[str]:
        """è·å–æœ‰æ•ˆç´¢å¼•åˆ—è¡¨"""
        return self.valid_indeces.copy()

    def is_valid_index(self, index: str) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ"""
        return index in self.valid_indeces

    async def __aenter__(self):
        logger.debug("è¿›å…¥ESæœåŠ¡å¼‚æ­¥ä¸Šä¸‹æ–‡")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        logger.debug("é€€å‡ºESæœåŠ¡å¼‚æ­¥ä¸Šä¸‹æ–‡")
        await self.close()
