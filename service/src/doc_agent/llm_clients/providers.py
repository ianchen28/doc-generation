# service/src/doc_agent/llm_clients/providers.py
import json
import pprint
import re
import time
from collections.abc import AsyncGenerator, Generator

import httpx

from doc_agent.core.logger import logger
from doc_agent.llm_clients.base import BaseOutputParser, LLMClient
from doc_agent.utils.timing import CodeTimer


class ReasoningParser(BaseOutputParser):
    """
    æ¨ç†è¾“å‡ºè§£æå™¨
    å»é™¤æ¨¡å‹å“åº”ä¸­çš„æ¨ç†è¿‡ç¨‹ï¼Œåªä¿ç•™æœ€ç»ˆç­”æ¡ˆ
    """

    def __init__(self, reasoning: bool = False):
        self.reasoning = reasoning

    def parse(self, response: str) -> str:
        if self.reasoning:
            # ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤ <think> å’Œ </think> ä¹‹é—´çš„å†…å®¹
            response = re.sub(r'<think>.*?</think>',
                              '',
                              response,
                              flags=re.DOTALL)
            return response.strip()
        else:
            return response.strip()


class GeminiClient(LLMClient):

    def __init__(self,
                 base_url: str,
                 model_name: str = "gemini-1.5-pro-latest",
                 api_key: str = None,
                 reasoning: bool = False,
                 timeout: float = 60.0):
        """
        åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯
        Args:
            api_key: Gemini APIå¯†é’¥
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºgemini-1.5-pro-latest
            base_url: APIåŸºç¡€URLï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤URL
            reasoning: æ˜¯å¦å¯ç”¨æ¨ç†æ¨¡å¼
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.api_key = api_key
        self.model_name = model_name
        self.reasoning = reasoning
        self.timeout = timeout
        self.parser = ReasoningParser(reasoning=reasoning)
        # å¦‚æœæä¾›äº†base_urlï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤URL
        if base_url:
            self.base_url = base_url.rstrip('/')
        else:
            self.base_url = "https://generativelanguage.googleapis.com/v1beta/models"

    def invoke(self, prompt: str, **kwargs) -> str:
        """
        è°ƒç”¨Gemini API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰
        Returns:
            str: æ¨¡å‹å“åº”çš„å†…å®¹
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "maxOutputTokens": max_tokens
                }
            }

            # å‘é€è¯·æ±‚ - ä¿®å¤ Gemini API URL
            # ChatAI API ä½¿ç”¨ä¸åŒçš„ç«¯ç‚¹æ ¼å¼
            if "chataiapi.com" in self.base_url:
                # ChatAI API æ ¼å¼
                url = f"{self.base_url}/chat/completions"
                headers = {"Authorization": f"Bearer {self.api_key}"}
                # ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼
                data = {
                    "model": self.model_name,
                    "messages": [{
                        "role": "user",
                        "content": prompt
                    }],
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }
            else:
                # æ ‡å‡† Gemini API æ ¼å¼
                url = f"{self.base_url}/{self.model_name}:generateContent?key={self.api_key}"
                headers = {}
                # ä½¿ç”¨ Gemini æ ¼å¼
                data = {
                    "contents": [{
                        "parts": [{
                            "text": prompt
                        }]
                    }],
                    "generationConfig": {
                        "temperature": temperature,
                        "maxOutputTokens": max_tokens
                    }
                }

            logger.debug(
                f"Gemini API request:\nURL: {url}\nData: {pprint.pformat(data)}"
            )

            with httpx.Client(timeout=self.timeout) as client:
                response = client.post(url, json=data, headers=headers)
                response.raise_for_status()

                result = response.json()

                # æå–å“åº”å†…å®¹ - æ”¯æŒä¸¤ç§æ ¼å¼
                if "chataiapi.com" in self.base_url:
                    # ChatAI API è¿”å› OpenAI å…¼å®¹æ ¼å¼
                    if "choices" in result and len(result["choices"]) > 0:
                        content = result["choices"][0]["message"]["content"]
                        logger.debug(f"ğŸ” ChatAIåŸå§‹å“åº”: '{content}'")
                        parsed_content = self.parser.parse(content)
                        logger.debug(f"ğŸ” ChatAIè§£æå: '{parsed_content}'")
                        return parsed_content
                    else:
                        raise ValueError(
                            "No response content received from ChatAI API")
                else:
                    # æ ‡å‡† Gemini API æ ¼å¼
                    if "candidates" in result and len(
                            result["candidates"]) > 0:
                        content = result["candidates"][0]["content"]["parts"][
                            0]["text"]
                        logger.debug(f"ğŸ” GeminiåŸå§‹å“åº”: '{content}'")
                        parsed_content = self.parser.parse(content)
                        logger.debug(f"ğŸ” Geminiè§£æå: '{parsed_content}'")
                        return parsed_content
                    else:
                        raise ValueError(
                            "No response content received from Gemini API")

        except Exception as e:
            logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Gemini APIè°ƒç”¨å¤±è´¥: {str(e)}") from e

    async def astream(self, prompt: str,
                      **kwargs) -> AsyncGenerator[str, None]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨Gemini API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰
        Yields:
            str: æ¨¡å‹å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)

            # æ„å»ºè¯·æ±‚æ•°æ®
            if "chataiapi.com" in self.base_url:
                # ChatAI API æ ¼å¼
                url = f"{self.base_url}/chat/completions"
                headers = {"Authorization": f"Bearer {self.api_key}"}
                data = {
                    "model": self.model_name,
                    "messages": [{
                        "role": "user",
                        "content": prompt
                    }],
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "stream": True  # å¯ç”¨æµå¼è¾“å‡º
                }
            else:
                # æ ‡å‡† Gemini API æ ¼å¼
                url = f"{self.base_url}/{self.model_name}:streamGenerateContent?key={self.api_key}"
                headers = {}
                data = {
                    "contents": [{
                        "parts": [{
                            "text": prompt
                        }]
                    }],
                    "generationConfig": {
                        "temperature": temperature,
                        "maxOutputTokens": max_tokens
                    }
                }

            logger.debug(
                f"Gemini æµå¼APIè¯·æ±‚:\nURL: {url}\nData: {pprint.pformat(data)}")

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream("POST",
                                         url,
                                         json=data,
                                         headers=headers) as response:
                    response.raise_for_status()

                    if "chataiapi.com" in self.base_url:
                        # ChatAI API æµå¼æ ¼å¼
                        async for line in response.aiter_lines():
                            if line.startswith("data: "):
                                data_str = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                                if data_str.strip() == "[DONE]":
                                    break
                                try:
                                    chunk = json.loads(data_str)
                                    if "choices" in chunk and len(
                                            chunk["choices"]) > 0:
                                        delta = chunk["choices"][0].get(
                                            "delta", {})
                                        if "content" in delta and delta[
                                                "content"]:
                                            yield delta["content"]
                                except json.JSONDecodeError:
                                    continue
                    else:
                        # æ ‡å‡† Gemini API æµå¼æ ¼å¼
                        async for line in response.aiter_lines():
                            if line.strip():
                                try:
                                    chunk = json.loads(line)
                                    if "candidates" in chunk and len(
                                            chunk["candidates"]) > 0:
                                        content = chunk["candidates"][0][
                                            "content"]["parts"][0]["text"]
                                        if content:
                                            yield content
                                except json.JSONDecodeError:
                                    continue

        except Exception as e:
            logger.error(f"Gemini æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Gemini æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}") from e


class DeepSeekClient(LLMClient):

    def __init__(self,
                 base_url: str,
                 api_key: str,
                 model_name: str,
                 reasoning: bool = False,
                 timeout: float = 60.0):
        """
        åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
        Args:
            api_key: DeepSeek APIå¯†é’¥
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºdeepseek-chat
            reasoning: æ˜¯å¦å¯ç”¨æ¨ç†æ¨¡å¼
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.api_key = api_key
        self.model_name = model_name
        self.reasoning = reasoning
        self.timeout = timeout
        self.parser = ReasoningParser(reasoning=reasoning)
        self.base_url = base_url.rstrip('/')

    def invoke(self, prompt: str, **kwargs) -> str:
        """
        è°ƒç”¨DeepSeek API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰
        Returns:
            str: æ¨¡å‹å“åº”çš„å†…å®¹
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "messages": [{
                    "role": "user",
                    "content": prompt
                }],
                "temperature": temperature,
                "max_tokens": max_tokens
            }

            logger.debug(
                f"DeepSeek API request:\nURL: {self.base_url}/chat/completions\nData: {pprint.pformat(data)}"
            )

            # å‘é€è¯·æ±‚
            url = f"{self.base_url}/chat/completions"
            headers = {"Authorization": f"Bearer {self.api_key}"}

            with httpx.Client(timeout=self.timeout) as client:
                response = client.post(url, json=data, headers=headers)
                response.raise_for_status()

                result = response.json()

                # æå–å“åº”å†…å®¹
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"]
                    return self.parser.parse(content)
                else:
                    raise ValueError(
                        "No response content received from DeepSeek API")

        except Exception as e:
            logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}") from e

    async def astream(self, prompt: str,
                      **kwargs) -> AsyncGenerator[str, None]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨DeepSeek API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰
        Yields:
            str: æ¨¡å‹å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "messages": [{
                    "role": "user",
                    "content": prompt
                }],
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True  # å¯ç”¨æµå¼è¾“å‡º
            }

            logger.debug(
                f"DeepSeek æµå¼APIè¯·æ±‚:\nURL: {self.base_url}/chat/completions\nData: {pprint.pformat(data)}"
            )

            # å‘é€è¯·æ±‚
            url = f"{self.base_url}/chat/completions"
            headers = {"Authorization": f"Bearer {self.api_key}"}

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream("POST",
                                         url,
                                         json=data,
                                         headers=headers) as response:
                    response.raise_for_status()

                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data_str = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                            if data_str.strip() == "[DONE]":
                                break
                            try:
                                chunk = json.loads(data_str)
                                if "choices" in chunk and len(
                                        chunk["choices"]) > 0:
                                    delta = chunk["choices"][0].get(
                                        "delta", {})
                                    if "content" in delta and delta["content"]:
                                        yield delta["content"]
                            except json.JSONDecodeError:
                                continue

        except Exception as e:
            logger.error(f"DeepSeek æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"DeepSeek æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}") from e


class MoonshotClient(LLMClient):

    def __init__(self,
                 base_url: str,
                 api_key: str,
                 model_name: str,
                 reasoning: bool = False,
                 timeout: float = 180.0):
        """
        åˆå§‹åŒ–Moonshotå®¢æˆ·ç«¯
        Args:
            base_url: Moonshot APIåœ°å€
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            reasoning: æ˜¯å¦å¯ç”¨æ¨ç†æ¨¡å¼
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model_name = model_name
        self.reasoning = reasoning
        self.timeout = timeout
        self.parser = ReasoningParser(reasoning=reasoning)

    def invoke(self, prompt: str, **kwargs) -> str:
        """
        è°ƒç”¨Moonshot API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰
        Returns:
            str: æ¨¡å‹å“åº”çš„å†…å®¹
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)

            # æ„å»ºè¯·æ±‚æ•°æ® - ä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
            data = {
                "model": self.model_name,
                "messages": [{
                    "role": "user",
                    "content": prompt
                }],
                "temperature": temperature,
                "max_tokens": max_tokens
            }

            logger.debug(
                f"Moonshot API request:\nURL: {self.base_url}/chat/completions\nData: {pprint.pformat(data)}"
            )

            # å‘é€è¯·æ±‚
            url = f"{self.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            # åˆ›å»ºè‡ªå®šä¹‰çš„httpxå®¢æˆ·ç«¯ï¼Œé¿å…proxieså‚æ•°é—®é¢˜
            http_client = httpx.Client(timeout=self.timeout)

            with http_client as client:
                response = client.post(url, json=data, headers=headers)
                response.raise_for_status()

                result = response.json()

                # æå–å“åº”å†…å®¹
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"]
                    logger.debug(f"ğŸ” MoonshotåŸå§‹å“åº”: '{content}'")
                    parsed_content = self.parser.parse(content)
                    logger.debug(f"ğŸ” Moonshotè§£æå: '{parsed_content}'")
                    return parsed_content
                else:
                    raise ValueError(
                        "No response content received from Moonshot API")

        except Exception as e:
            logger.error(f"Moonshot APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Moonshot APIè°ƒç”¨å¤±è´¥: {str(e)}") from e

    async def astream(self, prompt: str,
                      **kwargs) -> AsyncGenerator[str, None]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨Moonshot API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰
        Yields:
            str: æ¨¡å‹å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)

            # æ„å»ºè¯·æ±‚æ•°æ® - ä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
            data = {
                "model": self.model_name,
                "messages": [{
                    "role": "user",
                    "content": prompt
                }],
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True  # å¯ç”¨æµå¼è¾“å‡º
            }

            logger.debug(
                f"Moonshot æµå¼APIè¯·æ±‚:\nURL: {self.base_url}/chat/completions\nData: {pprint.pformat(data)}"
            )

            # å‘é€è¯·æ±‚
            url = f"{self.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream("POST",
                                         url,
                                         json=data,
                                         headers=headers) as response:
                    response.raise_for_status()

                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data_str = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                            if data_str.strip() == "[DONE]":
                                break
                            try:
                                chunk = json.loads(data_str)
                                if "choices" in chunk and len(
                                        chunk["choices"]) > 0:
                                    delta = chunk["choices"][0].get(
                                        "delta", {})
                                    if "content" in delta and delta["content"]:
                                        yield delta["content"]
                            except json.JSONDecodeError:
                                continue

        except Exception as e:
            logger.error(f"Moonshot æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Moonshot æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}") from e


class InternalLLMClient(LLMClient):

    def __init__(self,
                 base_url: str,
                 api_key: str,
                 model_name: str,
                 reasoning: bool = False,
                 timeout: float = 180.0):
        """
        åˆå§‹åŒ–å†…éƒ¨æ¨¡å‹å®¢æˆ·ç«¯
        Args:
            base_url: å†…éƒ¨APIåœ°å€
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            reasoning: æ˜¯å¦å¯ç”¨æ¨ç†æ¨¡å¼
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model_name = model_name
        self.reasoning = reasoning
        self.timeout = timeout
        self.parser = ReasoningParser(reasoning=reasoning)

    def invoke(self, prompt: str, **kwargs) -> str:
        """
        è°ƒç”¨å†…éƒ¨æ¨¡å‹API

        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰

        Returns:
            str: æ¨¡å‹å“åº”çš„å†…å®¹
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 5000)

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "messages": [{
                    "role": "user",
                    "content": prompt
                }],
                "temperature": temperature,
                "max_tokens": max_tokens
            }

            logger.debug(
                f"Internal API request:\nURL: {self.base_url}/chat/completions\nData: {pprint.pformat(data)}"
            )

            # å‘é€è¯·æ±‚
            url = f"{self.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}"
            } if self.api_key != "EMPTY" else {}

            with CodeTimer("llm_call <timer>"):
                with httpx.Client(
                        timeout=self.timeout) as client:  # ä½¿ç”¨é…ç½®çš„timeout
                    response = client.post(url, json=data, headers=headers)
                    response.raise_for_status()

                    result = response.json()

                    # æå–å“åº”å†…å®¹
                    if "choices" in result and len(result["choices"]) > 0:
                        content = result["choices"][0]["message"]["content"]
                        return self.parser.parse(content)
                    else:
                        raise ValueError(
                            "No response content received from Internal API")

        except Exception as e:
            logger.error(f"Internal APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Internal APIè°ƒç”¨å¤±è´¥: {str(e)}") from e

    def stream(self, prompt: str, **kwargs) -> Generator[str, None, None]:
        """
        åŒæ­¥æµå¼è°ƒç”¨å†…éƒ¨æ¨¡å‹API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰
        Yields:
            str: æ¨¡å‹å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "messages": [{
                    "role": "user",
                    "content": prompt
                }],
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True  # å¯ç”¨æµå¼è¾“å‡º
            }

            logger.debug(
                f"Internal åŒæ­¥æµå¼APIè¯·æ±‚:\nURL: {self.base_url}/chat/completions\nData: {pprint.pformat(data)}"
            )

            # å‘é€è¯·æ±‚
            url = f"{self.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}"
            } if self.api_key != "EMPTY" else {}

            with httpx.Client(timeout=self.timeout) as client:  # ä½¿ç”¨é…ç½®çš„timeout
                with client.stream("POST", url, json=data,
                                   headers=headers) as response:
                    response.raise_for_status()

                    for line in response.iter_lines():
                        # line å·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œä¸éœ€è¦è§£ç 
                        line_str = line
                        if line_str.startswith("data: "):
                            data_str = line_str[6:]  # ç§»é™¤ "data: " å‰ç¼€
                            if data_str.strip() == "[DONE]":
                                break
                            try:
                                chunk = json.loads(data_str)
                                if "choices" in chunk and len(
                                        chunk["choices"]) > 0:
                                    delta = chunk["choices"][0].get(
                                        "delta", {})
                                    if "content" in delta and delta["content"]:
                                        yield delta["content"]
                            except json.JSONDecodeError:
                                continue

        except Exception as e:
            logger.error(f"Internal åŒæ­¥æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Internal åŒæ­¥æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}") from e

    async def astream(self, prompt: str,
                      **kwargs) -> AsyncGenerator[str, None]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨å†…éƒ¨æ¨¡å‹API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚temperature, max_tokensç­‰
        Yields:
            str: æ¨¡å‹å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            # è·å–å¯é€‰å‚æ•°
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "messages": [{
                    "role": "user",
                    "content": prompt
                }],
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True  # å¯ç”¨æµå¼è¾“å‡º
            }

            logger.debug(
                f"Internal æµå¼APIè¯·æ±‚:\nURL: {self.base_url}/chat/completions\nData: {pprint.pformat(data)}"
            )

            # å‘é€è¯·æ±‚
            url = f"{self.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}"
            } if self.api_key != "EMPTY" else {}

            async with httpx.AsyncClient(
                    timeout=self.timeout) as client:  # ä½¿ç”¨é…ç½®çš„timeout
                async with client.stream("POST",
                                         url,
                                         json=data,
                                         headers=headers) as response:
                    response.raise_for_status()

                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data_str = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                            if data_str.strip() == "[DONE]":
                                break
                            try:
                                chunk = json.loads(data_str)
                                if "choices" in chunk and len(
                                        chunk["choices"]) > 0:
                                    delta = chunk["choices"][0].get(
                                        "delta", {})
                                    if "content" in delta and delta["content"]:
                                        content = delta["content"]
                                        # è°ƒè¯•ï¼šæ£€æŸ¥å†…å®¹æ˜¯å¦ä¸º Unicode ç¼–ç 
                                        if content.startswith('\\u'):
                                            logger.debug(
                                                f"æ£€æµ‹åˆ° Unicode ç¼–ç å†…å®¹: {content}")
                                            # å°è¯•è§£ç  Unicode è½¬ä¹‰åºåˆ—
                                            try:
                                                decoded_content = content.encode(
                                                    'utf-8').decode(
                                                        'unicode_escape')
                                                logger.debug(
                                                    f"è§£ç åå†…å®¹: {decoded_content}"
                                                )
                                                yield decoded_content
                                            except Exception as e:
                                                logger.warning(
                                                    f"Unicode è§£ç å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹å†…å®¹"
                                                )
                                                yield content
                                        else:
                                            yield content
                            except json.JSONDecodeError:
                                continue

        except Exception as e:
            logger.error(f"Internal æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Internal æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}") from e


class RerankerClient(LLMClient):

    def __init__(self, base_url: str, api_key: str):
        """
        åˆå§‹åŒ–Rerankerå®¢æˆ·ç«¯
        Args:
            base_url: Reranker APIåœ°å€
            api_key: APIå¯†é’¥
        """
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key

    def invoke(self, prompt: str, **kwargs) -> dict:
        """
        è°ƒç”¨Reranker API
        Args:
            prompt: è¾“å…¥æç¤º
            **kwargs: å…¶ä»–å‚æ•°
        Returns:
            dict: é‡æ’åºç»“æœ
        """
        try:
            documents = kwargs.get("documents", [])
            doc_objs = [{"text": doc} for doc in documents]
            size = kwargs.get("size", len(doc_objs))
            data = {"query": prompt, "doc_list": doc_objs, "size": size}
            url = f"{self.base_url}"
            headers = {
                "Authorization": f"Bearer {self.api_key}"
            } if self.api_key != "EMPTY" else {}

            logger.debug(
                f"Reranker API request:\nURL: {url}\nData: {pprint.pformat(data)}"
            )

            with httpx.Client(timeout=60.0) as client:
                response = client.post(url, json=data, headers=headers)
                response.raise_for_status()
                result = response.json()
                return result
        except Exception as e:
            logger.error(f"Reranker APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Reranker APIè°ƒç”¨å¤±è´¥: {str(e)}") from e

    def stream(self, prompt: str, **kwargs) -> Generator[str, None, None]:
        """
        Rerankerå®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œè¿”å›ç©ºç”Ÿæˆå™¨
        """
        logger.warning("Rerankerå®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œè¿”å›ç©ºç»“æœ")
        return
        yield  # è¿™è¡Œæ°¸è¿œä¸ä¼šæ‰§è¡Œï¼Œåªæ˜¯ä¸ºäº†æ»¡è¶³ç±»å‹æ³¨è§£

    async def astream(self, prompt: str,
                      **kwargs) -> AsyncGenerator[str, None]:
        """
        Rerankerå®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼è¾“å‡º
        """
        raise NotImplementedError("Rerankerå®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼è¾“å‡º")
        yield  # è¿™è¡Œæ°¸è¿œä¸ä¼šæ‰§è¡Œï¼Œåªæ˜¯ä¸ºäº†æ»¡è¶³ç±»å‹æ³¨è§£


class EmbeddingClient(LLMClient):

    def __init__(self, base_url: str, api_key: str):
        """
        åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯
        Args:
            base_url: Embedding APIåœ°å€
            api_key: APIå¯†é’¥
        """
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key

    def invoke(self, prompt: str, **kwargs) -> str:
        """
        è°ƒç”¨Embedding API
        Args:
            prompt: è¾“å…¥æ–‡æœ¬
            **kwargs: å…¶ä»–å‚æ•°
        Returns:
            str: åµŒå…¥å‘é‡ï¼ˆJSONæ ¼å¼ï¼‰
        """
        try:
            # æ„å»ºè¯·æ±‚æ•°æ® - ä¿®å¤å­—æ®µå
            data = {"inputs": prompt, "model": kwargs.get("model", "gte-qwen")}

            logger.debug(
                f"Embedding API request:\nURL: {self.base_url}\nData: {pprint.pformat(data)}"
            )

            # å‘é€è¯·æ±‚ - ç›´æ¥ä½¿ç”¨æ ¹ç«¯ç‚¹ï¼Œå› ä¸ºæµ‹è¯•æ˜¾ç¤ºå®ƒå·¥ä½œæ­£å¸¸
            url = f"{self.base_url}"
            headers = {
                "Authorization": f"Bearer {self.api_key}"
            } if self.api_key != "EMPTY" else {}

            with httpx.Client(timeout=60.0) as client:
                response = client.post(url, json=data, headers=headers)
                response.raise_for_status()
                result = response.json()
                return str(result)  # è¿”å›åµŒå…¥å‘é‡

        except Exception as e:
            logger.error(f"Embedding APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Embedding APIè°ƒç”¨å¤±è´¥: {str(e)}") from e

    def stream(self, prompt: str, **kwargs) -> Generator[str, None, None]:
        """
        Embeddingå®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œè¿”å›ç©ºç”Ÿæˆå™¨
        """
        logger.warning("Embeddingå®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œè¿”å›ç©ºç»“æœ")
        return
        yield  # è¿™è¡Œæ°¸è¿œä¸ä¼šæ‰§è¡Œï¼Œåªæ˜¯ä¸ºäº†æ»¡è¶³ç±»å‹æ³¨è§£

    async def astream(self, prompt: str,
                      **kwargs) -> AsyncGenerator[str, None]:
        """
        Embeddingå®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼è¾“å‡º
        """
        raise NotImplementedError("Embeddingå®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼è¾“å‡º")
        yield  # è¿™è¡Œæ°¸è¿œä¸ä¼šæ‰§è¡Œï¼Œåªæ˜¯ä¸ºäº†æ»¡è¶³ç±»å‹æ³¨è§£


if __name__ == "__main__":
    client = InternalLLMClient(base_url="http://10.238.130.28:10004/v1",
                               api_key="EMPTY",
                               model_name="hdy_model",
                               reasoning=True)
    print(client.invoke("ä½ å¥½"))
