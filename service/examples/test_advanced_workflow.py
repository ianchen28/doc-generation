# service/examples/test_advanced_workflow.py

import asyncio
import json
import sys
import uuid
from datetime import datetime
from pathlib import Path

from doc_agent.core.logger import logger
from doc_agent.core.config import settings

# --- å¯¼å…¥æ ¸å¿ƒç»„ä»¶ ---
from doc_agent.core.container import container
from doc_agent.core.logging_config import setup_logging
from doc_agent.graph.state import ResearchState
from doc_agent.workers.tasks import (
    get_redis_client,  # æˆ‘ä»¬éœ€è¦ Redis å®¢æˆ·ç«¯æ¥æ¨¡æ‹Ÿ Worker çš„è¡Œä¸º
)

# --- åˆ›å»ºè¾“å‡ºç›®å½• ---
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = Path("output") / f"advanced_{timestamp}"
output_dir.mkdir(parents=True, exist_ok=True)
logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

# --- é…ç½®æ—¥å¿— ---
log_file = output_dir / "advanced_workflow_test.log"

# é…ç½®loguruè¾“å‡ºåˆ°æ–‡ä»¶
logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
logger.add(
    log_file,
    format=
    "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}"
)
logger.add(sys.stderr, format="{time:HH:mm:ss} | {level} | {message}")


# --- ç®€åŒ–é…ç½®å‡½æ•° ---
def setup_simple_config():
    """è®¾ç½®ç®€åŒ–çš„é…ç½®ä»¥åŠ å¿«æµ‹è¯•é€Ÿåº¦"""
    logger.info("ğŸ”§ Setting up simplified configuration for faster testing...")

    # å¼ºåˆ¶è¦†ç›–æœç´¢é…ç½®ä»¥ç¡®ä¿ç®€åŒ–æ¨¡å¼
    settings.search_config.max_search_rounds = 1
    settings.search_config.max_queries = 1
    settings.search_config.max_results_per_query = 3

    logger.info("ğŸ”§ å¼ºåˆ¶è®¾ç½®ç®€åŒ–æœç´¢é…ç½®:")
    logger.info(
        f"   - max_search_rounds: {settings.search_config.max_search_rounds}")
    logger.info(f"   - max_queries: {settings.search_config.max_queries}")
    logger.info(
        f"   - max_results_per_query: {settings.search_config.max_results_per_query}"
    )

    # ä¿®æ”¹æ–‡æ¡£ç”Ÿæˆé…ç½® - å‡å°‘ç« èŠ‚æ•°
    if hasattr(settings, 'document_generation_config'):
        # å¼ºåˆ¶è®¾ç½®ç« èŠ‚æ•°é‡
        settings.document_generation_config.document_length.chapter_count = 2
        settings.document_generation_config.document_length.chapter_target_words = 300
        settings.document_generation_config.document_length.total_target_words = 600

        logger.info("ğŸ“„ æ–‡æ¡£ç”Ÿæˆé…ç½®:")
        logger.info(
            f"   - chapter_count: {settings.document_generation_config.document_length.chapter_count}"
        )
        logger.info(
            f"   - chapter_target_words: {settings.document_generation_config.document_length.chapter_target_words}"
        )
        logger.info(
            f"   - total_target_words: {settings.document_generation_config.document_length.total_target_words}"
        )
    else:
        logger.warning("âš ï¸  document_generation_config ä¸å­˜åœ¨")

    # è®¾ç½® Agent ç»„ä»¶é…ç½®ä»¥æ§åˆ¶æœç´¢è¡Œä¸º
    if hasattr(settings, '_yaml_config') and settings._yaml_config:
        agent_config = settings._yaml_config.get('agent_config', {})
        # ç¡®ä¿æœç´¢é…ç½®è¢«æ­£ç¡®è®¾ç½®
        if 'search_config' not in agent_config:
            agent_config['search_config'] = {}
        agent_config['search_config'].update({
            'max_search_rounds': 1,
            'max_queries': 1,
            'max_results_per_query': 3
        })
        settings._yaml_config['agent_config'] = agent_config
        logger.info("âœ… Agent é…ç½®å·²æ›´æ–°")

    logger.info("ğŸ“ Using simplified prompt versions for faster processing")


def create_fast_test_config():
    """åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºå¿«é€Ÿæµ‹è¯•çš„é…ç½®"""
    return {
        "search_config": {
            "max_search_rounds": 1,
            "max_queries": 1,
            "max_results_per_query": 3
        },
        "document_generation_config": {
            "document_length": {
                "chapter_count": 2,
                "chapter_target_words": 300,
                "total_target_words": 600
            }
        },
        "agent_config": {
            "task_planner": {
                "temperature": 0.3,
                "max_tokens": 1000
            },
            "document_writer": {
                "temperature": 0.5,
                "max_tokens": 1500
            }
        }
    }


def verify_config():
    """éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½ï¼ˆä¸å†å¼ºåˆ¶è¦†ç›–ï¼Œåªè¯»å– config.yamlï¼‰"""
    logger.info("ğŸ” éªŒè¯å½“å‰é…ç½®ï¼ˆä» config.yaml åŠ è½½ï¼‰:")
    logger.info(
        f"   - search_config.max_search_rounds: {settings.search_config.max_search_rounds}"
    )
    logger.info(
        f"   - search_config.max_queries: {settings.search_config.max_queries}"
    )
    logger.info(
        f"   - search_config.max_results_per_query: {settings.search_config.max_results_per_query}"
    )

    # è·å–æ–‡æ¡£é…ç½®ï¼ˆä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼‰
    doc_config = settings.get_document_config(fast_mode=True)
    logger.info("   - å¿«é€Ÿæ¨¡å¼é…ç½®:")
    logger.info(
        f"     * chapter_count: {doc_config.get('chapter_count', 'N/A')}")
    logger.info(
        f"     * total_target_words: {doc_config.get('total_target_words', 'N/A')}"
    )
    logger.info(
        f"     * chapter_target_words: {doc_config.get('chapter_target_words', 'N/A')}"
    )
    logger.info(f"     * rerank_size: {doc_config.get('rerank_size', 'N/A')}")

    if hasattr(settings, 'document_generation_config'):
        logger.info(
            f"   - fast_test_mode.enabled: {settings.document_generation_config.fast_test_mode.enabled}"
        )
        logger.info(
            f"   - fast_test_mode.chapter_count: {settings.document_generation_config.fast_test_mode.chapter_count}"
        )

    logger.info("âœ… é…ç½®éªŒè¯å®Œæˆï¼Œæ‰€æœ‰é…ç½®éƒ½æ¥è‡ª config.yaml")


# --- æ¨¡æ‹Ÿçš„ä¸Šä¼ æ–‡ä»¶å†…å®¹ ---

# 1. é£æ ¼èŒƒä¾‹ (æ¨¡ä»¿ä¸€æ®µæ…·æ…¨æ¿€æ˜‚çš„è®²è¯ç¨¿)
STYLE_GUIDE_CONTENT = """
åŒå¿—ä»¬ï¼Œæœ‹å‹ä»¬ï¼
ä»Šå¤©ï¼Œæˆ‘ä»¬ç«™åœ¨è¿™é‡Œï¼Œä¸æ˜¯ä¸ºäº†å›é¡¾è¿‡å»çš„è¾‰ç…Œï¼Œè€Œæ˜¯ä¸ºäº†å±•æœ›æœªæ¥çš„æ— é™å¯èƒ½ï¼æˆ‘ä»¬é¢ä¸´çš„ä¸æ˜¯æŒ‘æˆ˜ï¼Œè€Œæ˜¯æœºé‡ï¼›æˆ‘ä»¬çœ‹åˆ°çš„ä¸æ˜¯å›°éš¾ï¼Œè€Œæ˜¯é€šå¾€æˆåŠŸçš„é˜¶æ¢¯ï¼
æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯åˆ›æ–°ï¼æ˜¯çªç ´ï¼æ˜¯ç”¨ç§‘æŠ€çš„åŠ›é‡ï¼Œç‚¹ç‡ƒæœªæ¥çš„ç«ç‚¬ï¼Œç…§äº®å‰è¡Œçš„é“è·¯ï¼è®©æˆ‘ä»¬æºæ‰‹å¹¶è¿›ï¼Œå…±åˆ›è¾‰ç…Œï¼
"""

# 2. éœ€æ±‚æŒ‡ä»¤ (æ˜ç¡®è¦æ±‚å¤§çº²å¿…é¡»åŒ…å«çš„å†…å®¹)
REQUIREMENTS_CONTENT = """
- æŠ¥å‘Šå¿…é¡»é¦–å…ˆå®šä¹‰ä»€ä¹ˆæ˜¯â€œå¯è§‚æµ‹æ€§â€ï¼Œå¹¶ä¸ä¼ ç»Ÿç›‘æ§è¿›è¡Œæ˜ç¡®å¯¹æ¯”ã€‚
- å¿…é¡»è¯¦ç»†åˆ†æ Prometheus çš„æ‹‰å–æ¨¡å‹ (pull-based model) çš„ä¼˜ç¼ºç‚¹ã€‚
- å¿…é¡»åŒ…å«ä¸€ä¸ªå…³äº OpenTelemetry æœªæ¥å‘å±•è¶‹åŠ¿çš„ç« èŠ‚ã€‚
- ç»“è®ºéƒ¨åˆ†å¿…é¡»ä¸ºä¸åŒè§„æ¨¡çš„ä¼ä¸šæä¾›æ˜ç¡®çš„æŠ€æœ¯é€‰å‹å»ºè®®ã€‚
"""


# 3. (å¯é€‰) å†…å®¹å‚è€ƒ
# åœ¨è¿™ä¸ªæµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è®© Agent è‡ªè¡Œç ”ç©¶ï¼Œæ‰€ä»¥å†…å®¹æ–‡ä»¶ä¸ºç©º
async def setup_test_context_in_redis(context_id: str):
    """
    æ¨¡æ‹Ÿ process_files_task Worker çš„è¡Œä¸ºï¼Œå°†é£æ ¼å’Œéœ€æ±‚å†…å®¹å­˜å…¥ Redisã€‚
    """
    logger.info(f"ğŸ”§ Simulating file processing for context_id: {context_id}")
    redis_client = await get_redis_client()
    context_key = f"context:{context_id}"

    # ä½¿ç”¨ aio-redis çš„ pipeline æ¥æ‰¹é‡æ“ä½œ
    async with redis_client.pipeline(transaction=True) as pipe:
        pipe.hset(context_key, "style_guide_content", STYLE_GUIDE_CONTENT)
        pipe.hset(context_key, "requirements_content", REQUIREMENTS_CONTENT)
        pipe.hset(context_key, "status", "READY")
        await pipe.execute()

    logger.success(
        f"âœ… Mock context created in Redis for context_id: {context_id}")
    await redis_client.aclose()


async def load_context_for_state(context_id: str) -> dict:
    """
    æ¨¡æ‹Ÿ run_main_workflow ä»»åŠ¡å¼€å§‹æ—¶çš„è¡Œä¸ºï¼Œä» Redis åŠ è½½ä¸Šä¸‹æ–‡å†…å®¹ã€‚
    """
    logger.info(f"ğŸ“¥ Loading context from Redis for context_id: {context_id}")
    redis_client = await get_redis_client()
    context_key = f"context:{context_id}"

    context_data = await redis_client.hgetall(context_key)

    # å› ä¸ºè®¾ç½®äº† decode_responses=Trueï¼Œè¿”å›çš„å·²ç»æ˜¯å­—ç¬¦ä¸²
    style_content = context_data.get('style_guide_content', '')
    requirements_content = context_data.get('requirements_content', '')

    logger.success("âœ… Context loaded successfully.")
    await redis_client.aclose()

    return {
        "style_guide_content": style_content,
        "requirements_content": requirements_content
    }


async def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè¿è¡ŒåŒ…å«å¤šç±»å‹è¾“å…¥çš„å®Œæ•´å·¥ä½œæµã€‚
    """
    setup_logging(settings)

    # éªŒè¯é…ç½®ï¼ˆä» config.yaml åŠ è½½ï¼‰
    verify_config()

    # --- 1. æ¨¡æ‹Ÿä¸Šä¸‹æ–‡åˆ›å»º ---
    # ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ context_id ç”¨äºæœ¬æ¬¡æµ‹è¯•
    test_context_id = f"ctx-{uuid.uuid4().hex}"
    await setup_test_context_in_redis(test_context_id)

    # --- 2. å‡†å¤‡åˆå§‹çŠ¶æ€ ---
    # ä½¿ç”¨åŸæ¥çš„å¤æ‚ä¸»é¢˜ä»¥æµ‹è¯•å¤šç±»å‹è¾“å…¥åŠŸèƒ½
    topic = "ä»¥'å¯è§‚æµ‹æ€§'ä¸ºæ ¸å¿ƒï¼Œå¯¹æ¯”åˆ†æ Prometheus, Zabbix å’Œ OpenTelemetry ä¸‰ç§æŠ€æœ¯æ–¹æ¡ˆåœ¨ç°ä»£äº‘åŸç”Ÿç¯å¢ƒä¸‹çš„ä¼˜ç¼ºç‚¹"

    # æ¨¡æ‹Ÿ Worker ä» Redis åŠ è½½ä¸Šä¸‹æ–‡
    context_from_redis = await load_context_for_state(test_context_id)

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    logger.info("ğŸ” ä» Redis åŠ è½½çš„ä¸Šä¸‹æ–‡æ•°æ®:")
    logger.info(
        f"   - style_guide_content é•¿åº¦: {len(context_from_redis['style_guide_content'])}"
    )
    logger.info(
        f"   - requirements_content é•¿åº¦: {len(context_from_redis['requirements_content'])}"
    )
    logger.info(
        f"   - requirements_content å†…å®¹: {context_from_redis['requirements_content'][:100]}..."
    )

    initial_state = ResearchState(
        topic=topic,
        context_id=test_context_id,
        # å°†ä» Redis åŠ è½½çš„å†…å®¹æ³¨å…¥åˆ°åˆå§‹çŠ¶æ€ä¸­
        style_guide_content=context_from_redis["style_guide_content"],
        requirements_content=context_from_redis["requirements_content"],
        # å…¶ä»–å­—æ®µä½¿ç”¨é»˜è®¤å€¼
        initial_sources=[],
        document_outline={},
        chapters_to_process=[],
        current_chapter_index=0,
        current_citation_index=0,  # æ·»åŠ å¼•ç”¨ç´¢å¼•åˆå§‹åŒ–
        completed_chapters=[],
        final_document="",
        messages=[],
    )

    # éªŒè¯åˆå§‹çŠ¶æ€
    logger.info("ğŸ” åˆå§‹çŠ¶æ€éªŒè¯:")
    logger.info(
        f"   - requirements_content åœ¨çŠ¶æ€ä¸­: {'requirements_content' in initial_state}"
    )
    logger.info(
        f"   - requirements_content å€¼: {initial_state.get('requirements_content', 'NOT_FOUND')}"
    )
    logger.info(
        f"   - requirements_content é•¿åº¦: {len(initial_state.get('requirements_content', ''))}"
    )

    logger.info("ğŸš€ Starting Advanced Workflow Test with multi-type inputs...")
    logger.info(f"   Topic: {topic}")
    logger.info(f"   Context ID: {test_context_id}")
    logger.info(f"   Log file: {log_file}")
    logger.info("   ğŸ“ Using simplified configuration for faster testing:")
    logger.info("      - Max 1 search round")
    logger.info("      - Max 1 query per research")
    logger.info("      - Max 3 results per query")
    logger.info("      - Max 2 chapters")
    logger.info("      - Max 300 words per chapter")
    print("-" * 80)

    # --- 3. æµå¼è°ƒç”¨ä¸»å›¾ ---
    # è®°å½•å·¥ä½œæµæ­¥éª¤
    workflow_steps = []
    final_result = None

    try:
        async for step_output in container.main_graph.astream(initial_state):
            node_name = list(step_output.keys())[0]
            node_data = list(step_output.values())[0]

            logger.info(f"âœ… Finished step: [ {node_name} ]")

            # è®°å½•æ­¥éª¤ä¿¡æ¯
            step_info = {
                "node_name":
                node_name,
                "timestamp":
                datetime.now().isoformat(),
                "data_keys":
                list(node_data.keys()) if isinstance(node_data, dict) else str(
                    type(node_data))
            }

            # æ·»åŠ ç‰¹å®šæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
            if node_name == "initial_research":
                if "initial_sources" in node_data:
                    step_info["sources_count"] = len(
                        node_data["initial_sources"])
                    step_info["sources_preview"] = [
                        f"[{s.id}] {s.title[:50]}..."
                        for s in node_data["initial_sources"][:3]
                    ]
            elif node_name == "outline_generation":
                if "document_outline" in node_data:
                    outline = node_data["document_outline"]
                    step_info["chapters_count"] = len(
                        outline.get("chapters", []))
                    step_info["chapters"] = [
                        ch.get("chapter_title", "")
                        for ch in outline.get("chapters", [])
                    ]
            elif "researcher" in node_name:
                if "gathered_sources" in node_data:
                    step_info["sources_count"] = len(
                        node_data["gathered_sources"])
                    step_info["sources_preview"] = [
                        f"[{s.id}] {s.title[:50]}..."
                        for s in node_data["gathered_sources"][:3]
                    ]
            elif "writer" in node_name or node_name == "chapter_processing":
                if "final_document" in node_data:
                    step_info["document_length"] = len(
                        node_data["final_document"])
                    step_info["document_preview"] = node_data[
                        "final_document"][:200] + "..."
                if "cited_sources_in_chapter" in node_data:
                    step_info["cited_sources_count"] = len(
                        node_data["cited_sources_in_chapter"])
            elif node_name == "generate_bibliography":
                if "final_document" in node_data:
                    step_info["final_document_length"] = len(
                        node_data["final_document"])
                    if "## å‚è€ƒæ–‡çŒ®" in node_data["final_document"]:
                        step_info["bibliography_added"] = True

            workflow_steps.append(step_info)
            final_result = node_data

    except Exception as e:
        logger.error(f"âŒ An error occurred during the workflow execution: {e}",
                     exception=e)
        return

    # ä¿å­˜å·¥ä½œæµæ­¥éª¤ä¿¡æ¯
    steps_file = output_dir / "advanced_workflow_steps.json"
    with open(steps_file, 'w', encoding='utf-8') as f:
        json.dump(workflow_steps, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ“Š Workflow steps saved to: {steps_file}")

    # --- 4. å±•ç¤ºæœ€ç»ˆç»“æœ ---
    logger.success("\n\nğŸ‰ WORKFLOW COMPLETED! ğŸ‰")
    print("=" * 80)
    logger.info("Final Document:")
    print("=" * 80)

    final_document_content = final_result.get("final_document")
    if final_document_content:
        # ä¿å­˜æœ€ç»ˆæ–‡æ¡£
        result_file = output_dir / "final_document.md"
        with open(result_file, 'w', encoding='utf-8') as f:
            f.write(final_document_content)

        logger.success(f"ğŸ“„ Final document saved to: {result_file}")
        logger.success(
            f"ğŸ“Š Document length: {len(final_document_content)} characters")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å‚è€ƒæ–‡çŒ®
        if "## å‚è€ƒæ–‡çŒ®" in final_document_content:
            logger.success(
                "âœ… Bibliography successfully added to final document")
        else:
            logger.warning("âš ï¸  No bibliography found in final document")

        # æ˜¾ç¤ºæ–‡æ¡£é¢„è§ˆ
        print(final_document_content[:1000] + "...")
    else:
        logger.warning("No final document was generated.")
        if final_result:
            result_file = output_dir / "final_state.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(final_result,
                          f,
                          ensure_ascii=False,
                          indent=2,
                          default=str)
            logger.info(f"ğŸ“Š Final state saved to: {result_file}")

    # åˆ†æå·¥ä½œæµæ‰§è¡Œæƒ…å†µ
    print("\nğŸ“Š Advanced Workflow Analysis:")
    research_steps = [
        step for step in workflow_steps if "researcher" in step["node_name"]
    ]
    print(f"   ğŸ” Research steps: {len(research_steps)}")

    writer_steps = [
        step for step in workflow_steps if "writer" in step["node_name"]
        or step["node_name"] == "chapter_processing"
    ]
    print(f"   âœï¸  Writer steps: {len(writer_steps)}")

    # åˆ†æå¼•ç”¨ç³»ç»Ÿ
    if final_result and "final_document" in final_result:
        if "## å‚è€ƒæ–‡çŒ®" in final_result["final_document"]:
            print("   ğŸ“š Bibliography: âœ… Added")

            # ç»Ÿè®¡å‚è€ƒæ–‡çŒ®æ•°é‡å’Œå¼•ç”¨ç¼–å·
            import re
            doc = final_result["final_document"]
            bib_start = doc.find("## å‚è€ƒæ–‡çŒ®")
            if bib_start != -1:
                bibliography = doc[bib_start:]
                citations = re.findall(r'\[(\d+)\]', bibliography)
                unique_citations = sorted({int(c) for c in citations})
                print(f"   ğŸ“– Bibliography entries: {len(unique_citations)}")
                print(f"   ğŸ”¢ Citation numbers: {unique_citations}")
        else:
            print("   ğŸ“š Bibliography: âŒ Missing")

    # æ˜¾ç¤ºæ–‡ä»¶ä½ç½®
    print("\nğŸ“ Output files:")
    print(f"   ğŸ“ Log: {log_file}")
    print(f"   ğŸ“Š Steps: {steps_file}")
    if final_result and "final_document" in final_result:
        print(f"   ğŸ“„ Document: {result_file}")

    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
