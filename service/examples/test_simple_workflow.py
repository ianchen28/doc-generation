#!/usr/bin/env python3
"""
ç®€åŒ–çš„å·¥ä½œæµæµ‹è¯•è„šæœ¬
ä½¿ç”¨ç®€å•çš„ä»»åŠ¡æµ‹è¯•æ•´ä½“æµç¨‹ï¼Œå¹¶å°†æ—¥å¿—å’Œç»“æœåˆ†åˆ«ä¿å­˜åˆ°æ–‡ä»¶
"""

import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path

from loguru import logger

# --- å¯¼å…¥æ ¸å¿ƒç»„ä»¶ ---
from doc_agent.core.config import settings
from doc_agent.core.container import container
from doc_agent.core.logging_config import setup_logging
from doc_agent.graph.state import ResearchState

# --- åˆ›å»ºè¾“å‡ºç›®å½• ---
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = Path("output") / timestamp
output_dir.mkdir(parents=True, exist_ok=True)
logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

# --- é…ç½®æ—¥å¿— ---
log_file = output_dir / "workflow_test.log"

# é…ç½®loguruè¾“å‡ºåˆ°æ–‡ä»¶
logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
logger.add(
    log_file,
    format=
    "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}"
)
logger.add(sys.stderr, format="{time:HH:mm:ss} | {level} | {message}")


# --- ç®€åŒ–é…ç½®å‡½æ•° ---
def setup_simple_config():
    """è®¾ç½®ç®€åŒ–çš„é…ç½®"""
    # é…ç½®å·²ä»YAMLæ–‡ä»¶ä¸­è¯»å–ï¼Œæ— éœ€ç¡¬ç¼–ç è®¾ç½®
    logger.info(
        f"âœ… æœç´¢é…ç½®å·²åŠ è½½: max_search_rounds={settings.search_config.max_search_rounds}"
    )
    logger.info(f"âœ… æœç´¢é…ç½®å·²åŠ è½½: max_queries={settings.search_config.max_queries}")
    logger.info(
        f"âœ… æœç´¢é…ç½®å·²åŠ è½½: max_results_per_query={settings.search_config.max_results_per_query}"
    )

    # å¼ºåˆ¶è¦†ç›–æœç´¢é…ç½®ä»¥ç¡®ä¿ç®€åŒ–æ¨¡å¼
    settings.search_config.max_search_rounds = 1
    settings.search_config.max_queries = 1
    settings.search_config.max_results_per_query = 3

    logger.info("ğŸ”§ å¼ºåˆ¶è®¾ç½®ç®€åŒ–æœç´¢é…ç½®:")
    logger.info(
        f"   - max_search_rounds: {settings.search_config.max_search_rounds}")
    logger.info(f"   - max_queries: {settings.search_config.max_queries}")
    logger.info(
        f"   - max_results_per_query: {settings.search_config.max_results_per_query}"
    )

    # ä¿®æ”¹æ–‡æ¡£ç”Ÿæˆé…ç½® - å‡å°‘ç« èŠ‚æ•°
    if hasattr(settings, 'document_generation_config'):
        # å¼ºåˆ¶è®¾ç½®ç« èŠ‚æ•°é‡
        settings.document_generation_config.document_length.chapter_count = 3
        settings.document_generation_config.document_length.chapter_target_words = 500
        settings.document_generation_config.document_length.total_target_words = 1500

        logger.info("ğŸ“„ æ–‡æ¡£ç”Ÿæˆé…ç½®:")
        logger.info(
            f"   - chapter_count: {settings.document_generation_config.document_length.chapter_count}"
        )
        logger.info(
            f"   - chapter_target_words: {settings.document_generation_config.document_length.chapter_target_words}"
        )
        logger.info(
            f"   - total_target_words: {settings.document_generation_config.document_length.total_target_words}"
        )
    else:
        logger.warning("âš ï¸  document_generation_config ä¸å­˜åœ¨")

    # è®¾ç½®ä½¿ç”¨ç®€åŒ–çš„promptç‰ˆæœ¬
    logger.info("ğŸ“ Using simplified prompt versions:")
    logger.info("   - planner: v1_simple")
    logger.info("   - outline_generation: v1_simple")
    logger.info("   - writer: v2_simple_citations")
    logger.info("   - supervisor: v1_simple")
    logger.info("   - Max 1 search round (instead of 5)")


# --- ä¸»æ‰§è¡Œå‡½æ•° ---
async def main():
    """
    ç®€åŒ–çš„ä¸»æ‰§è¡Œå‡½æ•°ï¼Œç”¨äºæµ‹è¯•å®Œæ•´çš„æ–‡æ¡£ç”Ÿæˆå·¥ä½œæµ
    """
    # é…ç½®æ—¥å¿—
    setup_logging(settings)

    # è®¾ç½®ç®€åŒ–é…ç½®
    setup_simple_config()

    # ä½¿ç”¨ç®€åŒ–çš„ä¸»é¢˜ - æ•…æ„é€‰æ‹©å¯èƒ½éœ€è¦å¤šæ¬¡æœç´¢çš„ä¸»é¢˜
    topic = "äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µ"
    genre = "simple"  # ä½¿ç”¨ç®€åŒ–çš„genreé…ç½®

    initial_state = ResearchState(
        topic=topic,
        # ä½¿ç”¨æ–°çš„Source-basedå­—æ®µ
        initial_sources=[],
        gathered_sources=[],
        cited_sources={},
        cited_sources_in_chapter=[],
        # å…¶ä»–å­—æ®µä½¿ç”¨é»˜è®¤åˆå§‹å€¼
        document_outline={},
        chapters_to_process=[],
        current_chapter_index=0,
        current_citation_index=0,  # æ·»åŠ å¼•ç”¨ç´¢å¼•åˆå§‹åŒ–
        completed_chapters_content=[],
        final_document="",
        messages=[],
    )

    logger.info("ğŸš€ Starting Simplified Workflow Test...")
    logger.info(f"   Topic: {topic}")
    logger.info(f"   Genre: {genre}")
    logger.info(f"   Log file: {log_file}")
    logger.info("   ğŸ“ Using simplified configuration:")
    logger.info("      - Max 2 search queries per research")
    logger.info("      - Max 3 results per query")
    logger.info("      - Max 3 chapters")
    logger.info("      - Max 1000 tokens per chapter")
    logger.info("      - Using simple prompt versions")
    print("-" * 80)

    # è®°å½•å·¥ä½œæµæ­¥éª¤
    workflow_steps = []
    final_result = None

    try:
        # ä½¿ç”¨ genre-aware çš„ graph
        graph = container.get_graph_runnable_for_job("test-job", genre)

        async for step_output in graph.astream(initial_state):
            # step_output çš„æ ¼å¼æ˜¯ { "node_name": {"state_key": value} }
            node_name = list(step_output.keys())[0]
            node_data = list(step_output.values())[0]

            logger.info(f"âœ… Finished step: [ {node_name} ]")

            # è®°å½•æ­¥éª¤ä¿¡æ¯
            step_info = {
                "node_name":
                node_name,
                "timestamp":
                datetime.now().isoformat(),
                "data_keys":
                list(node_data.keys()) if isinstance(node_data, dict) else str(
                    type(node_data))
            }

            # æ·»åŠ ç‰¹å®šæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
            if node_name == "initial_research":
                if "initial_sources" in node_data:
                    step_info["sources_count"] = len(
                        node_data["initial_sources"])
                    step_info["sources_preview"] = [
                        f"[{s.id}] {s.title[:50]}..."
                        for s in node_data["initial_sources"][:3]
                    ]

            elif node_name == "outline_generation":
                if "document_outline" in node_data:
                    outline = node_data["document_outline"]
                    step_info["chapters_count"] = len(
                        outline.get("chapters", []))
                    step_info["chapters"] = [
                        ch.get("chapter_title", "")
                        for ch in outline.get("chapters", [])
                    ]

            elif node_name == "reflector":
                if "search_queries" in node_data:
                    step_info["new_queries"] = node_data["search_queries"]
                    step_info["reflection_triggered"] = True

            elif "researcher" in node_name:
                if "gathered_sources" in node_data:
                    step_info["sources_count"] = len(
                        node_data["gathered_sources"])
                    step_info["sources_preview"] = [
                        f"[{s.id}] {s.title[:50]}..."
                        for s in node_data["gathered_sources"][:3]
                    ]

            elif "writer" in node_name:
                if "final_document" in node_data:
                    step_info["document_length"] = len(
                        node_data["final_document"])
                    step_info["document_preview"] = node_data[
                        "final_document"][:200] + "..."
                if "cited_sources_in_chapter" in node_data:
                    step_info["cited_sources_count"] = len(
                        node_data["cited_sources_in_chapter"])
                    # æ·»åŠ å¼•ç”¨æºé¢„è§ˆ
                    if isinstance(node_data["cited_sources_in_chapter"], list):
                        step_info["cited_sources_preview"] = [
                            f"[{s.id}] {s.title[:30]}..."
                            for s in node_data["cited_sources_in_chapter"][:3]
                        ]

            elif node_name == "generate_bibliography":
                if "final_document" in node_data:
                    step_info["final_document_length"] = len(
                        node_data["final_document"])
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å‚è€ƒæ–‡çŒ®
                    if "## å‚è€ƒæ–‡çŒ®" in node_data["final_document"]:
                        step_info["bibliography_added"] = True
                        # æå–å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
                        doc = node_data["final_document"]
                        bib_start = doc.find("## å‚è€ƒæ–‡çŒ®")
                        if bib_start != -1:
                            bibliography = doc[bib_start:]
                            step_info[
                                "bibliography_preview"] = bibliography[:300] + "..."
                            # ç»Ÿè®¡å‚è€ƒæ–‡çŒ®æ•°é‡
                            import re
                            citations = re.findall(r'\[(\d+)\]', bibliography)
                            step_info["bibliography_count"] = len(
                                set(citations))

                # æ·»åŠ å…¨å±€å¼•ç”¨æºç»Ÿè®¡
                if "cited_sources" in node_data:
                    step_info["global_cited_sources_count"] = len(
                        node_data["cited_sources"])

            workflow_steps.append(step_info)
            final_result = node_data

    except Exception as e:
        logger.error(f"âŒ An error occurred during the workflow execution: {e}",
                     exception=e)
        return

    # ä¿å­˜å·¥ä½œæµæ­¥éª¤ä¿¡æ¯
    steps_file = output_dir / "workflow_steps.json"
    with open(steps_file, 'w', encoding='utf-8') as f:
        json.dump(workflow_steps, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ“Š Workflow steps saved to: {steps_file}")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    if final_result and "final_document" in final_result:
        result_file = output_dir / "final_document.md"
        with open(result_file, 'w', encoding='utf-8') as f:
            f.write(final_result["final_document"])

        logger.success(f"ğŸ“„ Final document saved to: {result_file}")
        logger.success(
            f"ğŸ“Š Document length: {len(final_result['final_document'])} characters"
        )

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å‚è€ƒæ–‡çŒ®
        if "## å‚è€ƒæ–‡çŒ®" in final_result["final_document"]:
            logger.success(
                "âœ… Bibliography successfully added to final document")
        else:
            logger.warning("âš ï¸  No bibliography found in final document")

        # æ˜¾ç¤ºæ–‡æ¡£é¢„è§ˆ
        print("\n" + "=" * 80)
        print("ğŸ“„ FINAL DOCUMENT PREVIEW")
        print("=" * 80)
        print(final_result["final_document"][:1000] + "...")
        print("=" * 80)

    else:
        logger.warning("No final document was generated.")
        if final_result:
            result_file = output_dir / "final_state.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(final_result,
                          f,
                          ensure_ascii=False,
                          indent=2,
                          default=str)
            logger.info(f"ğŸ“Š Final state saved to: {result_file}")

    # åˆ†æå·¥ä½œæµæ‰§è¡Œæƒ…å†µ
    print("\nğŸ“Š Workflow Analysis:")
    reflection_count = sum(1 for step in workflow_steps
                           if step.get("reflection_triggered", False))
    print(f"   ğŸ”„ Reflection triggered: {reflection_count} times")

    research_steps = [
        step for step in workflow_steps if "researcher" in step["node_name"]
    ]
    print(f"   ğŸ” Research steps: {len(research_steps)}")

    writer_steps = [
        step for step in workflow_steps if "writer" in step["node_name"]
    ]
    print(f"   âœï¸  Writer steps: {len(writer_steps)}")

    # åˆ†æå¼•ç”¨ç³»ç»Ÿ
    if final_result and "final_document" in final_result:
        if "## å‚è€ƒæ–‡çŒ®" in final_result["final_document"]:
            print("   ğŸ“š Bibliography: âœ… Added")

            # ç»Ÿè®¡å‚è€ƒæ–‡çŒ®æ•°é‡å’Œå¼•ç”¨ç¼–å·
            import re
            doc = final_result["final_document"]
            bib_start = doc.find("## å‚è€ƒæ–‡çŒ®")
            if bib_start != -1:
                bibliography = doc[bib_start:]
                citations = re.findall(r'\[(\d+)\]', bibliography)
                unique_citations = sorted({int(c) for c in citations})
                print(f"   ğŸ“– Bibliography entries: {len(unique_citations)}")
                print(f"   ğŸ”¢ Citation numbers: {unique_citations}")

                # æ£€æŸ¥ç¼–å·æ˜¯å¦è¿ç»­
                if unique_citations == list(range(1,
                                                  len(unique_citations) + 1)):
                    print("   âœ… Citation numbering: Consecutive")
                else:
                    print("   âŒ Citation numbering: Not consecutive")

            # ç»Ÿè®¡å…¨æ–‡ä¸­çš„å¼•ç”¨
            content_before_bib = doc[:bib_start] if bib_start != -1 else doc
            content_citations = re.findall(r'\[(\d+)\]', content_before_bib)
            unique_content_citations = sorted(
                {int(c)
                 for c in content_citations})
            print(f"   ğŸ“ In-text citations: {unique_content_citations}")

        else:
            print("   ğŸ“š Bibliography: âŒ Missing")

    # ç»Ÿè®¡å…¨å±€å¼•ç”¨æº
    bibliography_steps = [
        step for step in workflow_steps
        if step["node_name"] == "generate_bibliography"
    ]
    if bibliography_steps:
        last_bib_step = bibliography_steps[-1]
        if "global_cited_sources_count" in last_bib_step:
            print(
                f"   ğŸŒ Global sources tracked: {last_bib_step['global_cited_sources_count']}"
            )
        if "bibliography_count" in last_bib_step:
            print(
                f"   ğŸ“‹ Bibliography count: {last_bib_step['bibliography_count']}"
            )

    # æ˜¾ç¤ºæ–‡ä»¶ä½ç½®
    print("\nğŸ“ Output files:")
    print(f"   ğŸ“ Log: {log_file}")
    print(f"   ğŸ“Š Steps: {steps_file}")
    if final_result and "final_document" in final_result:
        print(f"   ğŸ“„ Document: {result_file}")


if __name__ == "__main__":
    # ä½¿ç”¨ asyncio.run() æ¥æ‰§è¡Œæˆ‘ä»¬çš„å¼‚æ­¥ main å‡½æ•°
    asyncio.run(main())
