"""
åæ€èŠ‚ç‚¹æ¨¡å—

è´Ÿè´£åˆ†æç°æœ‰æ•°æ®å¹¶ç”Ÿæˆæ›´ç²¾ç¡®çš„æœç´¢æŸ¥è¯¢
"""

from pprint import pformat as pprint
from typing import Any

from doc_agent.core.logging_config import get_logger

logger = get_logger(__name__)

from doc_agent.common.prompt_selector import PromptSelector
from doc_agent.core.config import settings
from doc_agent.graph.common import (
    format_sources_to_text as _format_sources_to_text, )
from doc_agent.graph.common import (
    parse_reflection_response as _parse_reflection_response, )
from doc_agent.graph.state import ResearchState
from doc_agent.llm_clients.base import LLMClient


async def reflection_node(state: ResearchState,
                          llm_client: LLMClient,
                          prompt_selector: PromptSelector,
                          genre: str = "default") -> dict[str, Any]:
    """
    æ™ºèƒ½æŸ¥è¯¢æ‰©å±•èŠ‚ç‚¹
    åˆ†æç°æœ‰çš„æœç´¢æŸ¥è¯¢å’Œå·²æ”¶é›†çš„æ•°æ®ï¼Œç”Ÿæˆæ›´ç²¾ç¡®ã€æ›´ç›¸å…³çš„æœç´¢æŸ¥è¯¢

    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topicã€search_queries å’Œ gathered_data
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        prompt_selector: PromptSelectorå®ä¾‹ï¼Œç”¨äºè·å–promptæ¨¡æ¿
        genre: genreç±»å‹ï¼Œé»˜è®¤ä¸º"default"

    Returns:
        dict: åŒ…å«æ›´æ–°åçš„ search_queries çš„å­—å…¸
    """
    # ä»çŠ¶æ€ä¸­è·å–å¿…è¦ä¿¡æ¯
    topic = state.get("topic", "")
    original_search_queries = state.get("search_queries", [])
    gathered_data = state.get("gathered_data", "")
    gathered_sources = state.get("gathered_sources", [])
    current_chapter_index = state.get("current_chapter_index", 0)
    chapters_to_process = state.get("chapters_to_process", [])

    # è·å–å½“å‰ç« èŠ‚ä¿¡æ¯
    current_chapter = None
    if current_chapter_index < len(chapters_to_process):
        current_chapter = chapters_to_process[current_chapter_index]

    chapter_title = current_chapter.get("chapter_title",
                                        "") if current_chapter else ""
    chapter_description = current_chapter.get("description",
                                              "") if current_chapter else ""

    # ä¼˜å…ˆä½¿ç”¨ gathered_sources çš„æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ gathered_data
    if gathered_sources and not gathered_data:
        gathered_data = _format_sources_to_text(gathered_sources)
        logger.info(
            f"ğŸ“Š ä» gathered_sources è½¬æ¢ä¸º gathered_dataï¼Œé•¿åº¦: {len(gathered_data)} å­—ç¬¦"
        )

    # è·å–å¤æ‚åº¦é…ç½®
    complexity_config = settings.get_complexity_config()
    logger.info(f"ğŸ”§ ä½¿ç”¨å¤æ‚åº¦çº§åˆ«: {complexity_config['level']}")

    logger.info("ğŸ¤” å¼€å§‹æ™ºèƒ½æŸ¥è¯¢æ‰©å±•åˆ†æ")
    logger.info(f"ğŸ“‹ ç« èŠ‚: {chapter_title}")
    logger.info(f"ğŸ” åŸå§‹æŸ¥è¯¢æ•°é‡: {len(original_search_queries)}")
    logger.info(f"ğŸ“Š å·²æ”¶é›†æ•°æ®é•¿åº¦: {len(gathered_data)} å­—ç¬¦")
    logger.info(f"ğŸ“š å·²æ”¶é›†æºæ•°é‡: {len(gathered_sources)}")

    # éªŒè¯è¾“å…¥æ•°æ®
    if not topic:
        logger.warning("âŒ ç¼ºå°‘ä¸»é¢˜ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡ŒæŸ¥è¯¢æ‰©å±•")
        return {"search_queries": original_search_queries}

    if not original_search_queries:
        logger.warning("âŒ æ²¡æœ‰åŸå§‹æŸ¥è¯¢ï¼Œæ— æ³•è¿›è¡Œæ‰©å±•")
        return {"search_queries": []}

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†æ
    has_sufficient_data = ((gathered_data and len(gathered_data.strip()) >= 50)
                           or (gathered_sources and len(gathered_sources) > 0))

    if not has_sufficient_data:
        logger.warning("âŒ æ”¶é›†çš„æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ")
        return {"search_queries": original_search_queries}

    # è·å–æŸ¥è¯¢æ‰©å±•å™¨é…ç½®
    query_expander_config = settings.get_agent_component_config(
        "query_expander")
    if not query_expander_config:
        temperature = 0.7
        max_tokens = 2000
        extra_params = {}
    else:
        temperature = query_expander_config.temperature
        max_tokens = query_expander_config.max_tokens
        extra_params = query_expander_config.extra_params

    # æ ¹æ®å¤æ‚åº¦è°ƒæ•´å‚æ•°
    timeout = complexity_config.get('llm_timeout', 180)

    # è·å–æç¤ºè¯æ¨¡æ¿
    prompt_template = _get_prompt_template(prompt_selector, genre,
                                           complexity_config)

    # å‡†å¤‡æ•°æ®æ‘˜è¦ï¼ˆé¿å…promptè¿‡é•¿ï¼‰
    gathered_data_summary = gathered_data
    if len(gathered_data) > 3000:
        gathered_data_summary = gathered_data[:
                                              1500] + "\n\n... (æ•°æ®å·²æˆªæ–­) ...\n\n" + gathered_data[
                                                  -1500:]

    # æ ¼å¼åŒ–åŸå§‹æŸ¥è¯¢
    original_queries_text = "\n".join(
        [f"{i+1}. {query}" for i, query in enumerate(original_search_queries)])

    # æ„å»º prompt
    prompt = prompt_template.format(
        topic=topic,
        chapter_title=chapter_title,
        chapter_description=chapter_description,
        original_queries=original_queries_text,
        gathered_data_summary=gathered_data_summary)

    logger.debug(f"Invoking LLM with reflection prompt:\n{pprint(prompt)}")

    try:
        # è°ƒç”¨ LLM ç”Ÿæˆæ–°çš„æŸ¥è¯¢
        response = llm_client.invoke(prompt,
                                     temperature=temperature,
                                     max_tokens=max_tokens,
                                     **extra_params)

        logger.debug(f"ğŸ” LLMåŸå§‹å“åº”: {repr(response)}")
        logger.debug(f"ğŸ“ å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")

        # è§£æå“åº”ï¼Œæå–æ–°çš„æŸ¥è¯¢
        new_queries = _parse_reflection_response(response)

        if new_queries:
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(new_queries)} ä¸ªæ–°æŸ¥è¯¢")
            for i, query in enumerate(new_queries, 1):
                logger.debug(f"  {i}. {query}")

            # æ ¹æ®å¤æ‚åº¦é™åˆ¶æŸ¥è¯¢æ•°é‡
            max_queries = complexity_config.get('chapter_search_queries', 3)
            if len(new_queries) > max_queries:
                new_queries = new_queries[:max_queries]
                logger.info(f"ğŸ“Š é™åˆ¶æŸ¥è¯¢æ•°é‡åˆ° {max_queries} ä¸ª")

            # è¿”å›æ›´æ–°åçš„æŸ¥è¯¢åˆ—è¡¨
            return {"search_queries": new_queries}
        else:
            logger.warning("âš ï¸ æ— æ³•è§£ææ–°æŸ¥è¯¢ï¼Œä¿æŒåŸå§‹æŸ¥è¯¢")
            return {"search_queries": original_search_queries}

    except Exception as e:
        logger.error(f"Reflection node error: {str(e)}")
        logger.warning("âš ï¸ æŸ¥è¯¢æ‰©å±•å¤±è´¥ï¼Œä¿æŒåŸå§‹æŸ¥è¯¢")
        return {"search_queries": original_search_queries}


def _get_prompt_template(prompt_selector, genre, complexity_config):
    """è·å–åˆé€‚çš„æç¤ºè¯æ¨¡æ¿"""
    try:
        # æ ¹æ®å¤æ‚åº¦å†³å®šæ˜¯å¦ä½¿ç”¨ç®€åŒ–æç¤ºè¯
        if complexity_config['use_simplified_prompts']:
            # å¿«é€Ÿæ¨¡å¼ä½¿ç”¨ç®€åŒ–çš„æç¤ºè¯
            return _get_simplified_prompt_template()

        # æ ‡å‡†æ¨¡å¼ä½¿ç”¨å®Œæ•´æç¤ºè¯
        prompt_template = prompt_selector.get_prompt("chapter_workflow",
                                                     "reflection", genre)
        logger.debug(f"âœ… æˆåŠŸè·å– reflection prompt æ¨¡æ¿ï¼Œgenre: {genre}")
        return prompt_template

    except Exception as e:
        logger.error(f"âŒ è·å– reflection prompt æ¨¡æ¿å¤±è´¥: {e}")
        # ä½¿ç”¨ prompts/reflection.py ä¸­çš„å¤‡ç”¨æ¨¡æ¿
        try:
            from doc_agent.prompts.reflection import PROMPTS
            prompt_template = PROMPTS.get("v1_fallback", PROMPTS["v1_default"])
            logger.debug("âœ… æˆåŠŸè·å– reflection å¤‡ç”¨æ¨¡æ¿")
            return prompt_template
        except Exception as e2:
            logger.error(f"âŒ è·å– reflection å¤‡ç”¨æ¨¡æ¿ä¹Ÿå¤±è´¥: {e2}")
            # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
            return _get_fallback_prompt_template()


def _get_simplified_prompt_template() -> str:
    """è·å–ç®€åŒ–çš„æç¤ºè¯æ¨¡æ¿ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰"""
    return """
ä½ æ˜¯ç ”ç©¶ä¸“å®¶ã€‚åŸºäºå·²æ”¶é›†çš„æ•°æ®ï¼Œç”Ÿæˆ2ä¸ªæ›´ç²¾ç¡®çš„æœç´¢æŸ¥è¯¢ã€‚

**ä¸»é¢˜:** {topic}
**ç« èŠ‚:** {chapter_title}

**åŸå§‹æŸ¥è¯¢:**
{original_queries}

**å·²æ”¶é›†æ•°æ®æ‘˜è¦:**
{gathered_data_summary}

**è¾“å‡ºJSONæ ¼å¼:**
{{
  "new_queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2"],
  "reasoning": "ç®€è¦è¯´æ˜"
}}
"""


def _get_fallback_prompt_template() -> str:
    """è·å–å¤‡ç”¨çš„æç¤ºè¯æ¨¡æ¿"""
    return """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶ä¸“å®¶å’ŒæŸ¥è¯¢ä¼˜åŒ–å¸ˆã€‚è¯·åˆ†æç°æœ‰çš„æœç´¢æŸ¥è¯¢å’Œå·²æ”¶é›†çš„æ•°æ®ï¼Œç”Ÿæˆæ›´ç²¾ç¡®ã€æ›´ç›¸å…³çš„æœç´¢æŸ¥è¯¢ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}

**å½“å‰ç« èŠ‚ä¿¡æ¯:**
- ç« èŠ‚æ ‡é¢˜: {chapter_title}
- ç« èŠ‚æè¿°: {chapter_description}

**åŸå§‹æœç´¢æŸ¥è¯¢:**
{original_queries}

**å·²æ”¶é›†çš„æ•°æ®æ‘˜è¦:**
{gathered_data_summary}

**ä»»åŠ¡è¦æ±‚:**
1. ä»”ç»†åˆ†æå·²æ”¶é›†çš„æ•°æ®ï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£ã€æ¨¡ç³Šä¹‹å¤„æˆ–æ–°çš„æœ‰è¶£æ–¹å‘
2. è€ƒè™‘åŸå§‹æŸ¥è¯¢çš„è¦†ç›–èŒƒå›´å’Œæ·±åº¦
3. ç”Ÿæˆ2-3ä¸ªæ–°çš„ã€é«˜åº¦ç›¸å…³çš„ã€æ›´å…·ä½“çš„æˆ–æ¢ç´¢æ€§çš„æœç´¢æŸ¥è¯¢
4. æ–°æŸ¥è¯¢åº”è¯¥ï¼š
   - å¡«è¡¥ä¿¡æ¯ç¼ºå£
   - æ·±å…¥ç‰¹å®šæ–¹é¢
   - æ¢ç´¢æ–°çš„è§’åº¦æˆ–è§†è§’
   - ä½¿ç”¨æ›´ç²¾ç¡®çš„å…³é”®è¯

**è¾“å‡ºæ ¼å¼:**
è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- new_queries: æ–°çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼ˆæ•°ç»„ï¼Œ2-3ä¸ªæŸ¥è¯¢ï¼‰
- reasoning: ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ–°æŸ¥è¯¢

è¯·ç«‹å³å¼€å§‹åˆ†æå¹¶ç”Ÿæˆæ–°çš„æŸ¥è¯¢ã€‚
"""
