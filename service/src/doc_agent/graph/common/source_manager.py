"""
æºç®¡ç†æ¨¡å—

æä¾›æºï¼ˆSourceï¼‰å¯¹è±¡çš„ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
- æºIDçš„è·å–æˆ–åˆ›å»º
- æºåˆ—è¡¨çš„åˆå¹¶ä¸å»é‡
"""

from doc_agent.core.logger import logger

from doc_agent.schemas import Source


def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºå‰100ä¸ªå­—ç¬¦ï¼‰
    
    Args:
        text1: ç¬¬ä¸€æ®µæ–‡æœ¬
        text2: ç¬¬äºŒæ®µæ–‡æœ¬
        
    Returns:
        float: ç›¸ä¼¼åº¦ç™¾åˆ†æ¯” (0-100)
    """
    if not text1 or not text2:
        return 0.0

    # å–å‰100ä¸ªå­—ç¬¦è¿›è¡Œæ¯”è¾ƒ
    text1_preview = text1[:100].strip()
    text2_preview = text2[:100].strip()

    if not text1_preview or not text2_preview:
        return 0.0

    # è®¡ç®—å…¬å…±å­—ç¬¦æ•°ï¼ˆè€ƒè™‘å­—ç¬¦ä½ç½®ï¼‰
    min_len = min(len(text1_preview), len(text2_preview))
    if min_len == 0:
        return 0.0

    # è®¡ç®—å­—ç¬¦çº§ç›¸ä¼¼åº¦
    common_chars = 0
    for i in range(min_len):
        if text1_preview[i] == text2_preview[i]:
            common_chars += 1

    # è®¡ç®—ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”
    similarity = (common_chars / min_len) * 100

    # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè¿›ä¸€æ­¥æ£€æŸ¥å…³é”®è¯åŒ¹é…
    if similarity > 80.0:
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        keywords1 = set(text1_preview.split())
        keywords2 = set(text2_preview.split())

        if keywords1 and keywords2:
            common_keywords = keywords1.intersection(keywords2)
            keyword_similarity = len(common_keywords) / max(
                len(keywords1), len(keywords2)) * 100

            # ç»¼åˆå­—ç¬¦ç›¸ä¼¼åº¦å’Œå…³é”®è¯ç›¸ä¼¼åº¦
            final_similarity = (similarity + keyword_similarity) / 2
            return final_similarity

    return similarity


def get_or_create_source_id(new_source: Source,
                            existing_sources: list[Source]) -> int:
    """
    è·å–æˆ–åˆ›å»ºä¿¡æºIDï¼Œé¿å…é‡å¤å¼•ç”¨
    
    Args:
        new_source: æ–°çš„ä¿¡æºå¯¹è±¡
        existing_sources: å·²çŸ¥çš„ä¿¡æºåˆ—è¡¨
        
    Returns:
        int: ä¿¡æºçš„IDï¼ˆå¦‚æœæ‰¾åˆ°é‡å¤çš„è¿”å›ç°æœ‰IDï¼Œå¦åˆ™è¿”å›æ–°IDï¼‰
    """
    if not existing_sources:
        # å¦‚æœæ²¡æœ‰ç°æœ‰ä¿¡æºï¼Œç›´æ¥è¿”å›æ–°ä¿¡æºçš„ID
        return new_source.id

    # æ£€æŸ¥URLåŒ¹é…
    if new_source.url:
        for existing_source in existing_sources:
            if existing_source.url and existing_source.url == new_source.url:
                logger.debug(
                    f"ğŸ”— é€šè¿‡URLåŒ¹é…æ‰¾åˆ°é‡å¤ä¿¡æº: [{existing_source.id}] {existing_source.title}"
                )
                return existing_source.id

    # æ£€æŸ¥å†…å®¹ç›¸ä¼¼åº¦
    for existing_source in existing_sources:
        similarity = calculate_text_similarity(new_source.content,
                                               existing_source.content)
        if similarity > 95.0:  # ç›¸ä¼¼åº¦é˜ˆå€¼95%
            logger.debug(
                f"ğŸ“„ é€šè¿‡å†…å®¹ç›¸ä¼¼åº¦åŒ¹é…æ‰¾åˆ°é‡å¤ä¿¡æº: [{existing_source.id}] {existing_source.title} (ç›¸ä¼¼åº¦: {similarity:.1f}%)"
            )
            return existing_source.id

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é‡å¤ï¼Œè¿”å›æ–°ä¿¡æºçš„ID
    logger.debug(f"ğŸ†• æœªæ‰¾åˆ°é‡å¤ä¿¡æºï¼Œä½¿ç”¨æ–°ID: [{new_source.id}] {new_source.title}")
    return new_source.id


def merge_sources_with_deduplication(
        new_sources: list[Source],
        existing_sources: list[Source]) -> list[Source]:
    """
    åˆå¹¶ä¿¡æºåˆ—è¡¨ï¼Œå»é™¤é‡å¤é¡¹
    
    Args:
        new_sources: æ–°çš„ä¿¡æºåˆ—è¡¨
        existing_sources: ç°æœ‰çš„ä¿¡æºåˆ—è¡¨
        
    Returns:
        list[Source]: å»é‡åçš„ä¿¡æºåˆ—è¡¨
    """
    if not new_sources:
        return existing_sources

    if not existing_sources:
        return new_sources

    # åˆ›å»ºç°æœ‰ä¿¡æºçš„æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
    existing_source_map = {source.id: source for source in existing_sources}
    merged_sources = existing_sources.copy()

    for new_source in new_sources:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒIDçš„ä¿¡æº
        if new_source.id in existing_source_map:
            logger.debug(f"ğŸ”„ è·³è¿‡é‡å¤IDçš„ä¿¡æº: [{new_source.id}] {new_source.title}")
            continue

        # æ£€æŸ¥URLå’Œå†…å®¹é‡å¤
        is_duplicate = False
        for existing_source in existing_sources:
            # URLåŒ¹é…æ£€æŸ¥
            if new_source.url and existing_source.url and new_source.url == existing_source.url:
                logger.debug(
                    f"ğŸ”— è·³è¿‡URLé‡å¤çš„ä¿¡æº: [{new_source.id}] {new_source.title}")
                is_duplicate = True
                break

            # å†…å®¹ç›¸ä¼¼åº¦æ£€æŸ¥
            similarity = calculate_text_similarity(new_source.content,
                                                   existing_source.content)
            if similarity > 95.0:
                logger.debug(
                    f"ğŸ“„ è·³è¿‡å†…å®¹é‡å¤çš„ä¿¡æº: [{new_source.id}] {new_source.title} (ç›¸ä¼¼åº¦: {similarity:.1f}%)"
                )
                is_duplicate = True
                break

        if not is_duplicate:
            merged_sources.append(new_source)
            logger.debug(f"âœ… æ·»åŠ æ–°ä¿¡æº: [{new_source.id}] {new_source.title}")

    logger.info(
        f"ğŸ”„ ä¿¡æºåˆå¹¶å®Œæˆ: åŸæœ‰ {len(existing_sources)} ä¸ªï¼Œæ–°å¢ {len(new_sources)} ä¸ªï¼Œåˆå¹¶å {len(merged_sources)} ä¸ª"
    )
    return merged_sources
