"""
è§„åˆ’èŠ‚ç‚¹æ¨¡å—

è´Ÿè´£ä¸ºç« èŠ‚ç”Ÿæˆç ”ç©¶è®¡åˆ’å’Œæœç´¢æŸ¥è¯¢
"""

from pprint import pformat as pprint
from typing import Any

from doc_agent.core.logging_config import get_logger

logger = get_logger(__name__)

from doc_agent.common.prompt_selector import PromptSelector
from doc_agent.core.config import settings
from doc_agent.graph.callbacks import publish_event
from doc_agent.graph.common import parse_planner_response
from doc_agent.graph.state import ResearchState
from doc_agent.llm_clients.base import LLMClient


def planner_node(state: ResearchState,
                 llm_client: LLMClient,
                 prompt_selector: PromptSelector,
                 genre: str = "default") -> dict[str, Any]:
    """
    èŠ‚ç‚¹1: è§„åˆ’ç ”ç©¶æ­¥éª¤
    ä»çŠ¶æ€ä¸­è·å– topic å’Œå½“å‰ç« èŠ‚ä¿¡æ¯ï¼Œåˆ›å»º prompt è°ƒç”¨ LLM ç”Ÿæˆç ”ç©¶è®¡åˆ’å’Œæœç´¢æŸ¥è¯¢
    
    Args:
        state: ç ”ç©¶çŠ¶æ€ï¼ŒåŒ…å« topic å’Œå½“å‰ç« èŠ‚ä¿¡æ¯
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        prompt_selector: PromptSelectorå®ä¾‹ï¼Œç”¨äºè·å–promptæ¨¡æ¿
        genre: genreç±»å‹ï¼Œé»˜è®¤ä¸º"default"
        
    Returns:
        dict: åŒ…å« research_plan å’Œ search_queries çš„å­—å…¸
    """
    topic = state.get("topic", "")
    job_id = state.get("job_id", "")
    chapters_to_process = state.get("chapters_to_process", [])
    current_chapter_index = state.get("current_chapter_index", 0)

    if not topic:
        raise ValueError("Topic is required in state")

    # è·å–å½“å‰ç« èŠ‚ä¿¡æ¯
    current_chapter = None
    if current_chapter_index < len(chapters_to_process):
        current_chapter = chapters_to_process[current_chapter_index]

    chapter_title = current_chapter.get("chapter_title",
                                        "") if current_chapter else ""
    chapter_description = current_chapter.get("description",
                                              "") if current_chapter else ""
    chapter_word_count = current_chapter.get("chapter_word_count")

    # è·å–å­èŠ‚ä¿¡æ¯
    sub_sections = current_chapter.get("sub_sections",
                                       []) if current_chapter else []

    logger.info(f"ğŸ“‹ è§„åˆ’ç« èŠ‚ç ”ç©¶: {chapter_title}")
    logger.info(f"ğŸ“ ç« èŠ‚æè¿°: {chapter_description}")
    logger.info(f"ğŸ“Š å­èŠ‚æ•°é‡: {len(sub_sections)}")

    # æ ¼å¼åŒ–å­èŠ‚ä¿¡æ¯
    sub_sections_text = ""
    if sub_sections:
        sub_sections_text = "\n\nå½“å‰ç« èŠ‚çš„å­èŠ‚ç»“æ„ï¼š\n"
        for sub_section in sub_sections:
            section_number = sub_section.get("section_number", "?")
            section_title = sub_section.get("section_title", "æœªå‘½åå­èŠ‚")
            section_description = sub_section.get("section_description", "")

            sub_sections_text += f"\n{section_number} {section_title}\n"
            if section_description:
                sub_sections_text += f"æè¿°: {section_description}\n"

    # è·å–å¤æ‚åº¦é…ç½®
    complexity_config = settings.get_complexity_config()
    logger.info(f"ğŸ”§ ä½¿ç”¨å¤æ‚åº¦çº§åˆ«: {complexity_config['level']}")

    # æ ¹æ®å¤æ‚åº¦é€‰æ‹© prompt
    if complexity_config['use_simplified_prompts']:
        # ä½¿ç”¨å¿«é€Ÿæç¤ºè¯ - ç°åœ¨ä»promptsæ¨¡å—è·å–
        from doc_agent.prompts.planner import V3_FAST
        prompt_template = V3_FAST
    else:
        # ä½¿ç”¨æ ‡å‡†æç¤ºè¯
        from doc_agent.prompts.planner import V1_DEFAULT
        prompt_template = V1_DEFAULT

    # è·å–ä»»åŠ¡è§„åˆ’å™¨é…ç½®
    task_planner_config = settings.get_agent_component_config("task_planner")
    if not task_planner_config:
        raise ValueError("Task planner configuration not found")

    # åˆ›å»ºç ”ç©¶è®¡åˆ’ç”Ÿæˆçš„ promptï¼Œè¦æ±‚ JSON æ ¼å¼å“åº”
    prompt = prompt_template.format(topic=topic,
                                    chapter_title=chapter_title,
                                    chapter_description=chapter_description,
                                    sub_sections_text=sub_sections_text)

    logger.debug(f"Invoking LLM with prompt:\n{pprint(prompt)}")

    try:
        # è°ƒç”¨ LLM ç”Ÿæˆç ”ç©¶è®¡åˆ’
        # æ ¹æ®å¤æ‚åº¦é…ç½®è°ƒæ•´è¶…æ—¶æ—¶é—´
        timeout = complexity_config.get('llm_timeout',
                                        task_planner_config.timeout)
        max_retries = complexity_config.get('max_retries', 5)

        response = llm_client.invoke(
            prompt,
            temperature=task_planner_config.temperature,
            max_tokens=task_planner_config.max_tokens,
            **task_planner_config.extra_params)

        logger.debug(f"ğŸ” LLMåŸå§‹å“åº”: {repr(response)}")
        logger.debug(f"ğŸ“ å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        logger.info(f"ğŸ” LLMå“åº”å†…å®¹:\n{response}")

        # è§£æ JSON å“åº”
        research_plan, search_queries = parse_planner_response(response)

        publish_event(
            job_id, "ç« èŠ‚è§„åˆ’", "document_generation", "SUCCESS", {
                "research_plan": research_plan,
                "search_queries": search_queries,
                "description": f"æ”¶é›† {chapter_title} ç›¸å…³ä¿¡æ¯"
            })

        # åº”ç”¨åŸºäºå¤æ‚åº¦çš„æŸ¥è¯¢æ•°é‡é™åˆ¶
        max_queries = complexity_config.get(
            'chapter_search_queries', complexity_config.get('max_queries', 5))
        logger.info(f"ğŸ“Š planner_node å½“å‰æŸ¥è¯¢æ•°é‡é…ç½®: {max_queries}")

        if len(search_queries) > max_queries:
            logger.info(f"ğŸ“Š é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡: {len(search_queries)} -> {max_queries}")
            search_queries = search_queries[:max_queries]

        logger.info(f"âœ… ç”Ÿæˆç ”ç©¶è®¡åˆ’: {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
        for i, query in enumerate(search_queries, 1):
            logger.debug(f"  {i}. {query}")

        # è¿”å›å®Œæ•´çš„çŠ¶æ€æ›´æ–°
        result = {
            "research_plan": research_plan,
            "search_queries": search_queries
        }
        logger.debug(f"ğŸ“¤ PlannerèŠ‚ç‚¹è¿”å›ç»“æœ: {pprint(result)}")
        return result

    except Exception as e:
        # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤è®¡åˆ’
        logger.error(f"Planner node error: {str(e)}")

        # æ ¹æ®å¤æ‚åº¦ç”Ÿæˆé»˜è®¤æŸ¥è¯¢
        if complexity_config['level'] == 'fast':
            default_queries = [
                f"{topic} {chapter_title} æ¦‚è¿°", f"{topic} {chapter_title} ä¸»è¦å†…å®¹"
            ]
        else:
            default_queries = [
                f"{topic} {chapter_title} æ¦‚è¿°", f"{topic} {chapter_title} ä¸»è¦å†…å®¹",
                f"{topic} {chapter_title} å…³é”®è¦ç‚¹",
                f"{topic} {chapter_title} æœ€æ–°å‘å±•", f"{topic} {chapter_title} é‡è¦æ€§"
            ]

        logger.warning(f"âš ï¸  ä½¿ç”¨é»˜è®¤æœç´¢æŸ¥è¯¢: {len(default_queries)} ä¸ª")
        for i, query in enumerate(default_queries, 1):
            logger.debug(f"  {i}. {query}")

        result = {
            "research_plan": f"ç ”ç©¶è®¡åˆ’ï¼šå¯¹ç« èŠ‚ {chapter_title} è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼Œæ”¶é›†ç›¸å…³ä¿¡æ¯å¹¶æ•´ç†æˆæ–‡æ¡£ã€‚",
            "search_queries": default_queries
        }
        logger.debug(f"ğŸ“¤ PlannerèŠ‚ç‚¹è¿”å›é»˜è®¤ç»“æœ: {pprint(result)}")
        return result


def _get_fallback_prompt_template() -> str:
    """è·å–å¤‡ç”¨çš„æç¤ºè¯æ¨¡æ¿"""
    return """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ç« èŠ‚åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’å’Œæœç´¢ç­–ç•¥ã€‚

**æ–‡æ¡£ä¸»é¢˜:** {topic}

**å½“å‰ç« èŠ‚ä¿¡æ¯:**
- ç« èŠ‚æ ‡é¢˜: {chapter_title}
- ç« èŠ‚æè¿°: {chapter_description}

**ä»»åŠ¡è¦æ±‚:**
1. åˆ†æç« èŠ‚ä¸»é¢˜ï¼Œç¡®å®šç ”ç©¶é‡ç‚¹å’Œæ–¹å‘
2. åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’ï¼ŒåŒ…æ‹¬ç ”ç©¶æ­¥éª¤å’Œæ–¹æ³•
3. ç”Ÿæˆ5-8ä¸ªé«˜è´¨é‡çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºæ”¶é›†ç›¸å…³ä¿¡æ¯
4. ç¡®ä¿æœç´¢æŸ¥è¯¢å…·æœ‰é’ˆå¯¹æ€§å’Œå…¨é¢æ€§

**è¾“å‡ºæ ¼å¼:**
è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- research_plan: è¯¦ç»†çš„ç ”ç©¶è®¡åˆ’
- search_queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰

è¯·ç«‹å³å¼€å§‹åˆ¶å®šç ”ç©¶è®¡åˆ’ã€‚
"""
