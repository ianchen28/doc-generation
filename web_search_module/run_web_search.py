#!/usr/bin/env python3
"""
Webæœç´¢æ¨¡å—å¿«é€Ÿå¯åŠ¨è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
python run_web_search.py [æŸ¥è¯¢å…³é”®è¯] [--full-content]

ç¤ºä¾‹:
python run_web_search.py "æ°´è½®æœº"
python run_web_search.py "æ°´åˆ©å·¥ç¨‹" --full-content
"""

import asyncio
import sys
import os
import json

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from web_search_module import WebSearchService, setup_logger


def print_usage():
    """æ‰“å°ä½¿ç”¨è¯´æ˜"""
    print("""
Webæœç´¢æ¨¡å—å¿«é€Ÿå¯åŠ¨è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python run_web_search.py [æŸ¥è¯¢å…³é”®è¯] [--full-content]

ç¤ºä¾‹:
    python run_web_search.py "æ°´è½®æœº"
    python run_web_search.py "æ°´åˆ©å·¥ç¨‹" --full-content

å‚æ•°:
    [æŸ¥è¯¢å…³é”®è¯] - è¦æœç´¢çš„å…³é”®è¯ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤å…³é”®è¯
    --full-content - è·å–ç½‘é¡µçš„å®Œæ•´å†…å®¹ï¼ˆå¯é€‰ï¼‰

é…ç½®æ–‡ä»¶:
    è¯·ç¡®ä¿ config.json æ–‡ä»¶å­˜åœ¨å¹¶åŒ…å«æ­£ç¡®çš„é…ç½®ä¿¡æ¯
    """)


async def search_and_display(query: str, fetch_full_content: bool = False):
    """æ‰§è¡Œæœç´¢å¹¶æ˜¾ç¤ºç»“æœ"""
    # è®¾ç½®æ—¥å¿—
    setup_logger()

    print(f"ğŸ” æ­£åœ¨æœç´¢: {query}")
    if fetch_full_content:
        print("ğŸ“„ å°†è·å–ç½‘é¡µå®Œæ•´å†…å®¹")
    print("=" * 60)

    try:
        # åˆ›å»ºwebæœç´¢æœåŠ¡
        web_search_service = WebSearchService()

        # æ‰§è¡Œæœç´¢
        result = await web_search_service.get_web_docs(
            query, fetch_full_content=fetch_full_content)

        if result:
            print(f"âœ… æœç´¢æˆåŠŸï¼æ‰¾åˆ° {len(result)} ä¸ªç»“æœ:")
            print()

            for i, doc in enumerate(result, 1):
                print(f"ğŸ“„ ç»“æœ {i}:")
                print(f"   æ ‡é¢˜: {doc['meta_data'].get('docName', 'Unknown')}")
                print(f"   URL: {doc['doc_id']}")
                print(f"   æ’å: {doc['rank']}")
                print(f"   å†…å®¹é•¿åº¦: {len(doc['text'])} å­—ç¬¦")
                print(f"   æ˜¯å¦è·å–å®Œæ•´å†…å®¹: {doc.get('full_content_fetched', False)}")

                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                content_preview = doc['text'][:] + "..." if len(
                    doc['text']) > 300 else doc['text']
                print(f"   å†…å®¹é¢„è§ˆ: {content_preview}")
                print(f"   å‘é‡ç»´åº¦: {len(doc['context_vector'])}")
                print("-" * 60)
        else:
            print("âŒ æœç´¢å¤±è´¥æˆ–æ— ç»“æœ")

    except Exception as e:
        print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. é…ç½®æ–‡ä»¶ config.json æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
        print("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. APIæœåŠ¡æ˜¯å¦å¯ç”¨")


async def fetch_single_url(url: str):
    """è·å–å•ä¸ªURLçš„å®Œæ•´å†…å®¹"""
    print(f"ğŸ” æ­£åœ¨è·å–URLå†…å®¹: {url}")
    print("=" * 60)

    try:
        # åˆ›å»ºwebæœç´¢æœåŠ¡
        web_search_service = WebSearchService()

        # è·å–å®Œæ•´å†…å®¹
        content = await web_search_service.get_full_content_for_url(url)

        if content:
            print(f"âœ… æˆåŠŸè·å–å†…å®¹ï¼")
            print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   å†…å®¹é¢„è§ˆ: {content[:500]}...")

            # ä¿å­˜åˆ°æ–‡ä»¶
            filename = f"content_{url.replace('://', '_').replace('/', '_').replace('.', '_')}.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"   å®Œæ•´å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
        else:
            print("âŒ è·å–å†…å®¹å¤±è´¥")

    except Exception as e:
        print(f"âŒ è·å–å†…å®¹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


def check_config_file():
    """æ£€æŸ¥é…ç½®æ–‡ä»¶"""
    config_files = ["config.json", "standalone_config.json"]

    for config_file in config_files:
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)

                web_params = config.get('web_search_params', {})
                embedding_config = config.get('embedding', {})

                if web_params.get('url') and web_params.get('token'):
                    print(f"âœ… é…ç½®æ–‡ä»¶ {config_file} æ£€æŸ¥é€šè¿‡")
                    return True
                else:
                    print(f"âš ï¸  é…ç½®æ–‡ä»¶ {config_file} ä¸­ç¼ºå°‘å¿…è¦çš„é…ç½®é¡¹")

            except Exception as e:
                print(f"âŒ é…ç½®æ–‡ä»¶ {config_file} è§£æå¤±è´¥: {e}")

    print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„é…ç½®æ–‡ä»¶")
    print("è¯·ç¡®ä¿ config.json æˆ– standalone_config.json æ–‡ä»¶å­˜åœ¨ä¸”åŒ…å«æ­£ç¡®çš„é…ç½®")
    return False


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Webæœç´¢æ¨¡å—å¿«é€Ÿå¯åŠ¨")
    print("=" * 60)

    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not check_config_file():
        return

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = sys.argv[1:]
    fetch_full_content = False
    query = None
    single_url = None

    i = 0
    while i < len(args):
        if args[i] == "--full-content":
            fetch_full_content = True
            i += 1
        elif args[i] == "--url" and i + 1 < len(args):
            single_url = args[i + 1]
            i += 2
        elif args[i] in ["-h", "--help", "help"]:
            print_usage()
            return
        else:
            query = args[i]
            i += 1

    # å¦‚æœæ²¡æœ‰æä¾›æŸ¥è¯¢å…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not query and not single_url:
        query = "æ°´è½®æœº"  # é»˜è®¤æŸ¥è¯¢
        print(f"æœªæä¾›æŸ¥è¯¢å…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯: {query}")

    # æ‰§è¡Œæ“ä½œ
    if single_url:
        await fetch_single_url(single_url)
    else:
        await search_and_display(query, fetch_full_content)


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] in ["-h", "--help", "help"]:
        print_usage()
    else:
        asyncio.run(main())
