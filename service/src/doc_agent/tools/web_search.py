# service/src/doc_agent/tools/web_search.py
"""
ç½‘ç»œæœç´¢å·¥å…·
åŸºäºå¤–éƒ¨æœç´¢APIå’Œç½‘é¡µæŠ“å–åŠŸèƒ½
"""

import asyncio
import functools
import re
import time
from typing import Any, Optional

import aiohttp
from bs4 import BeautifulSoup
from doc_agent.core.logger import logger


def timer(func=None, *, log_level="info"):
    """è®¡ç®—å‡½æ•°æ‰§è¡Œæ—¶é—´çš„è£…é¥°å™¨"""

    def decorator(func):

        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            elapsed_time = end_time - start_time
            log_func = getattr(logger, log_level)
            log_func(f"å‡½æ•° {func.__name__} æ‰§è¡Œè€—æ—¶: {elapsed_time:.4f} ç§’")
            return result

        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            result = await func(*args, **kwargs)
            end_time = time.time()
            elapsed_time = end_time - start_time
            log_func = getattr(logger, log_level)
            log_func(f"å¼‚æ­¥å‡½æ•° {func.__name__} æ‰§è¡Œè€—æ—¶: {elapsed_time:.4f} ç§’")
            return result

        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        return sync_wrapper

    if func is None:
        return decorator
    return decorator(func)


class WebScraper:
    """ç½‘é¡µå†…å®¹æŠ“å–å™¨"""

    def __init__(self):
        self.logger = logger.bind(name="web_scraper")

    async def fetch_full_content(self,
                                 url: str,
                                 timeout: int = 10) -> Optional[str]:
        """
        å¼‚æ­¥è·å–ç½‘é¡µå®Œæ•´å†…å®¹

        Args:
            url: ç½‘é¡µURL
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            ç½‘é¡µçš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            timeout_obj = aiohttp.ClientTimeout(total=timeout)
            async with aiohttp.ClientSession(timeout=timeout_obj) as session:
                async with session.get(url) as response:
                    response.raise_for_status()
                    html = await response.text()
                    return self.extract_text_from_html(html)
        except Exception as e:
            self.logger.error(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥ {url}: {e}")
            return None

    def extract_text_from_html(self, html: str) -> str:
        """
        ä»HTMLä¸­æå–æ–‡æœ¬å†…å®¹

        Args:
            html: HTMLå­—ç¬¦ä¸²

        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')

            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()

            # è·å–æ–‡æœ¬
            text = soup.get_text()

            # æ¸…ç†æ–‡æœ¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines
                      for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)

            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            text = re.sub(r'\s+', ' ', text)

            return text
        except Exception as e:
            self.logger.error(f"HTMLæ–‡æœ¬æå–å¤±è´¥: {e}")
            return ""


import os


class WebSearchConfig:
    """ç½‘ç»œæœç´¢é…ç½®"""

    def __init__(self, config: dict[str, Any] = None):
        # ç¡¬ç¼–ç é…ç½®
        default_config = {
            "url": "http://10.215.149.74:9930/api/v1/llm-qa/api/chat/net",
            "token":
            "eyJhbGciOiJIUzI1NiJ9.eyJqd3RfbmFtZSI6Iueul-azleiBlOe9keaOpeWPo-a1i-ivlSIsImp3dF91c2VyX2lkIjoyMiwiand0X3VzZXJfbmFtZSI6ImFkbWluIiwiZXhwIjoyMDA1OTc2MjY2LCJpYXQiOjE3NDY3NzYyNjZ9.YLkrXAdx-wyVUwWveVCF2ddjqZrOrwOKxaF8fLOuc6E",
            "count": 5,
            "timeout": 15,
            "retries": 3,
            "delay": 1,
            "fetch_full_content": True
        }

        # åˆå¹¶é…ç½®
        if config:
            default_config.update(config)

        self.url = default_config["url"]
        self.token = default_config["token"]
        self.count = default_config["count"]
        self.timeout = default_config["timeout"]
        self.retries = default_config["retries"]
        self.delay = default_config["delay"]
        self.fetch_full_content = default_config["fetch_full_content"]


class WebSearchTool:
    """
    ç½‘ç»œæœç´¢å·¥å…·ç±»
    æ”¯æŒå¼‚æ­¥æœç´¢ã€ç½‘é¡µæŠ“å–ã€é‡è¯•æœºåˆ¶
    """

    def __init__(self,
                 api_key: Optional[str] = None,
                 config: dict[str, Any] = None):
        """
        åˆå§‹åŒ–ç½‘ç»œæœç´¢å·¥å…·

        Args:
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œç”¨äºå…¼å®¹æ€§ï¼‰
            config: é…ç½®å­—å…¸
        """
        self.config = WebSearchConfig(config)
        self.web_scraper = WebScraper()
        self.logger = logger.bind(name="web_search")

        logger.info("åˆå§‹åŒ–ç½‘ç»œæœç´¢å·¥å…·")
        if api_key:
            logger.debug("APIå¯†é’¥å·²æä¾›")
        else:
            logger.warning("æœªæä¾›APIå¯†é’¥ï¼Œå°†ä½¿ç”¨é…ç½®ä¸­çš„token")

    @timer(log_level="info")
    async def get_web_search(self,
                             query: str) -> Optional[list[dict[str, Any]]]:
        """
        å¼‚æ­¥è¯·æ±‚å¤–éƒ¨æœç´¢æ¥å£å¹¶è¿”å›ç»“æœ

        Args:
            query: æŸ¥è¯¢å‚æ•°

        Returns:
            å¦‚æœè¯·æ±‚æˆåŠŸï¼Œè¿”å›å“åº”çš„æ•°æ®ï¼›å¦åˆ™è¿”å›None
        """
        headers = {"X-API-KEY-AUTH": f"Bearer {self.config.token}"}
        params = {"queryStr": query, "count": self.config.count}

        # åˆ›å»ºaiohttpè¶…æ—¶å¯¹è±¡
        timeout_obj = aiohttp.ClientTimeout(total=self.config.timeout)

        async with aiohttp.ClientSession(timeout=timeout_obj) as session:
            for attempt in range(1, self.config.retries + 1):
                try:
                    async with session.get(self.config.url,
                                           headers=headers,
                                           params=params) as response:
                        response.raise_for_status()
                        data = await response.json()

                        # æ£€æŸ¥APIå“åº”çŠ¶æ€
                        if isinstance(data, dict):
                            # å¦‚æœAPIè¿”å›é”™è¯¯çŠ¶æ€ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯å¹¶è¿”å›None
                            if data.get('status') is False:
                                error_msg = data.get('message', 'æœªçŸ¥é”™è¯¯')
                                error_code = data.get('code', 'æœªçŸ¥é”™è¯¯ç ')
                                self.logger.error(
                                    f"APIè¿”å›é”™è¯¯: {error_msg} (é”™è¯¯ç : {error_code})"
                                )
                                return None

                            # å¦‚æœAPIè¿”å›æˆåŠŸçŠ¶æ€ï¼Œè¿”å›æ•°æ®
                            if data.get('status') is True:
                                return data.get("data", [])

                            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„çŠ¶æ€å­—æ®µï¼Œå°è¯•ç›´æ¥è·å–data
                            if "data" in data:
                                return data.get("data", [])

                        # å¦‚æœå“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œè®°å½•è­¦å‘Š
                        self.logger.warning(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {data}")
                        return data.get("data", []) if isinstance(
                            data, dict) else []

                except Exception as e:
                    self.logger.error(f"ç¬¬ {attempt} æ¬¡è¯·æ±‚å¤±è´¥: {e}")
                    if attempt < self.config.retries:
                        await asyncio.sleep(self.config.delay)
                    else:
                        self.logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯·æ±‚å¤±è´¥")
                        return None

    async def get_web_docs(
            self,
            query: str,
            fetch_full_content: bool = True) -> list[dict[str, Any]]:
        """
        è·å–webæœç´¢ç»“æœå¹¶æ ¼å¼åŒ–ä¸ºæ–‡æ¡£æ ¼å¼

        Args:
            query: æŸ¥è¯¢å‚æ•°
            fetch_full_content: æ˜¯å¦è·å–å®Œæ•´å†…å®¹ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®

        Returns:
            æ ¼å¼åŒ–çš„æ–‡æ¡£åˆ—è¡¨
        """
        net_info = await self.get_web_search(query)
        if not net_info:
            return []

        # å¤„ç†æ¯ä¸ªæœç´¢ç»“æœ
        web_docs = []
        for index, web_page in enumerate(net_info):
            # è·å–å†…å®¹
            content = web_page.get('materialContent', '')

            # å¦‚æœéœ€è¦è·å–å®Œæ•´å†…å®¹ä¸”å½“å‰å†…å®¹è¾ƒçŸ­ï¼ˆå°‘äº200å­—ç¬¦ï¼‰
            if fetch_full_content and len(content) < 200 and web_page.get(
                    'url'):
                self.logger.info(f"è·å–å®Œæ•´å†…å®¹: {web_page.get('url')}")
                try:
                    full_content = await self.web_scraper.fetch_full_content(
                        web_page.get('url'))
                    if full_content:
                        content = full_content
                        web_page['full_content_fetched'] = True
                    else:
                        web_page['full_content_fetched'] = False
                except Exception as e:
                    self.logger.warning(f"è·å–å®Œæ•´å†…å®¹å¤±è´¥: {e}")
                    web_page['full_content_fetched'] = False
            else:
                web_page['full_content_fetched'] = False

            # æ ¼å¼åŒ–æ–‡æ¡£
            web_page["file_name"] = web_page.get("docName", "")
            doc = {
                "url": web_page.get("url", ""),
                "doc_id": web_page.get("file_name", ""),
                "doc_type": "text",
                "domain_ids": ["web"],
                "meta_data": web_page,
                "text": content,
                "_id": web_page.get("materialId", ""),
                "rank": str(index + 1),
                "full_content_fetched": web_page.get('full_content_fetched',
                                                     False)
            }
            web_docs.append(doc)

        return web_docs

    async def get_full_content_for_url(self, url: str) -> Optional[str]:
        """
        è·å–æŒ‡å®šURLçš„å®Œæ•´å†…å®¹

        Args:
            url: ç½‘é¡µURL

        Returns:
            ç½‘é¡µçš„å®Œæ•´æ–‡æœ¬å†…å®¹
        """
        return await self.web_scraper.fetch_full_content(url)

    def search(self, query: str) -> str:
        """
        åŒæ­¥æœç´¢æ¥å£ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            str: æœç´¢ç»“æœçš„æ–‡æœ¬è¡¨ç¤º
        """
        logger.info(f"å¼€å§‹ç½‘ç»œæœç´¢ï¼ŒæŸ¥è¯¢: '{query[:50]}...'")

        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­
            try:
                loop = asyncio.get_running_loop()
                # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œè¿”å›æç¤ºä¿¡æ¯
                logger.warning("åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è°ƒç”¨åŒæ­¥æœç´¢æ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨ search_async")
                return f"æœç´¢æŸ¥è¯¢: {query}\næ³¨æ„: åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¯·ä½¿ç”¨ search_async æ–¹æ³•"
            except RuntimeError:
                # ä¸åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨ run_until_complete
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    web_docs = loop.run_until_complete(
                        self.get_web_docs(query))
                finally:
                    loop.close()

            if not web_docs:
                return f"æœç´¢å¤±è´¥æˆ–æ— ç»“æœ: {query}"

            # æ ¼å¼åŒ–ç»“æœ
            result = f"Search results for: {query}\n\n"
            for i, doc in enumerate(web_docs[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
                meta = doc['meta_data']
                title = meta.get('docName', 'Unknown')
                url = doc['doc_id']
                content = doc['text'][:200] + "..." if len(
                    doc['text']) > 200 else doc['text']

                result += f"{i+1}. {title}\n"
                result += f"   URL: {url}\n"
                result += f"   å†…å®¹é•¿åº¦: {len(doc['text'])} å­—ç¬¦\n"
                result += f"   æ˜¯å¦è·å–å®Œæ•´å†…å®¹: {doc.get('full_content_fetched', False)}\n"
                result += f"   å†…å®¹é¢„è§ˆ: {content}\n\n"

            logger.info("ç½‘ç»œæœç´¢å®Œæˆ")
            return result

        except Exception as e:
            logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            return f"æœç´¢å¤±è´¥: {str(e)}"

    async def search_async(self,
                           query: str,
                           fetch_full_content: bool = True) -> str:
        """
        å¼‚æ­¥æœç´¢æ¥å£

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            str: æœç´¢ç»“æœçš„æ–‡æœ¬è¡¨ç¤º
        """
        logger.info(f"å¼€å§‹å¼‚æ­¥ç½‘ç»œæœç´¢ï¼ŒæŸ¥è¯¢: '{query[:50]}...'")

        try:
            web_docs = await self.get_web_docs(query, fetch_full_content)

            if not web_docs:
                return f"æœç´¢å¤±è´¥æˆ–æ— ç»“æœ: {query}"

            # æ ¼å¼åŒ–ç»“æœ
            result = f"Search results for: {query}\n\n"
            for i, doc in enumerate(web_docs):
                meta = doc['meta_data']
                title = meta.get('docName', 'Unknown')
                url = doc['url']
                content = doc['text'][:200] + "..." if len(
                    doc['text']) > 200 else doc['text']

                result += f"{i+1}. {title}\n"
                result += f"   URL: {url}\n"
                result += f"   å†…å®¹é•¿åº¦: {len(doc['text'])} å­—ç¬¦\n"
                result += f"   æ˜¯å¦è·å–å®Œæ•´å†…å®¹: {doc.get('full_content_fetched', False)}\n"
                result += f"   å†…å®¹é¢„è§ˆ: {content}\n\n"

            logger.info("å¼‚æ­¥ç½‘ç»œæœç´¢å®Œæˆ")
            return web_docs, result

        except Exception as e:
            logger.error(f"å¼‚æ­¥ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            return f"æœç´¢å¤±è´¥: {str(e)}"


if __name__ == "__main__":
    """
    ç½‘ç»œæœç´¢å·¥å…·æµ‹è¯•ä»£ç 
    æµ‹è¯•åŒæ­¥å’Œå¼‚æ­¥æœç´¢åŠŸèƒ½
    """
    import asyncio

    from doc_agent.core.logger import logger
    # é…ç½®æ—¥å¿—
    logger.add("logs/web_search_test.log",
               rotation="1 day",
               retention="7 days")

    async def test_async_search():
        """æµ‹è¯•å¼‚æ­¥æœç´¢åŠŸèƒ½"""
        logger.info("=== å¼€å§‹å¼‚æ­¥æœç´¢æµ‹è¯• ===")

        # åˆ›å»ºæœç´¢å·¥å…·å®ä¾‹
        web_search = WebSearchTool()

        # æµ‹è¯•æŸ¥è¯¢
        test_queries = ["Python å¼‚æ­¥ç¼–ç¨‹", "äººå·¥æ™ºèƒ½å‘å±•", "æœºå™¨å­¦ä¹ ç®—æ³•"]

        for query in test_queries:
            logger.info(f"æµ‹è¯•æŸ¥è¯¢: {query}")
            try:
                # æµ‹è¯•å¼‚æ­¥æœç´¢
                result = await web_search.search_async(query)
                logger.info(f"å¼‚æ­¥æœç´¢ç»“æœ:\n{result}")

                # æµ‹è¯•è·å–æ–‡æ¡£
                web_docs = await web_search.get_web_docs(query)
                logger.info(f"è·å–åˆ° {len(web_docs)} ä¸ªæ–‡æ¡£")

                # æ˜¾ç¤ºæ–‡æ¡£è¯¦æƒ…
                for i, doc in enumerate(web_docs[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
                    logger.info(f"æ–‡æ¡£ {i+1}:")
                    logger.info(f"  ID: {doc.get('_id', 'N/A')}")
                    logger.info(f"  URL: {doc.get('doc_id', 'N/A')}")
                    logger.info(
                        f"  æ ‡é¢˜: {doc.get('meta_data', {}).get('docName', 'N/A')}"
                    )
                    logger.info(f"  å†…å®¹é•¿åº¦: {len(doc.get('text', ''))} å­—ç¬¦")
                    logger.info(
                        f"  æ˜¯å¦è·å–å®Œæ•´å†…å®¹: {doc.get('full_content_fetched', False)}"
                    )

            except Exception as e:
                logger.error(f"å¼‚æ­¥æœç´¢æµ‹è¯•å¤±è´¥: {e}")

            logger.info("-" * 50)

    def test_sync_search():
        """æµ‹è¯•åŒæ­¥æœç´¢åŠŸèƒ½"""
        logger.info("=== å¼€å§‹åŒæ­¥æœç´¢æµ‹è¯• ===")

        # åˆ›å»ºæœç´¢å·¥å…·å®ä¾‹
        web_search = WebSearchTool()

        # æµ‹è¯•æŸ¥è¯¢
        test_query = "Python ç¼–ç¨‹æ•™ç¨‹"
        logger.info(f"æµ‹è¯•æŸ¥è¯¢: {test_query}")

        try:
            # æµ‹è¯•åŒæ­¥æœç´¢
            result = web_search.search(test_query)
            logger.info(f"åŒæ­¥æœç´¢ç»“æœ:\n{result}")

        except Exception as e:
            logger.error(f"åŒæ­¥æœç´¢æµ‹è¯•å¤±è´¥: {e}")

        logger.info("-" * 50)

    async def test_web_scraper():
        """æµ‹è¯•ç½‘é¡µæŠ“å–åŠŸèƒ½"""
        logger.info("=== å¼€å§‹ç½‘é¡µæŠ“å–æµ‹è¯• ===")

        # åˆ›å»ºç½‘é¡µæŠ“å–å™¨
        scraper = WebScraper()

        # æµ‹è¯•URL
        test_urls = ["https://www.python.org", "https://docs.python.org/3/"]

        for url in test_urls:
            logger.info(f"æµ‹è¯•æŠ“å–: {url}")
            try:
                content = await scraper.fetch_full_content(url)
                if content:
                    logger.info(f"æŠ“å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    logger.info(f"å†…å®¹é¢„è§ˆ: {content[:200]}...")
                else:
                    logger.warning("æŠ“å–å¤±è´¥ï¼Œè¿”å›ç©ºå†…å®¹")
            except Exception as e:
                logger.error(f"ç½‘é¡µæŠ“å–æµ‹è¯•å¤±è´¥: {e}")

            logger.info("-" * 30)

    async def test_config():
        """æµ‹è¯•é…ç½®åŠŸèƒ½"""
        logger.info("=== å¼€å§‹é…ç½®æµ‹è¯• ===")

        # æµ‹è¯•é»˜è®¤é…ç½®
        default_config = WebSearchConfig()
        logger.info(f"é»˜è®¤é…ç½®:")
        logger.info(f"  URL: {default_config.url}")
        logger.info(f"  Count: {default_config.count}")
        logger.info(f"  Timeout: {default_config.timeout}")
        logger.info(f"  Retries: {default_config.retries}")
        logger.info(f"  Delay: {default_config.delay}")
        logger.info(
            f"  Fetch Full Content: {default_config.fetch_full_content}")

        # æµ‹è¯•è‡ªå®šä¹‰é…ç½®
        custom_config = WebSearchConfig({
            "count": 3,
            "timeout": 10,
            "retries": 2,
            "delay": 0.5
        })
        logger.info(f"è‡ªå®šä¹‰é…ç½®:")
        logger.info(f"  Count: {custom_config.count}")
        logger.info(f"  Timeout: {custom_config.timeout}")
        logger.info(f"  Retries: {custom_config.retries}")
        logger.info(f"  Delay: {custom_config.delay}")

        logger.info("-" * 50)

    async def test_detailed_search():
        """è¯¦ç»†æµ‹è¯•æœç´¢åŠŸèƒ½ï¼ŒåŒ…å«è¯·æ±‚è¯¦æƒ…"""
        logger.info("=== å¼€å§‹è¯¦ç»†æœç´¢æµ‹è¯• ===")

        # åˆ›å»ºæœç´¢å·¥å…·å®ä¾‹
        web_search = WebSearchTool()

        # æµ‹è¯•æŸ¥è¯¢
        test_query = "Python ç¼–ç¨‹"
        logger.info(f"æµ‹è¯•æŸ¥è¯¢: {test_query}")

        try:
            # ç›´æ¥æµ‹è¯• get_web_search æ–¹æ³•
            logger.info("1. æµ‹è¯• get_web_search æ–¹æ³•")
            raw_results = await web_search.get_web_search(test_query)

            if raw_results is None:
                logger.error("get_web_search è¿”å› Noneï¼Œå¯èƒ½çš„åŸå› :")
                logger.error("  - API æœåŠ¡ä¸å¯ç”¨")
                logger.error("  - ç½‘ç»œè¿æ¥é—®é¢˜")
                logger.error("  - Token æ— æ•ˆæˆ–è¿‡æœŸ")
                logger.error("  - URL é…ç½®é”™è¯¯")
                return

            logger.info(f"åŸå§‹æœç´¢ç»“æœæ•°é‡: {len(raw_results)}")

            # æ˜¾ç¤ºåŸå§‹ç»“æœè¯¦æƒ…
            for i, result in enumerate(raw_results[:2]):
                logger.info(f"åŸå§‹ç»“æœ {i+1}:")
                logger.info(f"  materialId: {result.get('materialId', 'N/A')}")
                logger.info(f"  docName: {result.get('docName', 'N/A')}")
                logger.info(f"  url: {result.get('url', 'N/A')}")
                logger.info(
                    f"  materialContent é•¿åº¦: {len(result.get('materialContent', ''))}"
                )
                logger.info(f"  å®Œæ•´å†…å®¹: {result}")

            # æµ‹è¯• get_web_docs æ–¹æ³•
            logger.info("2. æµ‹è¯• get_web_docs æ–¹æ³•")
            web_docs = await web_search.get_web_docs(test_query)
            logger.info(f"æ ¼å¼åŒ–æ–‡æ¡£æ•°é‡: {len(web_docs)}")

            # æ˜¾ç¤ºæ ¼å¼åŒ–æ–‡æ¡£è¯¦æƒ…
            for i, doc in enumerate(web_docs[:2]):
                logger.info(f"æ ¼å¼åŒ–æ–‡æ¡£ {i+1}:")
                logger.info(f"  doc_id: {doc.get('doc_id', 'N/A')}")
                logger.info(f"  _id: {doc.get('_id', 'N/A')}")
                logger.info(f"  rank: {doc.get('rank', 'N/A')}")
                logger.info(f"  text é•¿åº¦: {len(doc.get('text', ''))}")
                logger.info(
                    f"  full_content_fetched: {doc.get('full_content_fetched', False)}"
                )
                logger.info(f"  meta_data: {doc.get('meta_data', {})}")

        except Exception as e:
            logger.error(f"è¯¦ç»†æœç´¢æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

        logger.info("-" * 50)

    async def test_network_connectivity():
        """æµ‹è¯•ç½‘ç»œè¿æ¥æ€§"""
        logger.info("=== å¼€å§‹ç½‘ç»œè¿æ¥æµ‹è¯• ===")

        import aiohttp

        # æµ‹è¯•é…ç½®
        config = WebSearchConfig()
        test_url = config.url
        headers = {"X-API-KEY-AUTH": f"Bearer {config.token}"}
        params = {"queryStr": "test", "count": 1}

        logger.info(f"æµ‹è¯•URL: {test_url}")
        logger.info(f"è¯·æ±‚å¤´: {headers}")
        logger.info(f"è¯·æ±‚å‚æ•°: {params}")

        try:
            timeout_obj = aiohttp.ClientTimeout(total=10)
            async with aiohttp.ClientSession(timeout=timeout_obj) as session:
                async with session.get(test_url,
                                       headers=headers,
                                       params=params) as response:
                    logger.info(f"å“åº”çŠ¶æ€ç : {response.status}")
                    logger.info(f"å“åº”å¤´: {dict(response.headers)}")

                    if response.status == 200:
                        try:
                            data = await response.json()
                            logger.info(f"å“åº”æ•°æ®: {data}")
                        except Exception as e:
                            logger.error(f"è§£æå“åº”JSONå¤±è´¥: {e}")
                            text = await response.text()
                            logger.info(f"å“åº”æ–‡æœ¬: {text[:500]}...")
                    else:
                        logger.error(f"HTTPé”™è¯¯: {response.status}")
                        text = await response.text()
                        logger.error(f"é”™è¯¯å“åº”: {text}")

        except Exception as e:
            logger.error(f"ç½‘ç»œè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

        logger.info("-" * 50)

    async def test_api_diagnosis():
        """APIè¯Šæ–­æµ‹è¯•"""
        logger.info("=== å¼€å§‹APIè¯Šæ–­æµ‹è¯• ===")

        import aiohttp

        # æµ‹è¯•é…ç½®
        config = WebSearchConfig()
        test_url = config.url
        headers = {"X-API-KEY-AUTH": f"Bearer {config.token}"}

        # æµ‹è¯•ä¸åŒçš„æŸ¥è¯¢å‚æ•°
        test_cases = [
            {
                "queryStr": "Python",
                "count": 1
            },
            {
                "queryStr": "äººå·¥æ™ºèƒ½",
                "count": 2
            },
            {
                "queryStr": "æœºå™¨å­¦ä¹ ",
                "count": 3
            },
            {
                "queryStr": "æ·±åº¦å­¦ä¹ ",
                "count": 5
            },
        ]

        logger.info(f"APIç«¯ç‚¹: {test_url}")
        logger.info(f"è®¤è¯Token: {config.token[:20]}...")
        logger.info(f"è¯·æ±‚å¤´: {headers}")

        for i, params in enumerate(test_cases, 1):
            logger.info(f"\næµ‹è¯•ç”¨ä¾‹ {i}: {params}")

            try:
                timeout_obj = aiohttp.ClientTimeout(total=10)
                async with aiohttp.ClientSession(
                        timeout=timeout_obj) as session:
                    async with session.get(test_url,
                                           headers=headers,
                                           params=params) as response:
                        logger.info(f"  å“åº”çŠ¶æ€ç : {response.status}")
                        logger.info(f"  å“åº”å¤´: {dict(response.headers)}")

                        if response.status == 200:
                            try:
                                data = await response.json()
                                logger.info(f"  å“åº”æ•°æ®: {data}")

                                # åˆ†æå“åº”
                                if isinstance(data, dict):
                                    status = data.get('status')
                                    message = data.get('message', '')
                                    code = data.get('code', '')

                                    if status is False:
                                        logger.error(
                                            f"  âŒ APIé”™è¯¯: {message} (é”™è¯¯ç : {code})"
                                        )

                                        # æ ¹æ®é”™è¯¯ç æä¾›å»ºè®®
                                        if code == -8000040:
                                            logger.error("  ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                                            logger.error(
                                                "    1. æ£€æŸ¥ç½‘ç»œæ£€ç´¢æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
                                            logger.error("    2. ç¡®è®¤APIæƒé™å’Œé…é¢")
                                            logger.error("    3. è”ç³»APIæœåŠ¡æä¾›å•†")
                                            logger.error("    4. æ£€æŸ¥æŸ¥è¯¢å‚æ•°æ ¼å¼æ˜¯å¦æ­£ç¡®")
                                    elif status is True:
                                        logger.info(
                                            f"  âœ… APIæˆåŠŸ: æ‰¾åˆ° {len(data.get('data', []))} ä¸ªç»“æœ"
                                        )
                                    else:
                                        logger.warning(
                                            f"  âš ï¸   APIçŠ¶æ€ä¸æ˜ç¡®: {status}")
                                else:
                                    logger.warning(
                                        f"  âš ï¸  å“åº”æ ¼å¼å¼‚å¸¸: {type(data)}")

                            except Exception as e:
                                logger.error(f"  âŒ è§£æå“åº”å¤±è´¥: {e}")
                                text = await response.text()
                                logger.info(f"  åŸå§‹å“åº”: {text[:500]}...")
                        else:
                            logger.error(f"  âŒ HTTPé”™è¯¯: {response.status}")
                            text = await response.text()
                            logger.error(f"  é”™è¯¯å“åº”: {text}")

            except Exception as e:
                logger.error(f"  âŒ è¯·æ±‚å¤±è´¥: {e}")

        logger.info("-" * 50)

    async def test_alternative_queries():
        """æµ‹è¯•æ›¿ä»£æŸ¥è¯¢æ–¹å¼"""
        logger.info("=== å¼€å§‹æ›¿ä»£æŸ¥è¯¢æµ‹è¯• ===")

        # åˆ›å»ºæœç´¢å·¥å…·å®ä¾‹
        web_search = WebSearchTool()

        # æµ‹è¯•ä¸åŒçš„æŸ¥è¯¢æ ¼å¼
        test_queries = [
            "Pythonç¼–ç¨‹", "AIæŠ€æœ¯", "æœºå™¨å­¦ä¹ ç®—æ³•", "æ·±åº¦å­¦ä¹ æ¡†æ¶", "è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰", "æ•°æ®ç§‘å­¦",
            "ç®—æ³•è®¾è®¡"
        ]

        for query in test_queries:
            logger.info(f"æµ‹è¯•æŸ¥è¯¢: '{query}'")
            try:
                result = await web_search.get_web_search(query)
                if result:
                    logger.info(f"  âœ… æˆåŠŸè·å– {len(result)} ä¸ªç»“æœ")
                else:
                    logger.warning(f"  âš ï¸  æ— ç»“æœ")
            except Exception as e:
                logger.error(f"  âŒ æŸ¥è¯¢å¤±è´¥: {e}")

        logger.info("-" * 50)

    async def main():
        """ä¸»æµ‹è¯•å‡½æ•°"""
        logger.info("å¼€å§‹ç½‘ç»œæœç´¢å·¥å…·æµ‹è¯•")

        try:
            # æµ‹è¯•é…ç½®
            await test_config()

            # æµ‹è¯•ç½‘ç»œè¿æ¥
            await test_network_connectivity()

            # APIè¯Šæ–­æµ‹è¯•
            await test_api_diagnosis()

            # æ›¿ä»£æŸ¥è¯¢æµ‹è¯•
            await test_alternative_queries()

            # æµ‹è¯•ç½‘é¡µæŠ“å–
            await test_web_scraper()

            # è¯¦ç»†æœç´¢æµ‹è¯•
            await test_detailed_search()

            # æµ‹è¯•å¼‚æ­¥æœç´¢
            await test_async_search()

            # æµ‹è¯•åŒæ­¥æœç´¢
            test_sync_search()

            logger.info("æ‰€æœ‰æµ‹è¯•å®Œæˆ")

        except Exception as e:
            logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    # è¿è¡Œæµ‹è¯•
    if __name__ == "__main__":
        asyncio.run(main())
