"""
è§£æå™¨æ¨¡å—

æä¾›å„ç§LLMå“åº”å’Œæœç´¢ç»“æœçš„è§£æåŠŸèƒ½
"""

import json
import re

from doc_agent.core.logger import logger

from doc_agent.schemas import Source
from doc_agent.tools.reranker import RerankedSearchResult


def parse_llm_json_response(response: str) -> dict:
    """
    é€šç”¨çš„ LLM JSON å“åº”è§£æå‡½æ•°
    
    Args:
        response: LLM çš„åŸå§‹å“åº”
        
    Returns:
        dict: è§£æåçš„ JSON æ•°æ®
        
    Raises:
        ValueError: å½“è§£æå¤±è´¥æ—¶
    """
    try:
        # æ¸…ç†å“åº”ï¼Œå»é™¤å‰åç©ºç™½
        cleaned = response.strip()

        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass

        # æå– JSON å—
        json_match = re.search(r'```json\s*\n(.*?)\n\s*```', cleaned,
                               re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            return json.loads(json_str)

        # å°è¯•æå–èŠ±æ‹¬å·å†…çš„å†…å®¹
        json_match = re.search(r'\{.*\}', cleaned, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            return json.loads(json_str)

        raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„ JSON")

    except Exception as e:
        logger.error(f"è§£æ JSON å“åº”å¤±è´¥: {e}")
        raise ValueError(f"JSON è§£æå¤±è´¥: {str(e)}") from e


def parse_planner_response(response: str) -> tuple[str, list[str]]:
    """
    è§£æè§„åˆ’å™¨çš„å“åº”ï¼Œæå–ç ”ç©¶è®¡åˆ’å’Œæœç´¢æŸ¥è¯¢
    
    Args:
        response: LLM çš„åŸå§‹å“åº”
        
    Returns:
        tuple: (ç ”ç©¶è®¡åˆ’, æœç´¢æŸ¥è¯¢åˆ—è¡¨)
        
    Raises:
        ValueError: å½“ JSON è§£æå¤±è´¥æ—¶
    """
    logger.info("å¼€å§‹è§£æè§„åˆ’å™¨å“åº”")
    logger.debug(f"å“åº”å†…å®¹é•¿åº¦: {len(response)} å­—ç¬¦")

    try:
        # ä½¿ç”¨é€šç”¨ JSON è§£æå‡½æ•°
        data = parse_llm_json_response(response)

        # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šresearch_plan å’Œ research_questions
        if "research_plan" in data:
            research_plan = data["research_plan"]
        elif "research_questions" in data:
            # å°† research_questions è½¬æ¢ä¸º research_plan æ ¼å¼
            questions = data["research_questions"]
            if isinstance(questions, list):
                research_plan = "ç ”ç©¶é—®é¢˜ï¼š\n" + "\n".join(
                    [f"- {q}" for q in questions])
            else:
                research_plan = str(questions)
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç ”ç©¶è®¡åˆ’ï¼Œä½¿ç”¨é»˜è®¤å€¼
            research_plan = "åŸºäºä¸»é¢˜è¿›è¡Œæ·±å…¥ç ”ç©¶"

        search_queries = data.get("search_queries", [])

        logger.debug(f"æå–çš„ç ”ç©¶è®¡åˆ’ç±»å‹: {type(research_plan)}")
        logger.debug(f"æå–çš„æœç´¢æŸ¥è¯¢æ•°é‡: {len(search_queries)}")

        # å¤„ç† research_planï¼Œå¦‚æœæ˜¯å¤æ‚å¯¹è±¡åˆ™è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(research_plan, dict):
            # å°†å¤æ‚å¯¹è±¡è½¬æ¢ä¸ºç»“æ„åŒ–çš„å­—ç¬¦ä¸²æè¿°
            logger.debug("å°†å¤æ‚çš„ç ”ç©¶è®¡åˆ’å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²")
            plan_parts = []
            for key, value in research_plan.items():
                if isinstance(value, list):
                    plan_parts.append(f"{key}:")
                    for item in value:
                        plan_parts.append(f"  - {item}")
                else:
                    plan_parts.append(f"{key}: {value}")
            research_plan = "\n".join(plan_parts)
        elif not isinstance(research_plan, str):
            # å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            logger.debug(f"å°†ç ”ç©¶è®¡åˆ’ä» {type(research_plan)} è½¬æ¢ä¸ºå­—ç¬¦ä¸²")
            research_plan = str(research_plan)

        # éªŒè¯æ•°æ®ç±»å‹
        if not isinstance(research_plan, str):
            raise ValueError(f"ç ”ç©¶è®¡åˆ’å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œä½†å¾—åˆ° {type(research_plan)}")
        if not isinstance(search_queries, list):
            raise ValueError(f"æœç´¢æŸ¥è¯¢å¿…é¡»æ˜¯åˆ—è¡¨ï¼Œä½†å¾—åˆ° {type(search_queries)}")

        logger.info(
            f"æˆåŠŸè§£æè§„åˆ’å™¨å“åº”: ç ”ç©¶è®¡åˆ’é•¿åº¦={len(research_plan)}, æœç´¢æŸ¥è¯¢æ•°é‡={len(search_queries)}"
        )
        return research_plan, search_queries

    except ValueError as e:
        logger.error(f"è§£æè§„åˆ’å™¨å“åº”å¤±è´¥: {e}")
        raise


def parse_reflection_response(response: str) -> list[str]:
    """
    è§£æ reflection èŠ‚ç‚¹çš„ LLM å“åº”ï¼Œæå–æ–°çš„æœç´¢æŸ¥è¯¢

    Args:
        response: LLM çš„åŸå§‹å“åº”

    Returns:
        list[str]: æ–°çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨
    """
    try:
        # å°è¯•è§£æ JSON æ ¼å¼
        cleaned_response = response.strip()

        # å°è¯•æå– JSON éƒ¨åˆ†
        json_match = re.search(r'\{.*\}', cleaned_response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            data = json.loads(json_str)

            if 'new_queries' in data and isinstance(data['new_queries'], list):
                queries = data['new_queries']
                # éªŒè¯æŸ¥è¯¢è´¨é‡
                valid_queries = [
                    q.strip() for q in queries
                    if q.strip() and len(q.strip()) > 5
                ]
                if valid_queries:
                    return valid_queries

        # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æŸ¥è¯¢
        # æŸ¥æ‰¾å¸¸è§çš„æŸ¥è¯¢æ¨¡å¼
        query_patterns = [
            r'(\d+\.\s*)([^\n]+)',  # 1. query
            r'[-â€¢]\s*([^\n]+)',  # - query æˆ– â€¢ query
            r'"([^"]+)"',  # "query"
        ]

        for pattern in query_patterns:
            try:
                matches = re.findall(pattern, cleaned_response, re.MULTILINE)
                if matches:
                    queries = []
                    for match in matches:
                        if isinstance(match, tuple):
                            query = match[1] if len(match) > 1 else match[0]
                        else:
                            query = match

                        query = query.strip()
                        if query and len(query) > 5:
                            queries.append(query)

                    if queries:
                        return queries
            except Exception as e:
                logger.debug(f"æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¤±è´¥: {e}")
                continue

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ç®€å•çš„è¡Œåˆ†å‰²
        lines = cleaned_response.split('\n')
        queries = []
        for line in lines:
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œã€æ•°å­—è¡Œã€æ ‡é¢˜è¡Œç­‰
            if (line and len(line) > 10 and not line.startswith('#')
                    and not re.match(r'^\d+\.?$', line)
                    and not re.match(r'^[A-Z\s]+$', line)):  # å…¨å¤§å†™å¯èƒ½æ˜¯æ ‡é¢˜
                queries.append(line)

        return queries[:3]  # æœ€å¤šè¿”å›3ä¸ªæŸ¥è¯¢

    except Exception as e:
        logger.error(f"è§£æ reflection å“åº”å¤±è´¥: {e}")
        return []


def parse_web_search_results(web_raw_results: list[dict], query: str,
                             start_id: int) -> list[Source]:
    """
    è§£æç½‘ç»œæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡åˆ—è¡¨

    Args:
        web_raw_results: ç½‘ç»œæœç´¢ç»“æœ
        query: æœç´¢æŸ¥è¯¢
        start_id: èµ·å§‹ID

    Returns:
        list[Source]: Source å¯¹è±¡åˆ—è¡¨
    """

    sources = []

    for index, web_raw_result in enumerate(web_raw_results):
        try:
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨
            source_id = start_id + index
            source_type = "web"

            # ä» meta_data ä¸­è·å–æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            meta_data = web_raw_result.get('meta_data', {})
            title = meta_data.get('docName', f'ç½‘é¡µ {source_id}')

            url = web_raw_result.get('url', '')
            content = web_raw_result.get('text', '')

            # æˆªæ–­å†…å®¹åˆ°500å­—ç¬¦
            if len(content) > 500:
                content = content[:500] + "..."

            # ä» meta_data ä¸­è·å–æ›´å¤šä¿¡æ¯
            date = meta_data.get('datePublished', '')
            author = meta_data.get('author', '')
            site_name = meta_data.get('siteName', '')

            source = Source(id=source_id,
                            doc_id=web_raw_result.get('_id',
                                                      f'web_{source_id}'),
                            doc_from="web",
                            domain_id="web_search",
                            index="web_pages",
                            source_type=source_type,
                            title=title,
                            url=url,
                            content=content,
                            date=date,
                            date_published=date,
                            author=author,
                            site_name=site_name,
                            metadata={
                                "file_name": title,
                                "locations": [],
                                "source": "web_search"
                            })

            sources.append(source)
            logger.debug(f"âœ… æˆåŠŸåˆ›å»ºç½‘é¡µæº: {source_id} - {title}")

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç½‘é¡µæºå¤±è´¥: {e}")
            logger.error(f"ğŸ“„ åŸå§‹æ•°æ®: {web_raw_result}")
            continue

    logger.info(f"ğŸ“Š æˆåŠŸè§£æ {len(sources)} ä¸ªç½‘é¡µæº")
    return sources


def parse_es_search_results(es_raw_results: list[RerankedSearchResult],
                            query: str, start_id: int) -> list[Source]:
    """
    è§£æESæœç´¢ç»“æœï¼Œåˆ›å»º Source å¯¹è±¡åˆ—è¡¨

    Args:
        es_raw_results: ESæœç´¢ç»“æœ
        query: æœç´¢æŸ¥è¯¢
        start_id: èµ·å§‹ID

    Returns:
        list[Source]: Source å¯¹è±¡åˆ—è¡¨
    """

    # RerankedSearchResult
    # id='bFEcIJYBwBkB_JLNVi-q',
    # doc_id='5938a73882a4f00515b3614b03dc419d',
    # index='personal_knowledge_base',
    # domain_id='documentUploadAnswer',
    # doc_from='self',
    # original_content='æ–‡æœ¬å†…å®¹',
    # score=1.6947638,
    # rerank_score=3.2050650119781494,
    # metadata={
    # 	'file_name': 'æ·±åœ³å¸‚åŸå¸‚è½¨é“äº¤é€šå·¥ç¨‹æ— äººæœºä½¿ç”¨åŠå®æ™¯å»ºæ¨¡è¦æ±‚ï¼ˆV1.0ï¼‰.pdf',
    # 	'locations': [],
    # 	'source': 'self'
    # },
    # alias_name='personal_knowledge_base'

    sources = []

    for index, es_raw_result in enumerate(es_raw_results):
        try:
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨
            source_id = start_id + index
            source_type = "es_result"
            title = es_raw_result.metadata.get(
                'file_name', f'æ–‡æ¡£ {source_id}'
            ) if es_raw_result.metadata else f'æ–‡æ¡£ {source_id}'
            url = es_raw_result.metadata.get(
                'url', '') if es_raw_result.metadata else ''
            content = es_raw_result.original_content or ''
            locations = es_raw_result.metadata.get('locations', [])
            source = es_raw_result.doc_from

            # æˆªæ–­å†…å®¹åˆ°500å­—ç¬¦
            if len(content) > 500:
                content = content[:500] + "..."

            # ä» metadata ä¸­è·å–æ›´å¤šä¿¡æ¯
            metadata = es_raw_result.metadata or {}
            date = metadata.get('date', '')
            author = metadata.get('author', '')
            file_token = metadata.get('file_token', '')
            page_number = metadata.get('page_number')

            # ä¿ç•™åŸæœ‰çš„ metadataï¼Œåªæ·»åŠ å¿…è¦çš„å­—æ®µ
            metadata = es_raw_result.metadata or {}
            # metadata.update({
            #     "file_name": title,
            #     "locations": locations,
            #     # ç»Ÿä¸€ä½¿ç”¨ doc_from ä½œä¸º source å€¼
            #     "source": es_raw_result.doc_from
            # })

            source = Source(id=source_id,
                            doc_id=es_raw_result.doc_id,
                            doc_from=es_raw_result.doc_from,
                            domain_id=es_raw_result.domain_id,
                            index=es_raw_result.index,
                            source_type=source_type,
                            title=title,
                            url=url,
                            content=content,
                            date=date,
                            author=author,
                            file_token=file_token,
                            page_number=page_number,
                            metadata=metadata)

            sources.append(source)
            logger.debug(f"âœ… æˆåŠŸåˆ›å»ºESæº: {source_id} - {title}")

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºESæºå¤±è´¥: {e}")
            logger.error(f"ğŸ“„ åŸå§‹æ•°æ®: {es_raw_result}")
            continue

    logger.info(f"ğŸ“Š æˆåŠŸè§£æ {len(sources)} ä¸ªESæº")
    return sources
