import asyncio
import json
from typing import Union

import redis.asyncio as redis
from doc_agent.core.logger import logger

# å¯¼å…¥ Celery åº”ç”¨ç¨‹åº
from .celery_app import celery_app

# å¯¼å…¥ Redis Stream Publisher
from doc_agent.core.redis_stream_publisher import RedisStreamPublisher


def _get_detailed_progress_message(node_name: str) -> str:
    """
    æ ¹æ®èŠ‚ç‚¹åç§°ç”Ÿæˆè¯¦ç»†çš„è¿›åº¦æ¶ˆæ¯
    
    Args:
        node_name: èŠ‚ç‚¹åç§°
        
    Returns:
        str: è¯¦ç»†çš„è¿›åº¦æ¶ˆæ¯
    """
    progress_messages = {
        "initial_research": "å·²å®Œæˆåˆå§‹ç ”ç©¶é˜¶æ®µï¼Œæ­£åœ¨æ•´ç†æ”¶é›†åˆ°çš„ä¿¡æ¯æºï¼Œä¸ºå¤§çº²ç”Ÿæˆåšå‡†å¤‡...",
        "outline_generation": "å·²å®Œæˆå¤§çº²ç”Ÿæˆé˜¶æ®µï¼Œæ­£åœ¨ä¼˜åŒ–æ–‡æ¡£ç»“æ„å’Œç« èŠ‚å®‰æ’...",
        "planner": "å·²å®Œæˆè®¡åˆ’åˆ¶å®šé˜¶æ®µï¼Œæ­£åœ¨ç¡®å®šç ”ç©¶æ–¹å‘å’Œé‡ç‚¹å†…å®¹...",
        "researcher": "å·²å®Œæˆæ·±å…¥ç ”ç©¶é˜¶æ®µï¼Œæ­£åœ¨åˆ†ææ”¶é›†åˆ°çš„è¯¦ç»†ä¿¡æ¯...",
        "writer": "å·²å®Œæˆå†…å®¹æ’°å†™é˜¶æ®µï¼Œæ­£åœ¨å®Œå–„æ–‡æ¡£å†…å®¹...",
        "reflection": "å·²å®Œæˆåæ€ä¼˜åŒ–é˜¶æ®µï¼Œæ­£åœ¨æ£€æŸ¥å’Œå®Œå–„æ–‡æ¡£è´¨é‡...",
        "supervisor": "å·²å®Œæˆç›‘ç£å†³ç­–é˜¶æ®µï¼Œæ­£åœ¨è¯„ä¼°å½“å‰è¿›åº¦å¹¶ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨...",
        "editor": "å·²å®Œæˆç¼–è¾‘ä¼˜åŒ–é˜¶æ®µï¼Œæ­£åœ¨å®Œå–„æ–‡æ¡£æ ¼å¼å’Œå†…å®¹...",
        "generation": "å·²å®Œæˆå†…å®¹ç”Ÿæˆé˜¶æ®µï¼Œæ­£åœ¨åˆ›å»ºæœ€ç»ˆçš„æ–‡æ¡£å†…å®¹...",
        "research": "å·²å®Œæˆç ”ç©¶é˜¶æ®µï¼Œæ­£åœ¨æ•´ç†å’Œåˆ†æç›¸å…³ä¿¡æ¯..."
    }

    return progress_messages.get(node_name, f"å·²å®Œæˆæ­¥éª¤: {node_name}")


# å»¶è¿Ÿå¯¼å…¥containerä»¥é¿å…å¾ªç¯å¯¼å…¥
def get_container():
    """å»¶è¿Ÿå¯¼å…¥containerä»¥é¿å…å¾ªç¯å¯¼å…¥"""
    from doc_agent.core.container import container
    return container


# Redisè¿æ¥ç°åœ¨æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„ï¼Œé¿å…è¿æ¥è¶…æ—¶é—®é¢˜


def get_redis_client() -> redis.Redis:
    """è·å–Rediså®¢æˆ·ç«¯å®ä¾‹"""
    try:
        # æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„è¿æ¥ï¼Œé¿å…è¿æ¥è¶…æ—¶é—®é¢˜
        from doc_agent.core.config import settings
        redis_client = redis.from_url(settings.redis_url,
                                      encoding="utf-8",
                                      decode_responses=True)
        # æµ‹è¯•è¿æ¥
        redis_client.ping()
        logger.info("Rediså®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        return redis_client
    except Exception as e:
        logger.error(f"Redisè¿æ¥å¤±è´¥: {e}")
        raise


@celery_app.task
def generate_outline_from_query_task(job_id: Union[str, int],
                                     task_prompt: str,
                                     is_online: bool = False,
                                     context_files: dict = None,
                                     style_guide_content: str = None,
                                     requirements: str = None,
                                     redis_stream_key: str = None) -> str:
    """
    ä»æŸ¥è¯¢ç”Ÿæˆå¤§çº²çš„å¼‚æ­¥ä»»åŠ¡

    Args:
        job_id: ä½œä¸šID
        task_prompt: ç”¨æˆ·çš„æ ¸å¿ƒæŒ‡ä»¤
        is_online: æ˜¯å¦è°ƒç”¨webæœç´¢
        context_files: ä¸Šä¸‹æ–‡æ–‡ä»¶åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        style_guide_content: é£æ ¼æŒ‡å—å†…å®¹ï¼ˆå¯é€‰ï¼‰
        requirements: ç¼–å†™è¦æ±‚ï¼ˆå¯é€‰ï¼‰
        redis_stream_key: Redisæµkeyï¼ˆå¯é€‰ï¼‰

    Returns:
        ä»»åŠ¡çŠ¶æ€
    """
    logger.info(f"å¤§çº²ç”Ÿæˆä»»åŠ¡å¼€å§‹ - Job ID: {job_id}")

    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(
            _generate_outline_from_query_task_async(job_id, task_prompt,
                                                    is_online, context_files,
                                                    style_guide_content,
                                                    requirements,
                                                    redis_stream_key))
    except Exception as e:
        logger.error(f"å¤§çº²ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
        return "FAILED"


async def _generate_outline_from_query_task_async(
        job_id: Union[str, int],
        task_prompt: str,
        is_online: bool = False,
        context_files: dict = None,
        style_guide_content: str = None,
        requirements: str = None,
        redis_stream_key: str = None) -> str:
    """å¼‚æ­¥å¤§çº²ç”Ÿæˆä»»åŠ¡çš„å†…éƒ¨å®ç°"""
    try:
        # è·å–Rediså®¢æˆ·ç«¯å’Œå‘å¸ƒå™¨
        redis = await get_redis_client()
        from doc_agent.core.redis_stream_publisher import RedisStreamPublisher
        publisher = RedisStreamPublisher(redis)

        # å‘å¸ƒä»»åŠ¡å¼€å§‹äº‹ä»¶
        await publisher.publish_task_started(job_id=job_id,
                                             task_type="outline_generation",
                                             task_prompt=task_prompt)

        logger.info(f"Job {job_id}: å¼€å§‹ç”Ÿæˆå¤§çº²ï¼Œä¸»é¢˜: '{task_prompt[:50]}...'")
        logger.info(f"  is_online: {is_online}")
        logger.info(
            f"  context_files: {len(context_files) if context_files else 0} ä¸ªæ–‡ä»¶"
        )
        logger.info(f"  style_guide_content: {bool(style_guide_content)}")
        logger.info(f"  requirements: {bool(requirements)}")
        logger.info(f"  redis_stream_key: {redis_stream_key}")

        # ä½¿ç”¨çœŸæ­£çš„å·¥ä½œæµç”Ÿæˆå¤§çº²
        from doc_agent.core.container import container
        from doc_agent.graph.state import ResearchState
        import uuid

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        run_id = f"run-{uuid.uuid4().hex[:8]}"
        initial_state = ResearchState(
            topic=task_prompt,
            style_guide_content=style_guide_content or "",
            requirements_content=requirements or "",
            initial_sources=[],
            document_outline={},
            chapters_to_process=[],
            current_chapter_index=0,
            current_citation_index=1,
            completed_chapters=[],
            final_document="",
            sources=[],
            all_sources=[],
            cited_sources=[],
            cited_sources_in_chapter=[],
            messages=[],
            run_id=run_id,
        )

        # å‘å¸ƒè¿›åº¦äº‹ä»¶
        await publisher.publish_task_progress(
            job_id=job_id,
            task_type="outline_generation",
            progress="æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚ï¼Œå‡†å¤‡å¼€å§‹å¤§çº²ç”Ÿæˆæµç¨‹...",
            step="analysis")

        # æ‰§è¡ŒçœŸæ­£çš„å·¥ä½œæµ
        outline_result = None
        try:
            logger.info(
                f"ğŸ” [API Task] å¼€å§‹æ‰§è¡Œå·¥ä½œæµï¼Œåˆå§‹çŠ¶æ€: {initial_state.get('topic', 'unknown')}"
            )

            async for step_output in container.outline_graph.astream(
                    initial_state):
                node_name = list(step_output.keys())[0]
                logger.info(f"âœ… [API Task] Finished step: [ {node_name} ]")
                step_result = list(step_output.values())[0]

                # å‘å¸ƒè¿›åº¦äº‹ä»¶
                progress_message = _get_detailed_progress_message(node_name)
                await publisher.publish_task_progress(
                    job_id=job_id,
                    task_type="outline_generation",
                    progress=progress_message,
                    step=node_name)

                logger.info(
                    f"ğŸ” [API Task] Step result keys: {list(step_result.keys()) if step_result else 'None'}"
                )

                if step_result and "document_outline" in step_result:
                    outline_result = step_result.get("document_outline")
                    logger.info(
                        f"âœ… [API Task] æ‰¾åˆ° document_outlineï¼Œç»“æ„: {list(outline_result.keys()) if outline_result else 'None'}"
                    )
                    break
                else:
                    logger.info(f"ğŸ“‹ [API Task] Step {node_name} å®Œæˆï¼Œç­‰å¾…ä¸‹ä¸€ä¸ªæ­¥éª¤...")

        except Exception as e:
            logger.error(f"âŒ [API Task] Error during outline generation: {e}")
            logger.error(
                f"âŒ [API Task] Exception details: {type(e).__name__}: {str(e)}"
            )
            import traceback
            logger.error(f"âŒ [API Task] Traceback: {traceback.format_exc()}")
            raise e

        if not outline_result:
            # å¦‚æœå·¥ä½œæµå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            logger.warning("âš ï¸  å·¥ä½œæµå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å¤§çº²ç”Ÿæˆæ–¹æ¡ˆ")
            outline_result = {
                "title":
                f"åŸºäº'{task_prompt}'çš„æŠ€æœ¯æ–‡æ¡£",
                "nodes": [{
                    "id": "node_1",
                    "title": "å¼•è¨€",
                    "content_summary": f"ä»‹ç»{task_prompt}çš„åŸºæœ¬æ¦‚å¿µå’ŒèƒŒæ™¯"
                }, {
                    "id": "node_2",
                    "title": "æŠ€æœ¯åŸç†",
                    "content_summary": f"è¯¦ç»†è§£é‡Š{task_prompt}çš„æ ¸å¿ƒåŸç†"
                }, {
                    "id": "node_3",
                    "title": "åº”ç”¨åœºæ™¯",
                    "content_summary": f"å±•ç¤º{task_prompt}çš„å®é™…åº”ç”¨æ¡ˆä¾‹"
                }]
            }

        # å‘å¸ƒå¤§çº²ç”Ÿæˆå®Œæˆäº‹ä»¶
        await publisher.publish_outline_generated(job_id, outline_result)

        # ä¿å­˜å¤§çº²ç»“æœåˆ°Redis
        outline_json = json.dumps(outline_result, ensure_ascii=False)
        await redis.set(f"job_result:{job_id}", outline_json, ex=3600)  # 1å°æ—¶è¿‡æœŸ

        # å‘å¸ƒä»»åŠ¡å®Œæˆäº‹ä»¶
        await publisher.publish_task_completed(
            job_id=job_id,
            task_type="outline_generation",
            result={"outline": outline_result},
            duration="7s")

        logger.info(f"Job {job_id}: å¤§çº²ç”Ÿæˆå®Œæˆ")

        return "COMPLETED"

    except Exception as e:
        logger.error(f"Job {job_id}: å¤§çº²ç”Ÿæˆå¤±è´¥: {e}")

        # å‘å¸ƒä»»åŠ¡å¤±è´¥äº‹ä»¶
        try:
            await publisher.publish_task_failed(job_id=job_id,
                                                task_type="outline_generation",
                                                error=str(e))
        except Exception as publish_error:
            logger.error(f"å‘å¸ƒå¤±è´¥äº‹ä»¶æ—¶å‡ºé”™: {publish_error}")

        return "FAILED"


@celery_app.task(name="workers.tasks.generate_document_from_outline_task")
def generate_document_from_outline_task(job_id: str,
                                        outline_json: str,
                                        session_id: str = "default_session"):
    """
    æ¥æ”¶ outline çš„ JSON å­—ç¬¦ä¸²ï¼Œå¹¶å¯åŠ¨æ–‡æ¡£ç”Ÿæˆå·¥ä½œæµçš„ Celery ä»»åŠ¡ã€‚
    """
    # å…³é”®çš„ç¬¬ä¸€æ­¥ï¼šåœ¨ä»»åŠ¡å¼€å§‹æ—¶ç«‹åˆ»æ‰“å°æ—¥å¿—ï¼Œç¡®è®¤ä»»åŠ¡å·²è¢«æ¥æ”¶
    logger.success(f"âœ… Celery Worker å·²æˆåŠŸæ¥æ”¶æ–‡æ¡£ç”Ÿæˆä»»åŠ¡: {job_id}")
    logger.info(f"   - Session ID: {session_id}")
    logger.info(f"   - Outline JSON (å‰100å­—ç¬¦): {outline_json[:100]}...")

    try:
        # ç¬¬äºŒæ­¥ï¼šåœ¨ä»»åŠ¡å†…éƒ¨è§£æ JSON å­—ç¬¦ä¸²
        outline_data = json.loads(outline_json)
        logger.info("âœ… Outline JSON è§£ææˆåŠŸã€‚")

        # è·å–ä¸»ç¼–æ’å™¨å›¾å®ä¾‹
        main_orchestrator = get_container().main_orchestrator_graph

        # å‡†å¤‡å·¥ä½œæµçš„åˆå§‹çŠ¶æ€
        initial_state = {
            "job_id": job_id,
            "session_id": session_id,
            "document_outline": outline_data,
            # åˆå§‹åŒ–å…¶ä»–å¿…è¦çš„çŠ¶æ€å­—æ®µ
            "sources": {},
            "citation_counter": 0,
            "completed_chapters": [],
            "current_chapter_index": 0,
        }

        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆå·¥ä½œæµ...")
        # åŒæ­¥æ‰§è¡Œå®Œæ•´çš„ LangGraph å·¥ä½œæµ
        final_state = main_orchestrator.invoke(
            initial_state, config={"configurable": {
                "thread_id": job_id
            }})

        logger.success(f"âœ… æ–‡æ¡£ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œå®Œæ¯•: {job_id}")
        return {
            "status": "COMPLETED",
            "final_document": final_state.get("final_document")
        }

    except json.JSONDecodeError as e:
        logger.error(f"âŒ è§£æ Outline JSON å¤±è´¥: {e}")
        # åœ¨è¿™é‡Œå¯ä»¥å¢åŠ ä»»åŠ¡å¤±è´¥çš„å¤„ç†é€»è¾‘
        return {"status": "FAILED", "error": "Invalid outline JSON"}
    except Exception as e:
        logger.error(f"âŒ æ–‡æ¡£ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return {"status": "FAILED", "error": str(e)}


async def _generate_document_from_outline_task_async(job_id: Union[str, int],
                                                     outline: dict,
                                                     session_id: str = None
                                                     ) -> str:
    """å¼‚æ­¥æ–‡æ¡£ç”Ÿæˆä»»åŠ¡çš„å†…éƒ¨å®ç°"""
    logger.info(f"ğŸ”„ å¼€å§‹å¼‚æ­¥æ–‡æ¡£ç”Ÿæˆ - Job ID: {job_id}")

    try:
        # è·å–Rediså®¢æˆ·ç«¯å’Œå‘å¸ƒå™¨
        logger.info(f"ğŸ”— è¿æ¥Redis...")
        redis = await get_redis_client()
        from doc_agent.core.redis_stream_publisher import RedisStreamPublisher
        publisher = RedisStreamPublisher(redis)
        logger.info(f"âœ… Redisè¿æ¥æˆåŠŸ")

        # å‘å¸ƒä»»åŠ¡å¼€å§‹äº‹ä»¶
        logger.info(f"ğŸ“¢ å‘å¸ƒä»»åŠ¡å¼€å§‹äº‹ä»¶...")
        await publisher.publish_task_started(job_id=job_id,
                                             task_type="document_generation",
                                             outline_title=outline.get(
                                                 "title", "æœªçŸ¥æ ‡é¢˜"))
        logger.info(f"âœ… ä»»åŠ¡å¼€å§‹äº‹ä»¶å·²å‘å¸ƒ")

        logger.info(
            f"ğŸ“ Job {job_id}: å¼€å§‹ç”Ÿæˆæ–‡æ¡£ï¼Œå¤§çº²æ ‡é¢˜: '{outline.get('title', 'æœªçŸ¥æ ‡é¢˜')}'")

        # å‘å¸ƒåˆ†æè¿›åº¦äº‹ä»¶
        logger.info(f"ğŸ“Š å‘å¸ƒåˆ†æè¿›åº¦äº‹ä»¶...")
        await publisher.publish_task_progress(job_id=job_id,
                                              task_type="document_generation",
                                              progress="æ­£åœ¨åˆ†æå¤§çº²ç»“æ„",
                                              step="analysis")
        logger.info(f"âœ… åˆ†æè¿›åº¦äº‹ä»¶å·²å‘å¸ƒ")

        # åˆå§‹åŒ–çŠ¶æ€ï¼Œå°†outlineé›†æˆåˆ°äºŒé˜¶æ®µå·¥ä½œæµç¨‹
        logger.info(f"ğŸ—ï¸ åˆå§‹åŒ–ç ”ç©¶çŠ¶æ€...")
        from doc_agent.graph.state import ResearchState

        # æ„å»ºåˆå§‹çŠ¶æ€ - å‚è€ƒ test_decoupled_workflow.py ä¸­çš„æ­£ç¡®é…ç½®
        initial_state = ResearchState(
            run_id=str(job_id),
            topic=outline.get("title", "æŠ€æœ¯æ–‡æ¡£"),
            style_guide_content=None,
            requirements_content=None,
            initial_sources=[],
            document_outline=outline,
            chapters_to_process=[],
            current_chapter_index=0,
            completed_chapters=[],
            final_document="",
            research_plan="",
            search_queries=[],
            gathered_sources=[],
            sources=[],  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å­—æ®µ
            all_sources=[],  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å­—æ®µ
            current_citation_index=1,  # ğŸ”§ ä¿®å¤ï¼šå¼•ç”¨ç´¢å¼•åº”è¯¥ä»1å¼€å§‹
            cited_sources=[],  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å­—æ®µ
            cited_sources_in_chapter=[],  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å­—æ®µ
            messages=[])
        logger.info(f"âœ… ç ”ç©¶çŠ¶æ€åˆå§‹åŒ–å®Œæˆ")

        # ä»outlineä¸­æå–ç« èŠ‚ä¿¡æ¯
        logger.info(f"ğŸ“– æå–ç« èŠ‚ä¿¡æ¯...")
        chapters = []
        for i, chapter in enumerate(outline.get("chapters", [])):
            chapter_info = {
                "chapter_title": chapter.get("title", f"ç« èŠ‚ {i+1}"),
                "description": chapter.get("description", ""),
                "node_id": f"chapter_{i+1}",
                "children": chapter.get("sections", [])
            }
            chapters.append(chapter_info)
            logger.info(f"ğŸ“„ ç« èŠ‚ {i+1}: {chapter_info['chapter_title']}")

        initial_state["chapters_to_process"] = chapters
        logger.info(f"âœ… æå–åˆ° {len(chapters)} ä¸ªç« èŠ‚")

        # è·å–å®¹å™¨å’Œç« èŠ‚å·¥ä½œæµå›¾
        logger.info(f"ğŸ”§ è·å–å®¹å™¨å’Œç« èŠ‚å·¥ä½œæµ...")
        container = get_container()
        chapter_workflow = container.chapter_graph

        if not chapter_workflow:
            logger.error(f"âŒ Job {job_id}: ç« èŠ‚å·¥ä½œæµå›¾æœªæ‰¾åˆ°")
            raise Exception("ç« èŠ‚å·¥ä½œæµå›¾æœªåˆå§‹åŒ–")
        logger.info(f"âœ… ç« èŠ‚å·¥ä½œæµå›¾è·å–æˆåŠŸ")

        # å¼€å§‹å¤„ç†æ¯ä¸ªç« èŠ‚
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†ç« èŠ‚...")
        completed_chapters = []
        all_sources = []
        cited_sources = []
        all_answer_origins = []
        all_web_sources = []

        for chapter_index, chapter in enumerate(chapters):
            chapter_title = chapter["chapter_title"]
            logger.info(
                f"ğŸ“ Job {job_id}: å¼€å§‹å¤„ç†ç« èŠ‚ {chapter_index + 1}/{len(chapters)}: {chapter_title}"
            )

            # å‘å¸ƒç« èŠ‚å¼€å§‹äº‹ä»¶
            logger.info(f"ğŸ“¢ å‘å¸ƒç« èŠ‚å¼€å§‹äº‹ä»¶...")
            await publisher.publish_event(
                job_id, {
                    "eventType": "chapter_started",
                    "taskType": "document_generation",
                    "chapterTitle": chapter_title,
                    "chapterIndex": chapter_index,
                    "totalChapters": len(chapters),
                    "status": "running"
                })
            logger.info(f"âœ… ç« èŠ‚å¼€å§‹äº‹ä»¶å·²å‘å¸ƒ")

            # æ›´æ–°å½“å‰ç« èŠ‚ç´¢å¼•
            logger.info(f"ğŸ”„ æ›´æ–°å½“å‰ç« èŠ‚çŠ¶æ€...")
            current_state = initial_state.copy()
            current_state["current_chapter_index"] = chapter_index
            current_state["chapters_to_process"] = chapters
            current_state["completed_chapters"] = completed_chapters
            current_state["all_sources"] = all_sources
            current_state["cited_sources"] = cited_sources
            logger.info(f"âœ… ç« èŠ‚çŠ¶æ€æ›´æ–°å®Œæˆ")

            # å‘å¸ƒç« èŠ‚å¤„ç†è¿›åº¦
            logger.info(f"ğŸ“Š å‘å¸ƒç« èŠ‚å¤„ç†è¿›åº¦...")
            await publisher.publish_task_progress(
                job_id=job_id,
                task_type="document_generation",
                progress=f"æ­£åœ¨å¤„ç†ç« èŠ‚: {chapter_title}",
                step=f"chapter_{chapter_index + 1}")
            logger.info(f"âœ… ç« èŠ‚å¤„ç†è¿›åº¦å·²å‘å¸ƒ")

            try:
                # æ¨¡æ‹Ÿç« èŠ‚å¤„ç†æ­¥éª¤ï¼ˆå‚ç…§ mock å®ç°ï¼‰
                logger.info(f"ğŸ”„ å¼€å§‹æ¨¡æ‹Ÿç« èŠ‚å¤„ç†æ­¥éª¤...")
                steps = ["planner", "researcher", "supervisor", "writer"]
                for step in steps:
                    logger.info(f"âš™ï¸ æ‰§è¡Œæ­¥éª¤: {step}")
                    await publisher.publish_event(
                        job_id, {
                            "eventType": "chapter_progress",
                            "taskType": "document_generation",
                            "chapterTitle": chapter_title,
                            "step": step,
                            "progress": f"æ­£åœ¨æ‰§è¡Œ{step}æ­¥éª¤",
                            "status": "running"
                        })
                    # æ¨¡æ‹Ÿæ­¥éª¤å¤„ç†æ—¶é—´
                    await asyncio.sleep(0.5)
                    logger.info(f"âœ… æ­¥éª¤ {step} å®Œæˆ")

                # æ‰§è¡Œç« èŠ‚å·¥ä½œæµ
                logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œç« èŠ‚å·¥ä½œæµ...")
                logger.info(f"ğŸ“Š å½“å‰çŠ¶æ€é”®: {list(current_state.keys())}")

                # æ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
                logger.info(f"ğŸ” ç« èŠ‚å·¥ä½œæµç±»å‹: {type(chapter_workflow)}")
                logger.info(f"ğŸ” ç« èŠ‚å·¥ä½œæµæ–¹æ³•: {dir(chapter_workflow)}")

                result = await chapter_workflow.ainvoke(current_state)
                logger.info(f"âœ… ç« èŠ‚å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
                logger.info(
                    f"ğŸ“Š å·¥ä½œæµç»“æœé”®: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}"
                )

                # æå–ç« èŠ‚ç»“æœ
                if "final_document" in result:
                    chapter_content = result["final_document"]
                    logger.info(f"ğŸ“„ ä»ç»“æœä¸­æå–åˆ°ç« èŠ‚å†…å®¹ï¼Œé•¿åº¦: {len(chapter_content)}")
                else:
                    chapter_content = f"ç« èŠ‚ {chapter_title} çš„å†…å®¹..."
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°final_documentï¼Œä½¿ç”¨é»˜è®¤å†…å®¹")

                # æ”¶é›†å¼•ç”¨æº
                chapter_sources = result.get("cited_sources_in_chapter", [])
                all_sources.extend(chapter_sources)
                cited_sources.extend(chapter_sources)
                logger.info(f"ğŸ“š æ”¶é›†åˆ° {len(chapter_sources)} ä¸ªå¼•ç”¨æº")

                # ç”Ÿæˆæ¨¡æ‹Ÿçš„å¼•ç”¨æºï¼ˆå‚ç…§ mock å®ç°ï¼‰
                logger.info(f"ğŸ­ ç”Ÿæˆæ¨¡æ‹Ÿå¼•ç”¨æº...")
                chapter_answer_origins = generate_mock_sources_for_chapter(
                    chapter_title, chapter_index)
                chapter_web_sources = generate_mock_web_sources_for_chapter(
                    chapter_title, chapter_index)

                all_answer_origins.extend(chapter_answer_origins)
                all_web_sources.extend(chapter_web_sources)
                logger.info(
                    f"âœ… ç”Ÿæˆæ¨¡æ‹Ÿå¼•ç”¨æºå®Œæˆ: {len(chapter_answer_origins)} ä¸ªæ–‡æ¡£æº, {len(chapter_web_sources)} ä¸ªç½‘é¡µæº"
                )

                # ä¿å­˜ç« èŠ‚ç»“æœ
                completed_chapter = {
                    "title": chapter_title,
                    "content": chapter_content,
                    "sources": chapter_sources,
                    "chapter_index": chapter_index
                }
                completed_chapters.append(completed_chapter)
                logger.info(f"ğŸ’¾ ç« èŠ‚ç»“æœå·²ä¿å­˜")

                # å‘å¸ƒç« èŠ‚å®Œæˆäº‹ä»¶
                logger.info(f"ğŸ“¢ å‘å¸ƒç« èŠ‚å®Œæˆäº‹ä»¶...")
                await publisher.publish_event(
                    job_id, {
                        "eventType": "chapter_completed",
                        "taskType": "document_generation",
                        "chapterTitle": chapter_title,
                        "chapterContent": chapter_content,
                        "chapterIndex": chapter_index,
                        "status": "completed"
                    })
                logger.info(f"âœ… ç« èŠ‚å®Œæˆäº‹ä»¶å·²å‘å¸ƒ")

                # å‘å¸ƒå†™ä½œå¼€å§‹äº‹ä»¶
                logger.info(f"ğŸ“¢ å‘å¸ƒå†™ä½œå¼€å§‹äº‹ä»¶...")
                await publisher.publish_event(
                    job_id, {
                        "eventType": "writer_started",
                        "taskType": "document_generation",
                        "progress": f"å¼€å§‹ç¼–å†™ç« èŠ‚ {chapter_index + 1}",
                        "status": "running"
                    })
                logger.info(f"âœ… å†™ä½œå¼€å§‹äº‹ä»¶å·²å‘å¸ƒ")

                # æµå¼è¾“å‡ºç« èŠ‚å†…å®¹
                logger.info(f"ğŸ“¤ å¼€å§‹æµå¼è¾“å‡ºç« èŠ‚å†…å®¹...")
                await stream_document_content(publisher, job_id,
                                              chapter_content)
                logger.info(f"âœ… ç« èŠ‚å†…å®¹æµå¼è¾“å‡ºå®Œæˆ")

                logger.info(f"âœ… Job {job_id}: ç« èŠ‚ {chapter_title} å¤„ç†å®Œæˆ")

            except Exception as chapter_error:
                logger.error(
                    f"âŒ Job {job_id}: ç« èŠ‚ {chapter_title} å¤„ç†å¤±è´¥: {chapter_error}"
                )
                logger.error(
                    f"ğŸ” ç« èŠ‚é”™è¯¯è¯¦æƒ…: {type(chapter_error).__name__}: {str(chapter_error)}"
                )
                import traceback
                logger.error(f"ğŸ“‹ ç« èŠ‚é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

                # å‘å¸ƒç« èŠ‚å¤±è´¥äº‹ä»¶
                await publisher.publish_event(
                    job_id, {
                        "eventType": "chapter_failed",
                        "taskType": "document_generation",
                        "chapterTitle": chapter_title,
                        "chapterIndex": chapter_index,
                        "error": str(chapter_error),
                        "status": "failed"
                    })
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªç« èŠ‚
                continue

        # åˆå¹¶æ‰€æœ‰ç« èŠ‚å†…å®¹
        logger.info(f"ğŸ“„ å¼€å§‹åˆå¹¶æ‰€æœ‰ç« èŠ‚å†…å®¹...")
        final_document_parts = []
        final_document_parts.append(f"# {outline.get('title', 'æŠ€æœ¯æ–‡æ¡£')}\n\n")

        for chapter in completed_chapters:
            final_document_parts.append(f"## {chapter['title']}\n\n")
            final_document_parts.append(f"{chapter['content']}\n\n")

        final_document = "".join(final_document_parts)
        logger.info(f"âœ… ç« èŠ‚å†…å®¹åˆå¹¶å®Œæˆï¼Œæ€»é•¿åº¦: {len(final_document)}")

        # å‘å¸ƒå‚è€ƒæ–‡çŒ®äº‹ä»¶
        logger.info(f"ğŸ“š å‘å¸ƒå‚è€ƒæ–‡çŒ®äº‹ä»¶...")
        citations_data = {
            "answerOrigins": all_answer_origins,
            "webs": all_web_sources
        }

        await publisher.publish_event(
            job_id, {
                "eventType": "citations_completed",
                "taskType": "document_generation",
                "citations": citations_data,
                "totalAnswerOrigins": len(all_answer_origins),
                "totalWebSources": len(all_web_sources),
                "status": "completed"
            })
        logger.info(f"âœ… å‚è€ƒæ–‡çŒ®äº‹ä»¶å·²å‘å¸ƒ")

        # ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ç»“æœ
        logger.info(f"ğŸ“‹ ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ç»“æœ...")

        # å°†Sourceå¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        serializable_sources = []
        for source in all_sources:
            if hasattr(source, 'model_dump'):
                # å¦‚æœæ˜¯Pydanticæ¨¡å‹ï¼Œä½¿ç”¨model_dump()
                serializable_sources.append(source.model_dump())
            elif hasattr(source, '__dict__'):
                # å¦‚æœæ˜¯æ™®é€šå¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
                serializable_sources.append(source.__dict__)
            else:
                # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                serializable_sources.append(str(source))

        document_result = {
            "title": outline.get("title", "æŠ€æœ¯æ–‡æ¡£"),
            "content": final_document,
            "word_count": len(final_document.split()),
            "char_count": len(final_document),
            "sections": len(completed_chapters),
            "generated_at": str(asyncio.get_event_loop().time()),
            "chapters": completed_chapters,
            "sources": serializable_sources,  # ä½¿ç”¨å¯åºåˆ—åŒ–çš„ç‰ˆæœ¬
            "citations": citations_data
        }
        logger.info(f"âœ… æœ€ç»ˆæ–‡æ¡£ç»“æœç”Ÿæˆå®Œæˆ")

        # å‘å¸ƒæ–‡æ¡£ç”Ÿæˆå®Œæˆäº‹ä»¶
        logger.info(f"ğŸ“¢ å‘å¸ƒæ–‡æ¡£ç”Ÿæˆå®Œæˆäº‹ä»¶...")
        await publisher.publish_document_generated(job_id, document_result)
        logger.info(f"âœ… æ–‡æ¡£ç”Ÿæˆå®Œæˆäº‹ä»¶å·²å‘å¸ƒ")

        # ä¿å­˜æ–‡æ¡£ç»“æœåˆ°Redis
        logger.info(f"ğŸ’¾ ä¿å­˜æ–‡æ¡£ç»“æœåˆ°Redis...")
        document_json = json.dumps(document_result, ensure_ascii=False)
        await redis.set(f"job_result:{job_id}", document_json,
                        ex=3600)  # 1å°æ—¶è¿‡æœŸ
        logger.info(f"âœ… æ–‡æ¡£ç»“æœå·²ä¿å­˜åˆ°Redis")

        # å‘å¸ƒä»»åŠ¡å®Œæˆäº‹ä»¶
        logger.info(f"ğŸ“¢ å‘å¸ƒä»»åŠ¡å®Œæˆäº‹ä»¶...")
        await publisher.publish_task_completed(
            job_id=job_id,
            task_type="document_generation",
            result={"document": document_result},
            duration="completed")
        logger.info(f"âœ… ä»»åŠ¡å®Œæˆäº‹ä»¶å·²å‘å¸ƒ")

        logger.info(
            f"ğŸ‰ Job {job_id}: æ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œå…±å¤„ç† {len(completed_chapters)} ä¸ªç« èŠ‚")

        return "COMPLETED"

    except Exception as e:
        logger.error(f"âŒ Job {job_id}: æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")
        logger.error(f"ğŸ” é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

        # å‘å¸ƒä»»åŠ¡å¤±è´¥äº‹ä»¶
        try:
            logger.info(f"ğŸ“¢ å‘å¸ƒä»»åŠ¡å¤±è´¥äº‹ä»¶...")
            await publisher.publish_task_failed(
                job_id=job_id, task_type="document_generation", error=str(e))
            logger.info(f"âœ… ä»»åŠ¡å¤±è´¥äº‹ä»¶å·²å‘å¸ƒ")
        except Exception as publish_error:
            logger.error(f"âŒ å‘å¸ƒå¤±è´¥äº‹ä»¶æ—¶å‡ºé”™: {publish_error}")

        return "FAILED"


async def stream_document_content(publisher: RedisStreamPublisher,
                                  job_id: str,
                                  content: str,
                                  chunk_size: int = 10):
    """æµå¼è¾“å‡ºæ–‡æ¡£å†…å®¹åˆ°Redis Stream"""
    try:
        # å°†å†…å®¹æŒ‰å­—ç¬¦åˆ†å‰²æˆtoken
        tokens = list(content)

        for i in range(0, len(tokens), chunk_size):
            chunk = tokens[i:i + chunk_size]
            chunk_text = ''.join(chunk)

            # å‘å¸ƒå†…å®¹æµäº‹ä»¶
            await publisher.publish_event(
                job_id, {
                    "eventType": "document_content_stream",
                    "taskType": "document_generation",
                    "content": chunk_text,
                    "tokenIndex": i,
                    "totalTokens": len(tokens),
                    "progress": f"{i + len(chunk)}/{len(tokens)}",
                    "status": "streaming"
                })

            # æ¨¡æ‹Ÿtokenç”Ÿæˆçš„æ—¶é—´é—´éš”
            await asyncio.sleep(0.1)

        # å‘å¸ƒæµå¼è¾“å‡ºå®Œæˆäº‹ä»¶
        await publisher.publish_event(
            job_id, {
                "eventType": "document_content_completed",
                "taskType": "document_generation",
                "totalTokens": len(tokens),
                "status": "completed"
            })

        logger.info(f"æ–‡æ¡£å†…å®¹æµå¼è¾“å‡ºå®Œæˆï¼Œå…± {len(tokens)} ä¸ªtoken")

    except Exception as e:
        logger.error(f"æµå¼è¾“å‡ºæ–‡æ¡£å†…å®¹å¤±è´¥: {e}")


def generate_mock_sources_for_chapter(chapter_title: str,
                                      chapter_index: int) -> list:
    """ä¸ºç« èŠ‚ç”Ÿæˆæ¨¡æ‹Ÿçš„å¼•ç”¨æºåˆ—è¡¨"""
    import time

    sources = []

    # ç”Ÿæˆæ–‡æ¡£ç±»å‹çš„å¼•ç”¨æº
    sources.append({
        "id":
        f"{chapter_index * 3 + 1}",
        "detailId":
        f"{chapter_index * 3 + 1}",
        "originInfo":
        f"å…³äº{chapter_title}çš„é‡è¦ç ”ç©¶æˆæœå’Œæœ€æ–°å‘ç°ã€‚æœ¬æ–‡ä¸»è¦å‚è€ƒäº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶æ–‡çŒ®ï¼ŒåŒ…æ‹¬ç†è®ºåŸºç¡€ã€å®è¯ç ”ç©¶å’Œåº”ç”¨æ¡ˆä¾‹ã€‚",
        "title":
        f"{chapter_title}ç ”ç©¶ç»¼è¿°.pdf",
        "fileToken":
        f"token_{chapter_index * 3 + 1}_{int(time.time())}",
        "domainId":
        "document",
        "isFeishuSource":
        None,
        "valid":
        "true",
        "metadata":
        json.dumps(
            {
                "file_name": f"{chapter_title}ç ”ç©¶ç»¼è¿°.pdf",
                "locations": [{
                    "pagenum": chapter_index + 1
                }],
                "source": "data_platform"
            },
            ensure_ascii=False)
    })

    # ç”Ÿæˆæ ‡å‡†ç±»å‹çš„å¼•ç”¨æº
    sources.append({
        "id":
        f"{chapter_index * 3 + 2}",
        "detailId":
        f"{chapter_index * 3 + 2}",
        "originInfo":
        f"è¯¦ç»†çš„æŠ€æœ¯åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«{chapter_title}çš„æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹å’Œæ ‡å‡†è§„èŒƒã€‚",
        "title":
        f"GB/T {chapter_index + 1000}-2023 {chapter_title}æŠ€æœ¯æ ‡å‡†.pdf",
        "fileToken":
        f"token_{chapter_index * 3 + 2}_{int(time.time())}",
        "domainId":
        "standard",
        "isFeishuSource":
        None,
        "valid":
        "true",
        "metadata":
        json.dumps(
            {
                "file_name":
                f"GB/T {chapter_index + 1000}-2023 {chapter_title}æŠ€æœ¯æ ‡å‡†.pdf",
                "locations": [{
                    "pagenum": chapter_index + 5
                }],
                "code": f"GB/T {chapter_index + 1000}-2023",
                "gfid": f"GB/T {chapter_index + 1000}-2023",
                "source": "data_platform"
            },
            ensure_ascii=False)
    })

    # ç”Ÿæˆä¹¦ç±ç±»å‹çš„å¼•ç”¨æº
    sources.append({
        "id":
        f"{chapter_index * 3 + 3}",
        "detailId":
        f"{chapter_index * 3 + 3}",
        "originInfo":
        f"æœ€æ–°çš„å­¦æœ¯ç ”ç©¶æˆæœï¼Œä¸º{chapter_title}æä¾›äº†ç†è®ºæ”¯æ’‘å’Œå®è·µæŒ‡å¯¼ã€‚",
        "title":
        f"{chapter_title}æŠ€æœ¯æ‰‹å†Œ.pdf",
        "fileToken":
        f"token_{chapter_index * 3 + 3}_{int(time.time())}",
        "domainId":
        "book",
        "isFeishuSource":
        None,
        "valid":
        "true",
        "metadata":
        json.dumps(
            {
                "file_name": f"{chapter_title}æŠ€æœ¯æ‰‹å†Œ.pdf",
                "locations": [{
                    "pagenum": chapter_index + 10
                }],
                "source": "data_platform"
            },
            ensure_ascii=False)
    })

    return sources


def generate_mock_web_sources_for_chapter(chapter_title: str,
                                          chapter_index: int) -> list:
    """ä¸ºç« èŠ‚ç”Ÿæˆæ¨¡æ‹Ÿçš„ç½‘é¡µå¼•ç”¨æºåˆ—è¡¨"""
    import time

    web_sources = []

    web_sources.append({
        "id":
        f"web_{chapter_index * 2 + 1}",
        "detailId":
        f"web_{chapter_index * 2 + 1}",
        "materialContent":
        f"å…³äº{chapter_title}çš„åœ¨çº¿èµ„æ–™å’Œæœ€æ–°åŠ¨æ€ã€‚åŒ…å«ç›¸å…³æŠ€æœ¯å‘å±•ã€è¡Œä¸šè¶‹åŠ¿å’Œå®é™…åº”ç”¨æ¡ˆä¾‹ã€‚",
        "materialTitle":
        f"{chapter_title}æŠ€æœ¯å‘å±•åŠ¨æ€-æŠ€æœ¯èµ„è®¯",
        "url":
        f"https://example.com/tech/{chapter_title.lower().replace(' ', '-')}",
        "siteName":
        "æŠ€æœ¯èµ„è®¯ç½‘",
        "datePublished":
        "2023å¹´12æœˆ15æ—¥",
        "materialId":
        f"web_{chapter_index * 2 + 1}_{int(time.time())}"
    })

    web_sources.append({
        "id":
        f"web_{chapter_index * 2 + 2}",
        "detailId":
        f"web_{chapter_index * 2 + 2}",
        "materialContent":
        f"{chapter_title}ç›¸å…³çš„ç ”ç©¶æŠ¥å‘Šå’Œè¡Œä¸šåˆ†æã€‚",
        "materialTitle":
        f"{chapter_title}è¡Œä¸šåˆ†ææŠ¥å‘Š-ç ”ç©¶æŠ¥å‘Š",
        "url":
        f"https://research.example.com/report/{chapter_index + 1}",
        "siteName":
        "ç ”ç©¶æŠ¥å‘Šç½‘",
        "datePublished":
        "2023å¹´11æœˆ20æ—¥",
        "materialId":
        f"web_{chapter_index * 2 + 2}_{int(time.time())}"
    })

    return web_sources


@celery_app.task
def get_job_status(job_id: Union[str, int]) -> dict:
    """
    è·å–ä»»åŠ¡çŠ¶æ€

    Args:
        job_id: ä»»åŠ¡ID

    Returns:
        ä»»åŠ¡çŠ¶æ€å­—å…¸
    """
    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(_get_job_status_async(job_id))
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return {"status": "error", "error": str(e)}


async def _get_job_status_async(job_id: Union[str, int]) -> dict:
    """å¼‚æ­¥è·å–ä»»åŠ¡çŠ¶æ€çš„å†…éƒ¨å®ç°"""
    try:
        redis = await get_redis_client()
        job_data = await redis.hgetall(f"job:{job_id}")

        if not job_data:
            return {"status": "not_found"}

        return job_data

    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return {"status": "error", "error": str(e)}


# Celery ä»»åŠ¡å¯¼å…¥
from .celery_app import celery_app


@celery_app.task
def run_main_workflow(job_id: str, topic: str, genre: str = "default") -> str:
    """
    ä¸»è¦çš„å¼‚æ­¥å·¥ä½œæµå‡½æ•°
    ä½¿ç”¨çœŸå®çš„å›¾æ‰§è¡Œå™¨å’ŒRediså›è°ƒå¤„ç†å™¨

    Args:
        job_id: ä»»åŠ¡ID
        topic: æ–‡æ¡£ä¸»é¢˜
        genre: æ–‡æ¡£ç±»å‹ï¼Œç”¨äºé€‰æ‹©ç›¸åº”çš„promptç­–ç•¥

    Returns:
        ä»»åŠ¡çŠ¶æ€
    """
    logger.info(f"ä¸»å·¥ä½œæµå¼€å§‹ - Job ID: {job_id}, Topic: {topic}, Genre: {genre}")

    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(_run_main_workflow_async(job_id, topic, genre))
    except Exception as e:
        logger.error(f"ä¸»å·¥ä½œæµä»»åŠ¡å¤±è´¥: {e}")
        return "FAILED"


async def _run_main_workflow_async(job_id: str,
                                   topic: str,
                                   genre: str = "default") -> str:
    """å¼‚æ­¥ä¸»å·¥ä½œæµä»»åŠ¡çš„å†…éƒ¨å®ç°"""
    try:
        # è·å–Rediså®¢æˆ·ç«¯
        redis = await get_redis_client()

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿›è¡Œä¸­
        await redis.hset(f"job:{job_id}",
                         mapping={
                             "status": "processing",
                             "topic": topic,
                             "genre": genre,
                             "started_at": str(asyncio.get_event_loop().time())
                         })

        # è·å–å¸¦æœ‰Rediså›è°ƒå¤„ç†å™¨çš„å›¾æ‰§è¡Œå™¨
        logger.info(f"Job {job_id}: è·å–å›¾æ‰§è¡Œå™¨...")
        container = get_container()
        runnable = container.get_graph_runnable_for_job(job_id, genre)

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        from doc_agent.graph.state import ResearchState
        import uuid

        run_id = f"run-{uuid.uuid4().hex[:8]}"
        initial_state = ResearchState(
            topic=topic,
            style_guide_content="",
            requirements_content="",
            initial_sources=[],
            document_outline={},
            chapters_to_process=[],
            current_chapter_index=0,
            current_citation_index=1,
            completed_chapters=[],
            final_document="",
            sources=[],
            all_sources=[],
            cited_sources=[],
            cited_sources_in_chapter=[],
            messages=[],
            run_id=run_id,
        )

        logger.info(f"Job {job_id}: å¼€å§‹æ‰§è¡Œå·¥ä½œæµ...")

        # æ‰§è¡Œå·¥ä½œæµ
        async for step_output in runnable.astream(initial_state):
            node_name = list(step_output.keys())[0]
            logger.info(f"Job {job_id}: å®Œæˆæ­¥éª¤ [{node_name}]")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        await redis.hset(f"job:{job_id}",
                         mapping={
                             "status": "completed",
                             "completed_at":
                             str(asyncio.get_event_loop().time())
                         })

        logger.info(f"Job {job_id}: å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        return "COMPLETED"

    except Exception as e:
        logger.error(f"Job {job_id}: å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        try:
            await redis.hset(f"job:{job_id}",
                             mapping={
                                 "status": "failed",
                                 "error": str(e),
                                 "failed_at":
                                 str(asyncio.get_event_loop().time())
                             })
        except Exception as update_error:
            logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {update_error}")

        return "FAILED"


@celery_app.task
def test_celery_task(message: str) -> str:
    """
    æµ‹è¯• Celery ä»»åŠ¡
    
    Args:
        message: æµ‹è¯•æ¶ˆæ¯
        
    Returns:
        str: è¿”å›çš„æ¶ˆæ¯
    """
    logger.info(f"æ‰§è¡Œ Celery æµ‹è¯•ä»»åŠ¡: {message}")
    return f"Celery ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ: {message}"


@celery_app.task
def generate_document_celery(job_id: Union[str, int], topic: str) -> dict:
    """
    ä½¿ç”¨ Celery æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆä»»åŠ¡
    
    Args:
        job_id: ä½œä¸šID
        topic: æ–‡æ¡£ä¸»é¢˜
        
    Returns:
        dict: æ‰§è¡Œç»“æœ
    """
    logger.info(f"å¼€å§‹ Celery æ–‡æ¡£ç”Ÿæˆä»»åŠ¡: {job_id}, ä¸»é¢˜: {topic}")

    try:
        # è¿™é‡Œå¯ä»¥è°ƒç”¨ç°æœ‰çš„æ–‡æ¡£ç”Ÿæˆé€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        result = {
            "job_id": job_id,
            "status": "completed",
            "topic": topic,
            "document_length": 5000,
            "chapters": 5
        }

        logger.info(f"Celery æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å®Œæˆ: {job_id}")
        return result

    except Exception as e:
        logger.error(f"Celery æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¤±è´¥: {job_id}, é”™è¯¯: {e}")
        return {"job_id": job_id, "status": "failed", "error": str(e)}


@celery_app.task
def process_files_task(context_id: str, files: list[dict]) -> str:
    """
    å¤„ç†ä¸Šä¼ æ–‡ä»¶çš„å¼‚æ­¥ä»»åŠ¡
    
    Args:
        context_id: ä¸Šä¸‹æ–‡ID
        files: æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å« file_id, file_name, storage_url, file_type
        
    Returns:
        ä»»åŠ¡çŠ¶æ€
    """
    logger.info(f"æ–‡ä»¶å¤„ç†ä»»åŠ¡å¼€å§‹ - Context ID: {context_id}, æ–‡ä»¶æ•°é‡: {len(files)}")

    try:
        # ä½¿ç”¨åŒæ­¥æ–¹å¼è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(_process_files_task_async(context_id, files))
    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
        return "FAILED"


async def _process_files_task_async(context_id: str, files: list[dict]) -> str:
    """å¼‚æ­¥æ–‡ä»¶å¤„ç†ä»»åŠ¡çš„å†…éƒ¨å®ç°"""
    try:
        # è·å–Rediså®¢æˆ·ç«¯
        redis_client = await get_redis_client()

        # åˆå§‹åŒ–å†…å®¹åˆ—è¡¨
        style_contents = []
        requirements_contents = []

        # éå†æ‰€æœ‰æ–‡ä»¶
        for file_info in files:
            file_id = file_info.get("file_id")
            file_name = file_info.get("file_name")
            storage_url = file_info.get("storage_url")
            file_type = file_info.get("file_type", "content")  # é»˜è®¤ä¸ºcontent

            logger.info(f"å¤„ç†æ–‡ä»¶: {file_name} (ç±»å‹: {file_type})")

            if file_type == "content":
                # ä¿æŒç°æœ‰çš„å‘é‡ç´¢å¼•é€»è¾‘
                logger.info(f"å¤„ç†å†…å®¹æ–‡ä»¶: {file_name}")
                # TODO: å®ç°å‘é‡ç´¢å¼•é€»è¾‘
                # è¿™é‡Œåº”è¯¥è°ƒç”¨ç°æœ‰çš„å‘é‡ç´¢å¼•å¤„ç†å‡½æ•°

            elif file_type == "style":
                # è¯»å–æ ·å¼æŒ‡å—å†…å®¹
                logger.info(f"å¤„ç†æ ·å¼æŒ‡å—æ–‡ä»¶: {file_name}")
                try:
                    content = read_file_content(storage_url)
                    style_contents.append(content)
                    logger.info(f"æ ·å¼æŒ‡å—æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}")
                except Exception as e:
                    logger.error(f"å¤„ç†æ ·å¼æŒ‡å—æ–‡ä»¶å¤±è´¥: {file_name}, é”™è¯¯: {e}")

            elif file_type == "requirements":
                # è¯»å–éœ€æ±‚æ–‡æ¡£å†…å®¹
                logger.info(f"å¤„ç†éœ€æ±‚æ–‡æ¡£æ–‡ä»¶: {file_name}")
                try:
                    content = read_file_content(storage_url)
                    requirements_contents.append(content)
                    logger.info(f"éœ€æ±‚æ–‡æ¡£æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}")
                except Exception as e:
                    logger.error(f"å¤„ç†éœ€æ±‚æ–‡æ¡£æ–‡ä»¶å¤±è´¥: {file_name}, é”™è¯¯: {e}")

            else:
                logger.warning(f"æœªçŸ¥æ–‡ä»¶ç±»å‹: {file_type}, æ–‡ä»¶: {file_name}")

        # å­˜å‚¨æ ·å¼æŒ‡å—å†…å®¹åˆ°Redis
        if style_contents:
            style_guide_content = "\n\n".join(style_contents)
            await redis_client.hset(f"context:{context_id}",
                                    "style_guide_content", style_guide_content)
            logger.info(f"æ ·å¼æŒ‡å—å†…å®¹å·²å­˜å‚¨åˆ°Redis, é•¿åº¦: {len(style_guide_content)} å­—ç¬¦")

        # å­˜å‚¨éœ€æ±‚æ–‡æ¡£å†…å®¹åˆ°Redis
        if requirements_contents:
            requirements_content = "\n\n".join(requirements_contents)
            await redis_client.hset(f"context:{context_id}",
                                    "requirements_content",
                                    requirements_content)
            logger.info(f"éœ€æ±‚æ–‡æ¡£å†…å®¹å·²å­˜å‚¨åˆ°Redis, é•¿åº¦: {len(requirements_content)} å­—ç¬¦")

        # æ›´æ–°ä¸Šä¸‹æ–‡çŠ¶æ€ä¸ºå°±ç»ª
        await redis_client.hset(f"context:{context_id}", "status", "READY")

        logger.info(f"æ–‡ä»¶å¤„ç†ä»»åŠ¡å®Œæˆ - Context ID: {context_id}")
        return "SUCCESS"

    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤„ç†ä»»åŠ¡å¤±è´¥ - Context ID: {context_id}, é”™è¯¯: {e}")
        return "FAILED"


def read_file_content(storage_url: str) -> str:
    """
    è¯»å–æ–‡ä»¶å†…å®¹çš„å·¥å…·å‡½æ•°
    
    Args:
        storage_url: æ–‡ä»¶å­˜å‚¨URL
        
    Returns:
        æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²
    """
    # TODO: å®ç°å®é™…çš„æ–‡ä»¶è¯»å–é€»è¾‘
    # è¿™é‡Œåº”è¯¥æ ¹æ®storage_urlè¯»å–æ–‡ä»¶å†…å®¹
    # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿå†…å®¹
    logger.info(f"è¯»å–æ–‡ä»¶å†…å®¹: {storage_url}")
    return f"æ¨¡æ‹Ÿæ–‡ä»¶å†…å®¹ - {storage_url}"
